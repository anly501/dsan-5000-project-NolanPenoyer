{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Data Gathering\"\n",
        "format: html\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Census Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I will attempt to gather all the data I need to answer my 10 questions. Most of the data I acquire will come from [census.gov](https://www.census.gov/). I will use data tables from the U.S. Census Bureau's [American Community Survey](https://www.census.gov/data/developers/data-sets/ACS-supplemental-data.html) (ACS), a nationwide survey that collects and produces information on social, economic, housing, and demographic characteristics about our nation's population each year.\n",
        "\n",
        "\n",
        "I chose census data and specifically the ACS tables because, from experience and knowledge I have accumulated in my life, I inferred that the variables in these tables could be useful in predicting home prices. For example, one variable I will examine is the number of people with bachelor's degrees. Usually, higher education is linked to higher incomes, and therefore, higher home prices. The variables I choose to include from the tables may prove valuable or they may not; part of my analysis is determining which variables are the most important. Also, a lot of my variables are heavily correlated with each other, like total households and total married households. This can become an issue in the models I create as multicollinearity increases variance and makes it hard to know the impact of an individual variable in the model. I am aware of this and will address it later in my project.\n",
        "\n",
        "I will import the DP02-DP05 tables from 2017-2022, excluding 2020 because there was not accurate data that year due to COVID. Ultimately, I will not need all the columns in the tables, but it will be nice to have easy access to them in my future analysis. I only went back to 2017 for now, but if I wanted to improve my models, I could retrieve more data. The same methodology will apply no matter how much data I acquire.\n",
        "\n",
        "Here are what the ACS tables contain:<br>\n",
        "DP02: Selected Social Characteristics in the United States <br>\n",
        "DP03: Selected Economic Characteristics in the United States <br>\n",
        "DP04: Selected Housing Characteristics <br>\n",
        "DP05: ACS Demographic and Housing Estimates\n",
        "\n",
        "Using the API, you can query different geography hierarchies, like if you want data on regions in the U.S. or, in my analysis, on U.S. states.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I will start by retreiving one table to see what we are working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1209</th>\n",
              "      <th>1210</th>\n",
              "      <th>1211</th>\n",
              "      <th>1212</th>\n",
              "      <th>1213</th>\n",
              "      <th>1214</th>\n",
              "      <th>1215</th>\n",
              "      <th>1216</th>\n",
              "      <th>1217</th>\n",
              "      <th>1218</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DP02_0001E</td>\n",
              "      <td>DP02_0001EA</td>\n",
              "      <td>DP02_0001M</td>\n",
              "      <td>DP02_0001MA</td>\n",
              "      <td>DP02_0001PE</td>\n",
              "      <td>DP02_0001PEA</td>\n",
              "      <td>DP02_0001PM</td>\n",
              "      <td>DP02_0001PMA</td>\n",
              "      <td>DP02_0002E</td>\n",
              "      <td>DP02_0002EA</td>\n",
              "      <td>...</td>\n",
              "      <td>DP02_0152EA</td>\n",
              "      <td>DP02_0152M</td>\n",
              "      <td>DP02_0152MA</td>\n",
              "      <td>DP02_0152PE</td>\n",
              "      <td>DP02_0152PEA</td>\n",
              "      <td>DP02_0152PM</td>\n",
              "      <td>DP02_0152PMA</td>\n",
              "      <td>GEO_ID</td>\n",
              "      <td>NAME</td>\n",
              "      <td>state</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1091980</td>\n",
              "      <td>None</td>\n",
              "      <td>9693</td>\n",
              "      <td>None</td>\n",
              "      <td>1091980</td>\n",
              "      <td>None</td>\n",
              "      <td>-888888888</td>\n",
              "      <td>(X)</td>\n",
              "      <td>716451</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>9854</td>\n",
              "      <td>None</td>\n",
              "      <td>73.4</td>\n",
              "      <td>None</td>\n",
              "      <td>0.6</td>\n",
              "      <td>None</td>\n",
              "      <td>0400000US28</td>\n",
              "      <td>Mississippi</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2385135</td>\n",
              "      <td>None</td>\n",
              "      <td>13054</td>\n",
              "      <td>None</td>\n",
              "      <td>2385135</td>\n",
              "      <td>None</td>\n",
              "      <td>-888888888</td>\n",
              "      <td>(X)</td>\n",
              "      <td>1527260</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>15052</td>\n",
              "      <td>None</td>\n",
              "      <td>81.3</td>\n",
              "      <td>None</td>\n",
              "      <td>0.4</td>\n",
              "      <td>None</td>\n",
              "      <td>0400000US29</td>\n",
              "      <td>Missouri</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>423091</td>\n",
              "      <td>None</td>\n",
              "      <td>4068</td>\n",
              "      <td>None</td>\n",
              "      <td>423091</td>\n",
              "      <td>None</td>\n",
              "      <td>-888888888</td>\n",
              "      <td>(X)</td>\n",
              "      <td>262726</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>4698</td>\n",
              "      <td>None</td>\n",
              "      <td>81.3</td>\n",
              "      <td>None</td>\n",
              "      <td>0.9</td>\n",
              "      <td>None</td>\n",
              "      <td>0400000US30</td>\n",
              "      <td>Montana</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>754490</td>\n",
              "      <td>None</td>\n",
              "      <td>4583</td>\n",
              "      <td>None</td>\n",
              "      <td>754490</td>\n",
              "      <td>None</td>\n",
              "      <td>-888888888</td>\n",
              "      <td>(X)</td>\n",
              "      <td>484989</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>6206</td>\n",
              "      <td>None</td>\n",
              "      <td>84.4</td>\n",
              "      <td>None</td>\n",
              "      <td>0.5</td>\n",
              "      <td>None</td>\n",
              "      <td>0400000US31</td>\n",
              "      <td>Nebraska</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1219 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0            1           2            3            4     \\\n",
              "0  DP02_0001E  DP02_0001EA  DP02_0001M  DP02_0001MA  DP02_0001PE   \n",
              "1     1091980         None        9693         None      1091980   \n",
              "2     2385135         None       13054         None      2385135   \n",
              "3      423091         None        4068         None       423091   \n",
              "4      754490         None        4583         None       754490   \n",
              "\n",
              "           5            6             7           8            9     ...  \\\n",
              "0  DP02_0001PEA  DP02_0001PM  DP02_0001PMA  DP02_0002E  DP02_0002EA  ...   \n",
              "1          None   -888888888           (X)      716451         None  ...   \n",
              "2          None   -888888888           (X)     1527260         None  ...   \n",
              "3          None   -888888888           (X)      262726         None  ...   \n",
              "4          None   -888888888           (X)      484989         None  ...   \n",
              "\n",
              "          1209        1210         1211         1212          1213  \\\n",
              "0  DP02_0152EA  DP02_0152M  DP02_0152MA  DP02_0152PE  DP02_0152PEA   \n",
              "1         None        9854         None         73.4          None   \n",
              "2         None       15052         None         81.3          None   \n",
              "3         None        4698         None         81.3          None   \n",
              "4         None        6206         None         84.4          None   \n",
              "\n",
              "          1214          1215         1216         1217   1218  \n",
              "0  DP02_0152PM  DP02_0152PMA       GEO_ID         NAME  state  \n",
              "1          0.6          None  0400000US28  Mississippi     28  \n",
              "2          0.4          None  0400000US29     Missouri     29  \n",
              "3          0.9          None  0400000US30      Montana     30  \n",
              "4          0.5          None  0400000US31     Nebraska     31  \n",
              "\n",
              "[5 rows x 1219 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "DP02_URL_2017=\"https://api.census.gov/data/2017/acs/acs1/profile?get=group(DP02)&for=state:*\"\n",
        "DP02_2017= requests.get(DP02_URL_2017)\n",
        "DP02_2017 = DP02_2017.json()\n",
        "DP02_2017=pd.DataFrame(DP02_2017)\n",
        "DP02_2017.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " The column names do not tell us much right now because they are codes that match to different variable labels in the census data. For example, DP02_0001E maps to total household counts. In the data cleaning section, we will be sure to give each column a proper label that tells us what the column represents. Also in the data cleaning section, we will only keep a select few columns out of the over 1000 columns in the tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I will use the following loop to get each table I want and turn them into csv files for easy retrieval for the rest of my analysis. If you would like to see my csv files, please refer to this [GitHub Repository](https://github.com/anly501/dsan-5000-project-NolanPenoyer/tree/main/Nolan%20Penoyer%20Website).<br>\n",
        "*Nolan Penoyer Website/data* (Every file that starts with a year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "string='https://api.census.gov/data/2016/acs/acs1/profile?get=group(DP05)&for=state'\n",
        "list1=['2016','2017','2018','2019','2021']\n",
        "list2=['2017','2018','2019','2021','2022']\n",
        "list3=['DP01','DP02','DP03','DP04']\n",
        "list4=['DP02','DP03','DP04','DP05']\n",
        "for i in range(5):\n",
        "    string=string.replace('DP05',list3[0])\n",
        "    for w in range(4):\n",
        "        file='./data/'\n",
        "        csv='.csv'\n",
        "        name=file+list2[i]+list4[w]+csv\n",
        "        csvname=file+list2[i]+list3[w]\n",
        "        string=string.replace(list1[i],list2[i])\n",
        "        string=string.replace(list3[w],list4[w])\n",
        "        response=requests.get(string)\n",
        "        response = response.json()\n",
        "        df=pd.DataFrame(response)\n",
        "        df.to_csv(name, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Home Value Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "My home value data comes from [Zillow](https://www.zillow.com/research/data/). Specifically, I will use Zillow's Home Value Index (ZHVI), which reflects the typical value of homes in the 35th to 65th percentile, a home that will be in my price range. The ZHVI was created to accurately capture the seasonally adjusted value of a typical property across the nation as opposed to just the homes that sold. If you would like to learn more about how Zillow calculates this value and why it is a more accurate measure of home value as opposed to other alternatives like median home price, you can visit the link I provided.\n",
        "\n",
        "Zillow has an API, but since it was one table I needed, I downloaded the csv file. Like my census data, I calculated this data on the state level for years 2017-2022, excluding 2020. If you would like to see my csv file, please refer to this [GitHub Repository](https://github.com/anly501/dsan-5000-project-NolanPenoyer/tree/main/Nolan%20Penoyer%20Website).<br>\n",
        "*Nolan Penoyer Website/data* (Zillow.csv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# News Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although not a focus of my project, news headline text data could potentially provide us with some insight on the sentiment of the real estate market and therefore what direction the general population thinks home prices are heading. The overall sentiment can provide us with signals that people are overvaluing or undervaluing homes, or that a rise or drop in home prices may be pending. Knowing if the general public believes buying a home, especially in a certain state, is a smart decision, can tell us which direction prices are headed, depending if you agree with the general public or not. Another potential data science project I would like to study in the future is if market sentiment matches market performance, and if there is a lag between the two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I will use the [news API](https://newsapi.org/) to search and retreive live articles from across the web. I will only search for articles that contain \"Real Estate\" in the headline. If you would like to see my csv file, please refer to this [GitHub Repository](https://github.com/anly501/dsan-5000-project-NolanPenoyer/tree/main/Nolan%20Penoyer%20Website).<br>\n",
        "*Nolan Penoyer Website/data* (news.csv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "baseURL = \"https://newsapi.org/v2/everything?\"\n",
        "total_requests=2\n",
        "verbose=True\n",
        "API_KEY='6a2b4aca491e4bedb90a4c7275f2ddf6'\n",
        "TOPIC='Real Estate'\n",
        "URLpost = {'apiKey': API_KEY,\n",
        "            'q': '+'+TOPIC,\n",
        "            'sortBy': 'relevancy',\n",
        "            'totalRequests': 1}\n",
        "\n",
        "#print(baseURL)\n",
        "response = requests.get(baseURL, URLpost)\n",
        "response = response.json()\n",
        "def string_cleaner(input_string):\n",
        "    try: \n",
        "        out=re.sub(r\"\"\"\n",
        "                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n",
        "                    \\ *           # plus zero or more copies of a space,\n",
        "                    \"\"\",\n",
        "                    \" \",          # and replace it with a single space\n",
        "                    input_string, flags=re.VERBOSE)\n",
        "\n",
        "        #REPLACE SELECT CHARACTERS WITH NOTHING\n",
        "        out = re.sub('[’.]+', '', input_string)\n",
        "\n",
        "        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n",
        "        out = re.sub(r'\\s+', ' ', out)\n",
        "\n",
        "        #CONVERT TO LOWER CASE\n",
        "        out=out.lower()\n",
        "    except:\n",
        "        #print(\"ERROR\")\n",
        "        out=''\n",
        "    return out\n",
        "\n",
        "article_list=response['articles']   #list of dictionaries for each article\n",
        "article_keys=article_list[0].keys()\n",
        "#print(\"AVAILABLE KEYS:\")\n",
        "#print(article_keys)\n",
        "index=0\n",
        "cleaned_data=[];  \n",
        "for article in article_list:\n",
        "    tmp=[]\n",
        "    for key in article_keys:\n",
        "        if(key=='source'):\n",
        "            src=string_cleaner(article[key]['name'])\n",
        "            tmp.append(src) \n",
        "\n",
        "        if(key=='author'):\n",
        "            author=string_cleaner(article[key])\n",
        "            #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n",
        "            if(src in author): \n",
        "                tmp.append(author)\n",
        "\n",
        "        if(key=='title'):\n",
        "            tmp.append(string_cleaner(article[key]))\n",
        "\n",
        "        # if(key=='description'):\n",
        "        #     tmp.append(string_cleaner(article[key]))\n",
        "\n",
        "        # if(key=='content'):\n",
        "        #     tmp.append(string_cleaner(article[key]))\n",
        "\n",
        "        if(key=='publishedAt'):\n",
        "            #DEFINE DATA PATERN FOR RE TO CHECK  .* --> wildcard\n",
        "            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n",
        "            date=article[key]\n",
        "            if(not ref.match(date)):\n",
        "                print(\" DATE ERROR:\",date); date=\"NA\"\n",
        "            tmp.append(date)\n",
        "\n",
        "    cleaned_data.append(tmp)\n",
        "    index+=1\n",
        "\n",
        "df = pd.DataFrame(cleaned_data)\n",
        "df.head()\n",
        "df.to_csv('./data/news.csv', index=False) #,index_label=['title','src','author','date','description'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lifehackercom</td>\n",
              "      <td>elizabeth yuko</td>\n",
              "      <td>what to do if your home inspector missed a maj...</td>\n",
              "      <td>2023-11-12T18:00:00Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>slashdotorg</td>\n",
              "      <td>msmash</td>\n",
              "      <td>almost no one pays a 6% real-estate commission...</td>\n",
              "      <td>2023-11-17T18:40:00Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>business insider</td>\n",
              "      <td>george glover</td>\n",
              "      <td>real estate made china rich now it's looking m...</td>\n",
              "      <td>2023-11-16T10:11:14Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>business insider</td>\n",
              "      <td>cork gaines</td>\n",
              "      <td>baby boomers got rich off real estate and they...</td>\n",
              "      <td>2023-11-19T10:32:01Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>business insider</td>\n",
              "      <td>theron mohamed</td>\n",
              "      <td>stocks may crash 30%, a recession looks immine...</td>\n",
              "      <td>2023-11-19T11:30:01Z</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0               1  \\\n",
              "0     lifehackercom  elizabeth yuko   \n",
              "1       slashdotorg          msmash   \n",
              "2  business insider   george glover   \n",
              "3  business insider     cork gaines   \n",
              "4  business insider  theron mohamed   \n",
              "\n",
              "                                                   2                     3  \n",
              "0  what to do if your home inspector missed a maj...  2023-11-12T18:00:00Z  \n",
              "1  almost no one pays a 6% real-estate commission...  2023-11-17T18:40:00Z  \n",
              "2  real estate made china rich now it's looking m...  2023-11-16T10:11:14Z  \n",
              "3  baby boomers got rich off real estate and they...  2023-11-19T10:32:01Z  \n",
              "4  stocks may crash 30%, a recession looks immine...  2023-11-19T11:30:01Z  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here I will use an API in R to retreive text data that can give me some insight into some real estate trends and what states people are talking about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reddit Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I can also retrieve Reddit text data for the same reason. Reddit comments are a more accurate measure of what the public believes than news data as there are no biases and incentives from corporations to report news a certain way. I also can break up the text data by state, which I attempted to do below. I need to find a work around as Reddit limits how many requests you are allowed in a period of time. I will not use Reddit data for the rest of this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "library(RedditExtractoR)\n",
        "library(dplyr)\n",
        "states <- c(\n",
        "  \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\",\n",
        "  \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\",\n",
        "  \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n",
        "  \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\",\n",
        "  \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\",\n",
        "  \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\",\n",
        "  \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\",\n",
        "  \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\",\n",
        "  \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\",\n",
        "  \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        ")\n",
        "text<-data.frame()\n",
        "subreddit=\"RealEstate\"\n",
        "for (state in states){\n",
        "    state_df<- find_thread_urls(keywords = state,subreddit=subreddit, sort_by=\"top\", period = 'year')\n",
        "    state_df<-state_df|>\n",
        "    mutate(State=state)\n",
        "    text<-rbind(text,state_df)\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "R",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
