[
  {
    "objectID": "NaiveBayes.html",
    "href": "NaiveBayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Introduction to Naive Bayes\nNaive Bayes is a supervised machine learning algorithm used for classification tasks. The algorithm is rooted in Bayes Theorem in that it is based on the probability of a hypothesis, given data and prior knowledge. Below is the equation the Naive Bayes Algorithm uses.\n\n\n\nBayes’ Theorem\n\n\nWhere P(c|x)= The probability of the target variable class given the features  P(x|c)= The probability of the features given the class  and P(c), P(x) are the prior probabilties of the target variable class and features.\nThe algorithm estimates the probability of each class for a given data point and assign the class with the highest probability to the data point. It assumes that all features are independent of each other; this is usually not the case in the real world but the algorithm still provides accurate predictions. Ultimately, through Naive Bayes you want to achieve accurate classifications given the algorithm is given data with a set of variables. In my case, I would like to predict if a certain state’s home value will increase substantially in a certain year based on census data. Different variants of Naive Bayes are used for different applications. Multinomial Naive Bayes is used for word counts and frequency analysis (discrete data). Gaussian Naive Bayes is used for numeric data that is approximately normally distributed and independent. Bernoulli Naive Bayes is used for binary data, like if a word appears in a document or not.\n\n\nPrepare Data for Naive Bayes\nCurrently my explanatory variable is continuious. I must change it to a label variable. To do this, I will choose home value percent changes that are above the mean to 1s, home value percent changes that are below the mean below the mean to 0s.\n\n\nCode\nimport pandas as pd\nrecord=pd.read_csv('data/RecordData.csv')\nmean=record['Typical Home Value'].mean()\nrecord['Typical Home Value']=record['Typical Home Value'].apply(lambda x:1 if x&gt;mean else 0)\nrecord=record.drop(['Year','RegionName'],axis=1)\n\n\n\n\nCode\nrecord.head()\n\n\n\n\n\n\n\n\n\nTotal households\nMarried couple households\nMarried couple with children of the householder under 18 years\nMale householder,no spouse/partner present with children of the householder under 18 years\nFemale householder, no spouse/partner present with children of the householder under 18 years\nNumber of women 15 to 50 years old who had a birth in the past 12 months\nLess than 9th grade\n9th to 12th grade,no diploma\nHigh school graduate (includes equivalency)\nSome college, no degree\n...\nMean Rent Paid\nTotal population\nSex ratio (males per 100 females)\nMedian Age\nRace-White\nRace-Black\nRace-American Indian and Alaska Native\nRace-Asian\nHispanic or Latino\nTypical Home Value\n\n\n\n\n0\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n0.033732\n...\n0.050667\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.047111\n0\n\n\n1\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n1.429793\n...\n0.024112\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n0.036934\n0\n\n\n2\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n0.007408\n...\n0.066914\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n0.082710\n1\n\n\n3\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n0.014854\n...\n0.060395\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n-0.125037\n0\n\n\n4\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n-0.065209\n...\n-0.019983\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.026241\n0\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\nFeature Selection\nThe text data was not very relevant to my project and something happened to my text data, so the code below does not work properly and produce a result that makes sense. It was not worth it to remedy the issue.\n\n\nCode\nimport pandas as pd\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport nltk\nnltk.download('vader_lexicon')\nsid = SentimentIntensityAnalyzer()\nimport numpy as np\nimport matplotlib.pyplot as plt\ntext=pd.read_csv('./data/news.csv')\ntext['score'] = text['2'].apply(lambda x: sid.polarity_scores(x))\ntext['score'] = text['score'].apply(lambda x: x['compound']).astype(float)\ntext['score'].mean()\n\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\npeno\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\nWill say if title is above mean it is positive/neutral, below mean negative.\n\n\nCode\ntitles=[]\ny=[]\n#ITERATE OVER ROWS\n# for i in range(0,10):  \nfor i in range(0,text.shape[0]):\n    # QUICKLY CLEAN TEXT\n    keep=\"abcdefghijklmnopqrstuvwxyz \"\n    replace=\".,!;\"\n    tmp=\"\"\n    for char in text[\"2\"][i].replace(\"&lt;br /&gt;\",\"\").lower():\n        if char in replace:\n            tmp+=\" \"\n        if char in keep:\n            tmp+=char\n    tmp=\" \".join(tmp.split())\n    titles.append(tmp)\n    if(text[\"score\"][i]&lt;=text['score'].mean()):\n        y.append(0)\n    if(text[\"score\"][i]&gt;=text['score'].mean()):\n        y.append(1)\ny=np.array(y)\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef vectorize(corpus,MAX_FEATURES):\n    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n    # RUN COUNT VECTORIZER ON OUR COURPUS \n    Xs  =  vectorizer.fit_transform(corpus)   \n    X=np.array(Xs.todense())\n    #CONVERT TO ONE-HOT VECTORS (can also be done with binary=true in CountVectorizer)\n    maxs=np.max(X,axis=0)\n    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n\n(x,vocab0)=vectorize(titles,MAX_FEATURES=10000)\nvocab1 = dict([(value, key) for key, value in vocab0.items()])\ndf2=pd.DataFrame(x)\ns = df2.sum(axis=0)\ndf2=df2[s.sort_values(ascending=False).index[:]]\ni1=0\nvocab2={}\nfor i2 in list(df2.columns):\n    # print(i2)\n    vocab2[i1]=vocab1[int(i2)]\n    i1+=1\ndf2.columns = range(df2.columns.size)\nx=df2.to_numpy()\nimport random\nN=x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\nprint(train_index[0:10])\nprint(test_index[0:10])\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval)\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\n#partial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\n#partial_grid_search(num_runs=20, min_index=1000, max_index=10000)\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\n[12, 73, 44, 63, 78, 72, 70, 8, 86, 5]\n[62, 40, 66, 60, 65, 83, 71, 9, 80, 59]\n\n\n\n\nNaive Bayes with Labeled Record Data\nNow I will perform the Naive Bayes algorithm on my now labeled record data.\nTo avoid overfitting, it is imperative that I split my data into a training and testing set. The training set will allow my model to learn the relationships in the data, while the testing set will prevent the model from just “memorizing” the training data, as it has to perform well on new data it has not seen before. This balance between bias and variance is key in machine learning.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nnaive = GaussianNB()\nnaive.fit(X_train, y_train)\ny_pred=naive.predict(X_test)\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\nsns.heatmap(cm_df, annot=True,)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.58      0.74        43\n           1       0.51      1.00      0.68        19\n\n    accuracy                           0.71        62\n   macro avg       0.76      0.79      0.71        62\nweighted avg       0.85      0.71      0.72        62\n\n\n\n\n\n\nPredicting the most frequent class:\n\n\nCode\ny_test.value_counts()\n43/(43+19)\n\n\n0.6935483870967742\n\n\nAccuracy on training data:\n\n\nCode\ny_pred_train=naive.predict(X_train)\naccuracy_score(y_train, y_pred_train)\n\n\n0.6338028169014085\n\n\nOverall, this model performs poorly. As you can see, it barley performs better than if you were to pick the most frequent class. Furthermore, the training accuracy is very low. This tells us there is some underfitting going on, meaning the bias is high. While Naive Bayes has its uses, for the purposes of my project I will be moving on to other supervised and unsupervised learning methods to hopefully create a model that produces meaningful results.\n\n\nNaive Bayes with Labeled Text Data\nThe text data was not very relevant to my project and something happened to my text data, so the code below does not work properly and produce a result that makes sense. It was not worth it to remedy the issue.\n\n\nCode\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n\nWe have 100 percent accuracy on training set and 70 percent accuracy on the test set, so overall not a bad model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "GU netID: nsp50\nHello, everyone! My name is Nolan Penoyer, and I am currently a graduate student in the Data Science and Analytics program at Georgetown University. I received my bachelor’s degree in data science from the United States Air Force Academy. Upon graduation, I will work as an operations research analyst in the Air Force. Recently, the Air Force modernized and reclassified its position to focus more on information and big data. The direction the military is headed is clear, and I desire to be a leader in the implementation of the emerging tools in information and advanced analytics in support of the defense of the United States."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n2025: M.S. Data Science and Analytics, Georgetown University\n2023: B.S. Data Science, United States Air Force Academy\n2019: East Syracuse Minoa Central Highschool"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "About Me",
    "section": "Interests",
    "text": "Interests\n\nTravel\nSports\nMusic\nReading"
  },
  {
    "objectID": "DecisionTrees.html",
    "href": "DecisionTrees.html",
    "title": "DecisionTrees",
    "section": "",
    "text": "Methods\nDecision trees are a supervised machine learning method which can be used for both classification and regression. The overall goal of a decision tree is to predict the value of a target variable by learning rules based on the features. The process begins at the root node, which represents the entire dataset you are working with. The root node has no parent node, in that there are no nodes before it; it is not the child of any other node. Then the root node is split into two or more sub nodes based on a set of conditional rules of a feature. For example, a rule could be, in my case, the percent change in the number of high school graduates is greater than 5%. If this a true for the given datapoint, then it will head towards a trajectory in the tree, and if it is not true it will go on a different trajectory. Each non-terminal node, or nodes that have children, is a decision node where a split happens. Decision trees use multiple algorithms to strategically split the tree on certain features and conditions, using various attribute selection measures as well to see which features are most important. The leaf node, or terminal node, is the end of the tree where a prediction is made, which could be either a number or label depending on if it is a classification or regression tree. Decision trees are useful for their interpretability and accuracy, but can be prone to overfitting. Pruning helps remedy this by removing decision nodes in the tree.\nRandom forests are an extension of decision trees that can also aid in avoiding overfitting while still maintaining accuracy and also can provide us insight on feature importance. The process for creating a random forest goes as follows. You create a bootstrapped dataset, where bootstrapping is sampling your existing dataset with replacement. Then, you create a decision tree from the bootstrapped dataset, but with a random subset of variables at each step.Then you repeat this many times constructing a “forest” of decision trees. Then, when you input a datapoint in your model, it is fed into each tree and each tree makes a prediction, then the trees ‘vote’ for classification, or average the predictions for regression, to make a final prediction. You can see the total amount that the RSS decreased due to splits over a given feature and average over all the trees to see which variables contributed most to decreasing error.\nBoosting works similarly to random forests, but the trees are made sequentially, in that each tree is grown based on information from previous trees. Specifically I will discuss the Adaboost model. Instead of constructing whole decision trees, the trees are usually just a node and two leaves, only using one variable to make a decision. We call it a stump. Individual stumps are weak predictors, but errors that the first stump makes influence how the second stump is made. By attaching weights to each sample from the dataset that you are bootstrapping, the next stump that will be made will take the errors of the current stump into account. By increasing sample weights for incorrectly classified samples, and decreasing sample weights for correctly classified samples, points that did poorly will be heavily sampled for the creation of the next stump. Each stump also has a different “Amount of say” for the final classification, depending on their performance.\n\n\nClass Distribution\nHere I will recycle code from my Naive Bayes analysis to transform my continuous target variable into a label.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport random\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import randint\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import RandomizedSearchCV\nrecord=pd.read_csv('data/RecordData.csv')\nmean=record['Typical Home Value'].mean()\nrecordClass=record.copy()\nrecordClass['Typical Home Value']=recordClass['Typical Home Value'].apply(lambda x:1 if x&gt;mean else 0)\nrecordClass=recordClass.drop(['Year','RegionName'],axis=1)\nrecordClass.head()\n\n\n\n\n\n\n\n\n\nTotal households\nMarried couple households\nMarried couple with children of the householder under 18 years\nMale householder,no spouse/partner present with children of the householder under 18 years\nFemale householder, no spouse/partner present with children of the householder under 18 years\nNumber of women 15 to 50 years old who had a birth in the past 12 months\nLess than 9th grade\n9th to 12th grade,no diploma\nHigh school graduate (includes equivalency)\nSome college, no degree\n...\nMean Rent Paid\nTotal population\nSex ratio (males per 100 females)\nMedian Age\nRace-White\nRace-Black\nRace-American Indian and Alaska Native\nRace-Asian\nHispanic or Latino\nTypical Home Value\n\n\n\n\n0\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n0.033732\n...\n0.050667\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.047111\n0\n\n\n1\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n1.429793\n...\n0.024112\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n0.036934\n0\n\n\n2\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n0.007408\n...\n0.066914\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n0.082710\n1\n\n\n3\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n0.014854\n...\n0.060395\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n-0.125037\n0\n\n\n4\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n-0.065209\n...\n-0.019983\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.026241\n0\n\n\n\n\n5 rows × 32 columns\n\n\n\nHere will be my data for my regression model.\n\n\nCode\nrecord=record.drop(['Year','RegionName'],axis=1)\nrecord.head()\n\n\n\n\n\n\n\n\n\nTotal households\nMarried couple households\nMarried couple with children of the householder under 18 years\nMale householder,no spouse/partner present with children of the householder under 18 years\nFemale householder, no spouse/partner present with children of the householder under 18 years\nNumber of women 15 to 50 years old who had a birth in the past 12 months\nLess than 9th grade\n9th to 12th grade,no diploma\nHigh school graduate (includes equivalency)\nSome college, no degree\n...\nMean Rent Paid\nTotal population\nSex ratio (males per 100 females)\nMedian Age\nRace-White\nRace-Black\nRace-American Indian and Alaska Native\nRace-Asian\nHispanic or Latino\nTypical Home Value\n\n\n\n\n0\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n0.033732\n...\n0.050667\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.047111\n0.038688\n\n\n1\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n1.429793\n...\n0.024112\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n0.036934\n0.070360\n\n\n2\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n0.007408\n...\n0.066914\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n0.082710\n0.263465\n\n\n3\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n0.014854\n...\n0.060395\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n-0.125037\n0.076587\n\n\n4\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n-0.065209\n...\n-0.019983\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.026241\n0.005789\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\nCode\nrecordClass['Typical Home Value'].value_counts()\n\n\nTypical Home Value\n0    145\n1     59\nName: count, dtype: int64\n\n\nThe classes are fairly uneven and there is overall not a lot of data, so the algorithm might struggle to learn patterns, especially with the minority class. The model might be biased towards the majority class as well. We also must not only rely on accuracy to understand the performance of our model, as just predicting the majority class the entire time yields a high accuracy score.\n\n\nBaseline Model\n\n\nCode\ndef random_classifier(y_data):\n    ypred=[];\n    max_label=np.max(y_data);\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\",Counter(ypred).values()) # counts the elements' frequency\n    print(\"probability of prediction:\",np.fromiter(Counter(ypred).values(), dtype=float)/len(y_data)) # counts the elements' frequency\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data, ypred))\n    \n\n\n\n\nCode\nprint(\"\\nBINARY CLASS: UNIFORM LOAD\")\ny=recordClass['Typical Home Value']\nrandom_classifier(y)\n\n\n\nBINARY CLASS: UNIFORM LOAD\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([99, 105])\nprobability of prediction: [0.48529412 0.51470588]\naccuracy 0.5294117647058824\npercision, recall, fscore, (array([0.73333333, 0.31313131]), array([0.53103448, 0.52542373]), array([0.616     , 0.39240506]), array([145,  59], dtype=int64))\n\n\nThis is just a model that randomly predicts each label with equal probability. We would obviously hope to better than this model, especially with the recall and f score on the minority class label.\nI’ll do a baseline model for regression as well.\n\n\nCode\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nnaive_model = DummyRegressor(strategy='mean')\nnaive_model.fit(X_train, y_train)\nnaive_predictions = naive_model.predict(X_test)\nmse = mean_squared_error(y_test, naive_predictions)\nprint(mse)\n\n\n0.009404738957389904\n\n\n\n\nDecision Tree Baseline\nMy PCA and data exploration revealed that a lot of my variables are highly correlated in my analysis, and that I need to get rid of some of my features. For now I will just include all my features and see how a baseline decision tree does. I will perform both regression and classification using a basic decision tree. I will start with classification.\n\n\nCode\nX=recordClass.drop(['Typical Home Value'],axis=1)\ny=recordClass['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=tree.DecisionTreeClassifier()\nmodel=model.fit(X_train,y_train)\ny_pred_train=model.predict(X_train)\nprint(classification_report(y_train, y_pred_train))\ncm = confusion_matrix(y_train, y_pred_train)\ncm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\nsns.heatmap(cm_df, annot=True,)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       102\n           1       1.00      1.00      1.00        40\n\n    accuracy                           1.00       142\n   macro avg       1.00      1.00      1.00       142\nweighted avg       1.00      1.00      1.00       142\n\n\n\n\n\n\nObviously this model perfectly predicted the data it was trained on, we will check now.\n\n\nCode\ny_pred=model.predict(X_test)\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\nsns.heatmap(cm_df, annot=True,)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.98      0.95        43\n           1       0.94      0.84      0.89        19\n\n    accuracy                           0.94        62\n   macro avg       0.94      0.91      0.92        62\nweighted avg       0.94      0.94      0.93        62\n\n\n\n\n\n\nThis performed fairly well, it most likely overfit though. We will tune the model later.\n\n\nCode\ndef plot_tree(model):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\n\n\n\nCode\nplot_tree(model)\n\n\n\n\n\n\n\nCode\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=4)\nregr_3 = DecisionTreeRegressor(max_depth=50)\nregr_1.fit(X_train, y_train)\nregr_2.fit(X_train, y_train)\nregr_3.fit(X_train, y_train)\nprint(\"MODEL-1: Training, test MSE:\",mean_squared_error(y_train, regr_1.predict(X_train)),mean_squared_error(y_test, regr_1.predict(X_test)))\nprint(\"MODEL-2: Training, test MSE:\",mean_squared_error(y_train, regr_2.predict(X_train)),mean_squared_error(y_test, regr_2.predict(X_test)))\nprint(\"MODEL-3: Training, test MSE:\",mean_squared_error(y_train, regr_3.predict(X_train)),mean_squared_error(y_test, regr_3.predict(X_test)))\n\n\nMODEL-1: Training, test MSE: 0.0021564653444053126 0.004572249112057642\nMODEL-2: Training, test MSE: 0.0007507994794317136 0.004495827804222975\nMODEL-3: Training, test MSE: 0.0 0.005443622831555732\n\n\nThe max depth of four seems to perform the best. Again, we can tune the depth later in our analysis. This performed better than the dummy regressor.\n\n\nCode\nplot_tree(regr_2)\n\n\n\n\n\n\n\nRandom Forest\nNow I will use a random forest model to see how well it does above the decision tree baseline.We will start with the classification task.\n\n\nCode\nX=recordClass.drop(['Typical Home Value'],axis=1)\ny=recordClass['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=RandomForestClassifier()\nmodel=model.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\nsns.heatmap(cm_df, annot=True,)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n              precision    recall  f1-score   support\n\n           0       0.93      1.00      0.97        43\n           1       1.00      0.84      0.91        19\n\n    accuracy                           0.95        62\n   macro avg       0.97      0.92      0.94        62\nweighted avg       0.95      0.95      0.95        62\n\n\n\n\n\n\nThis is a pretty good model and classifies well.\nNow lets create a regression random forest model.\n\n\nCode\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=RandomForestRegressor()\nmodel=model.fit(X_train,y_train)\nprint(\"MODEL: Training, test MSE:\",mean_squared_error(y_train, regr_1.predict(X_train)),mean_squared_error(y_test, regr_1.predict(X_test)))\n\n\nMODEL: Training, test MSE: 0.0021564653444053126 0.004572249112057642\n\n\nThe normal decision tree actually performed better. We can investigate this more.\n\n\nModel Tuning\nNow I will find the optimal paramaters for both the decision trees and random forests\n\n\nCode\nX=recordClass.drop(['Typical Home Value'],axis=1)\ny=recordClass['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\n\n\nCode\ntest_results=[]\ntrain_results=[]\nfor num_layer in range(1,20):\n    model=tree.DecisionTreeClassifier(max_depth=num_layer)\n    model=model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    y_pred_train=model.predict(X_train)\n    test_results.append([num_layer,accuracy_score(y_test,y_pred),recall_score(y_test,y_pred,pos_label=0),recall_score(y_test,y_pred,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(y_train,y_pred_train),recall_score(y_train,y_pred_train,pos_label=0),recall_score(y_train,y_pred_train,pos_label=1)])\n    \n\n\n\n\nCode\ntest_results=np.array(test_results)\ntrain_results=np.array(train_results)\n\n\n\n\nCode\ncol=1\nplt.plot(test_results[:, 0], test_results[:, col], '-or')\nplt.plot(train_results[:, 0], train_results[:, col], '-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('ACCURACY (Y=0): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\n\n\nCode\ncol=2\nplt.plot(test_results[:, 0], test_results[:, col], '-or')\nplt.plot(train_results[:, 0], train_results[:, col], '-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y=0): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\n\n\nCode\ncol=3\nplt.plot(test_results[:, 0], test_results[:, col], '-or')\nplt.plot(train_results[:, 0], train_results[:, col], '-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y=1): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\nLooks like optimal max_depth is 3.\n\n\nCode\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\n\n\nCode\nr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=4)\nregr_3 = DecisionTreeRegressor(max_depth=50)\nregr_1.fit(X_train, y_train)\nregr_2.fit(X_train, y_train)\nregr_3.fit(X_train, y_train)\nprint(\"MODEL-1: Training, test MSE:\",mean_squared_error(y_train, regr_1.predict(X_train)),mean_squared_error(y_test, regr_1.predict(X_test)))\nprint(\"MODEL-2: Training, test MSE:\",mean_squared_error(y_train, regr_2.predict(X_train)),mean_squared_error(y_test, regr_2.predict(X_test)))\nprint(\"MODEL-3: Training, test MSE:\",mean_squared_error(y_train, regr_3.predict(X_train)),mean_squared_error(y_test, regr_3.predict(X_test)))\n\n\nMODEL-1: Training, test MSE: 0.002156465344405312 0.004572249112057642\nMODEL-2: Training, test MSE: 0.0007507994794317137 0.004495827804222975\nMODEL-3: Training, test MSE: 0.0 0.005744803669638588\n\n\n\n\nCode\ntest_results=np.array(test_results)\ntrain_results=np.array(train_results)\n\n\n\n\nCode\ncol=1\nplt.plot(test_results[:, 0], test_results[:, col], '-or')\nplt.plot(train_results[:, 0], train_results[:, col], '-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('ACCURACY (Y=0): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\n\n\nCode\ncol=2\nplt.plot(test_results[:, 0], test_results[:, col], '-or')\nplt.plot(train_results[:, 0], train_results[:, col], '-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y=0): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\n\n\nCode\ncol=3\nplt.plot(test_results[:, 0], test_results[:, col], '-or')\nplt.plot(train_results[:, 0], train_results[:, col], '-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y=1): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\nLooks like 3 or 4 for the regression tree.\nNow on to the random forests.\n\n\nCode\nX=recordClass.drop(['Typical Home Value'],axis=1)\ny=recordClass['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=RandomForestClassifier()\n\n\n\n\nCode\nparam = {'n_estimators': randint(50,500),'max_depth': randint(1,20)}\nsearch = RandomizedSearchCV(model, param_distributions = param, n_iter=5, cv=5)\n\n\n\n\nCode\nsearch.fit(X_train, y_train)\n\n\nRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=5,\n                   param_distributions={'max_depth': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F535775F10&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F532392390&gt;})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=5,\n                   param_distributions={'max_depth': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F535775F10&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F532392390&gt;})estimator: RandomForestClassifierRandomForestClassifier()RandomForestClassifierRandomForestClassifier()\n\n\n\n\nCode\nbestParams = search.best_params_\nprint(f'Best Hyperparameters: {bestParams}')\n\n\nBest Hyperparameters: {'max_depth': 13, 'n_estimators': 419}\n\n\n\n\nCode\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=RandomForestRegressor()\n\n\n\n\nCode\nparam = {'n_estimators': randint(50,500),'max_depth': randint(1,20)}\nsearch = RandomizedSearchCV(model, param_distributions = param, n_iter=5, cv=5)\n\n\n\n\nCode\nsearch.fit(X_train, y_train)\n\n\nRandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=5,\n                   param_distributions={'max_depth': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F532694AD0&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F532695050&gt;})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=5,\n                   param_distributions={'max_depth': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F532694AD0&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F532695050&gt;})estimator: RandomForestRegressorRandomForestRegressor()RandomForestRegressorRandomForestRegressor()\n\n\n\n\nCode\nbestParams = search.best_params_\nprint(f'Best Hyperparameters: {bestParams}')\n\n\nBest Hyperparameters: {'max_depth': 15, 'n_estimators': 412}\n\n\n\n\nFinal Results\nNow lets train the models with our tuned hyper parameters and see their performance.\n\n\nCode\nX=recordClass.drop(['Typical Home Value'],axis=1)\ny=recordClass['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=tree.DecisionTreeClassifier(max_depth=3)\nmodel=model.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\nsns.heatmap(cm_df, annot=True,)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.86      0.89        43\n           1       0.73      0.84      0.78        19\n\n    accuracy                           0.85        62\n   macro avg       0.83      0.85      0.84        62\nweighted avg       0.86      0.85      0.86        62\n\n\n\n\n\n\nChanging the depth here doesnt change much.\n\n\nCode\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel = DecisionTreeRegressor(max_depth=4)\nmodel.fit(X_train, y_train)\nprint(\"MODEL: Training, test MSE:\",mean_squared_error(y_train, model.predict(X_train)),mean_squared_error(y_test, regr_1.predict(X_test)))\n\n\nMODEL: Training, test MSE: 0.0007507994794317137 0.004572249112057642\n\n\n\n\nCode\nX=recordClass.drop(['Typical Home Value'],axis=1)\ny=recordClass['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=RandomForestClassifier(max_depth= 13,n_estimators=419)\nmodel=model.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\nsns.heatmap(cm_df, annot=True,)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98        43\n           1       1.00      0.89      0.94        19\n\n    accuracy                           0.97        62\n   macro avg       0.98      0.95      0.96        62\nweighted avg       0.97      0.97      0.97        62\n\n\n\n\n\n\n\n\nCode\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=RandomForestRegressor(max_depth=15,n_estimators=412)\nmodel=model.fit(X_train,y_train)\nprint(\"MODEL: Training, test MSE:\",mean_squared_error(y_train, regr_1.predict(X_train)),mean_squared_error(y_test, regr_1.predict(X_test)))\n\n\nMODEL: Training, test MSE: 0.002156465344405312 0.004572249112057642\n\n\nMy final models did a fairly good job with predicting the percent increase in home value in states. With a random forest, the classification model only misclassfied three data points.My regression models still need some more work. The decision tree actually performed better than the random forest with the lowest mean squared error being .0046, this is compared to the baseline of .009. We posssibly could improve the regression decision tree and random forests be reducing the number of features as I suspect that is an issue."
  },
  {
    "objectID": "DataExploration.html",
    "href": "DataExploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Now I will attempt to get a better understanding of my record data through exploratory data analysis. By calculating and reporting basic summary statistics, I will know the central tendency, spread, and extremes of my variables. This can reveal flaws or inaccuracies in the data and provide us with insight into the distribution of the data. I will also create some data visualizations. There is a plethora of graphs I can make with all the variables I have. Since I already know what algorithms I will be using and what I want to achieve with my models, I will just create a few to show some general relationships and inspire other rabbit holes I could go down if I choose. Like I said before, some of my variables are closely related and correlated, so now we will finally visualize these correlations. Lastly, I will examine the outliers I have and decide what actions I should take with the outliers.\n\nDescriptive Statistics\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy import stats\ndf = pd.read_csv('./data/RecordData.csv')\ndf.head()\n\n\n\n\n\n\n\n\n\nYear\nTotal households\nMarried couple households\nMarried couple with children of the householder under 18 years\nMale householder,no spouse/partner present with children of the householder under 18 years\nFemale householder, no spouse/partner present with children of the householder under 18 years\nNumber of women 15 to 50 years old who had a birth in the past 12 months\nLess than 9th grade\n9th to 12th grade,no diploma\nHigh school graduate (includes equivalency)\n...\nTotal population\nSex ratio (males per 100 females)\nMedian Age\nRace-White\nRace-Black\nRace-American Indian and Alaska Native\nRace-Asian\nHispanic or Latino\nRegionName\nTypical Home Value\n\n\n\n\n0\n2018\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n...\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.047111\nAlabama\n0.038688\n\n\n1\n2019\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n...\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n0.036934\nAlabama\n0.070360\n\n\n2\n2021\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n...\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n0.082710\nAlabama\n0.263465\n\n\n3\n2022\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n...\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n-0.125037\nAlabama\n0.076587\n\n\n4\n2018\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n...\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.026241\nAlaska\n0.005789\n\n\n\n\n5 rows × 34 columns\n\n\n\n\n\nCode\npd.set_option('display.max_columns',None)\ndf.describe(include='all')\n\n\n\n\n\n\n\n\n\nYear\nTotal households\nMarried couple households\nMarried couple with children of the householder under 18 years\nMale householder,no spouse/partner present with children of the householder under 18 years\nFemale householder, no spouse/partner present with children of the householder under 18 years\nNumber of women 15 to 50 years old who had a birth in the past 12 months\nLess than 9th grade\n9th to 12th grade,no diploma\nHigh school graduate (includes equivalency)\nSome college, no degree\nAssociates degree\nBachelors degree\nNumber of people employed\nNumber of people unemployed\nMedian Household Income\nMean Household Income\nMedian earnings for male full-time, year-round workers\nMedian earnings for female full-time, year-round workers\nOccupied Housing Units\nTotal Housing Units\nMedian Rooms\nRenter Occupied Housing Units\nMean Rent Paid\nTotal population\nSex ratio (males per 100 females)\nMedian Age\nRace-White\nRace-Black\nRace-American Indian and Alaska Native\nRace-Asian\nHispanic or Latino\nRegionName\nTypical Home Value\n\n\n\n\ncount\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204\n204.000000\n\n\nunique\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n51\nNaN\n\n\ntop\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAlabama\nNaN\n\n\nfreq\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n4\nNaN\n\n\nmean\n2020.000000\n0.020044\n-0.050633\n-0.069625\n-0.089833\n-0.206925\n0.524127\n-0.109497\n-0.220982\n0.093385\n0.349233\n-0.113733\n0.205185\n0.012061\n-0.022911\n0.054935\n0.056360\n0.049179\n0.053833\n0.020044\n0.010245\n0.003344\n0.008161\n0.056447\n0.006469\n0.002941\n0.005769\n-0.021826\n0.001579\n0.059462\n0.019590\n0.019030\nNaN\n0.110084\n\n\nstd\n1.585028\n0.017131\n0.120031\n0.157685\n0.232427\n0.365501\n0.945060\n0.198461\n0.329615\n0.215643\n0.653535\n0.248473\n0.332987\n0.023825\n0.253585\n0.029235\n0.025530\n0.039958\n0.044326\n0.017131\n0.012175\n0.012598\n0.030682\n0.033730\n0.010885\n0.007600\n0.005859\n0.059540\n0.112276\n0.268675\n0.078313\n0.110523\nNaN\n0.099373\n\n\nmin\n2018.000000\n-0.009240\n-0.355454\n-0.490505\n-0.554143\n-0.888434\n-0.346730\n-0.656774\n-0.870279\n-0.112117\n-0.144863\n-0.869612\n-0.246902\n-0.083615\n-0.522982\n-0.023606\n-0.019416\n-0.050021\n-0.069051\n-0.009240\n-0.040806\n-0.050000\n-0.073401\n-0.019983\n-0.050583\n-0.020735\n-0.010390\n-0.383837\n-0.387319\n-0.578552\n-0.274559\n-0.476645\nNaN\n-0.016960\n\n\n25%\n2018.750000\n0.009339\n-0.065194\n-0.128874\n-0.290981\n-0.361259\n-0.017399\n-0.161193\n-0.355393\n-0.013111\n-0.016380\n-0.142606\n0.023646\n-0.001067\n-0.169444\n0.034074\n0.038350\n0.019872\n0.024355\n0.009339\n0.004823\n0.000000\n-0.009801\n0.031688\n-0.000028\n-0.001061\n0.002532\n-0.017285\n-0.032278\n-0.078743\n-0.013964\n0.000514\nNaN\n0.049067\n\n\n50%\n2020.000000\n0.016837\n0.004610\n-0.002334\n-0.026105\n-0.022791\n0.063992\n-0.047421\n-0.056749\n0.005985\n0.014486\n0.010017\n0.051750\n0.010269\n-0.067010\n0.054701\n0.051641\n0.043743\n0.047357\n0.016837\n0.007647\n0.000000\n0.007864\n0.052233\n0.004786\n0.002001\n0.005263\n-0.001994\n0.004109\n0.025383\n0.017288\n0.025703\nNaN\n0.066658\n\n\n75%\n2021.250000\n0.025940\n0.021921\n0.030698\n0.047844\n0.020779\n0.951449\n0.011528\n-0.004048\n0.052998\n0.231572\n0.035628\n0.153283\n0.025831\n0.052464\n0.070556\n0.071981\n0.073978\n0.077146\n0.025940\n0.013986\n0.016737\n0.025262\n0.078260\n0.012023\n0.007316\n0.008433\n0.004153\n0.026074\n0.104096\n0.049673\n0.045049\nNaN\n0.135000\n\n\nmax\n2022.000000\n0.096015\n0.083260\n0.210279\n0.571831\n0.310981\n4.711793\n0.583672\n0.233134\n1.166320\n3.713579\n0.200963\n1.860038\n0.081599\n0.985574\n0.155899\n0.139031\n0.187938\n0.199230\n0.096015\n0.107393\n0.039216\n0.130424\n0.176136\n0.063712\n0.024341\n0.026667\n0.042915\n0.940551\n1.598558\n0.394246\n0.716742\nNaN\n0.576048\n\n\n\n\n\n\n\nSome of the more niche variables, Number of women 15 to 50 years old who had a birth in the past 12 months, seem to be suspect as they have some very high maxes and standard deviation. We may just remove these columns completely later.\n\n\nCode\npd.reset_option('all')\n\n\n\n\nVisualizations\nI’ll start by just choosing a variable and state to compare against the target variable to find some relationships.\n\n\nCode\nAlabama_data = df[df['RegionName'] == 'Alabama']\nplt.figure(figsize=(10, 6))\nplt.plot(Alabama_data['Year'], Alabama_data['Typical Home Value'], label='Typical Home Value', marker='o', linestyle='-', color='blue', linewidth=2)\nplt.plot(Alabama_data['Year'], Alabama_data['Number of people employed'], label='Num People Employed', marker='s', linestyle='-', color='green', linewidth=2)\nplt.title(\"Percent Change of Typical Home Value and Number of People Employed in Alabama\", fontsize=16)\nplt.xlabel(\"Year\", fontsize=12)\nplt.ylabel(\"Percent Change\", fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\nThese two variables almost seem to be negatively correlated. This is the opposite of what I expected as I usually equate more people employed to a stronger economy and higher home prices. Let us see these two relationships in some other states.\n\n\nCode\nNewYork_data = df[df['RegionName'] == 'New York']\nplt.figure(figsize=(10, 6))\nplt.plot(NewYork_data['Year'], NewYork_data['Typical Home Value'], label='Typical Home Value', marker='o', linestyle='-', color='blue', linewidth=2)\nplt.plot(NewYork_data['Year'], NewYork_data['Number of people employed'], label='Num People Employed', marker='s', linestyle='-', color='green', linewidth=2)\nplt.title(\"Percent Change of Typical Home Value and Number of People Employed in New York\", fontsize=16)\nplt.xlabel(\"Year\", fontsize=12)\nplt.ylabel(\"Percent Change\", fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\nWe see the same trends in New York as well.\n\n\nCode\nMichigan_data = df[df['RegionName'] == 'Michigan']\nplt.figure(figsize=(10, 6))\nplt.plot(Michigan_data['Year'], Michigan_data['Typical Home Value'], label='Typical Home Value', marker='o', linestyle='-', color='blue', linewidth=2)\nplt.plot(Michigan_data['Year'], Michigan_data['Number of people employed'], label='Num People Employed', marker='s', linestyle='-', color='green', linewidth=2)\nplt.title(\"Percent Change of Typical Home Value and Number of People Employed in Michigan\", fontsize=16)\nplt.xlabel(\"Year\", fontsize=12)\nplt.ylabel(\"Percent Change\", fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\nAgain we see it here. This is why EDA is crucial if you would like to uncover hidden truths about the data. Although we would guess employment and home value were positively correlated, this is not the case here. This may be due to COVID turning the world on its head as many unprecedented changes happened during that time. This could be detrimental since I want to predict home values under normal conditions.\nNow I will make some box plots of some variables with potentially problematic values, prepping for the next section.\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=df[['Some college, no degree']], color='skyblue')\nplt.title('Box Plot of Percent Change in People with Some College, No Degree', fontsize=16)\nplt.ylabel('Percent Change', fontsize=12)\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.set(style='whitegrid')\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=df[['High school graduate (includes equivalency)']], color='skyblue')\nplt.title('Box Plot of Percent Change in High School Graduates (includes equivalency)', fontsize=16)\nplt.ylabel('Percent Change', fontsize=12)\nplt.show()\n\n\n\n\n\n\n\nCode\nsns.set(style='whitegrid')\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=df[['Number of women 15 to 50 years old who had a birth in the past 12 months']], color='skyblue')\nplt.title('Box Plot of Percent Change in Number of Women 15 to 50 years Old Who Had a Birth in the Past 12 months', fontsize=16)\nplt.ylabel('Percent Change', fontsize=12)\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=df[['High school graduate (includes equivalency)']], color='skyblue')\nplt.title('Box Plot of Highschool Graduate Percent Change')\nplt.ylabel('Percent Change')\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=df[['Typical Home Value']], color='skyblue')\nplt.title('TBox Plot of ypical Home Value Percent Change')\nplt.ylabel('Percent Change')\nplt.show()\n\n\n\n\n\nAs you can see, a reoccurring theme with these box plots are the many outliers, even with my target variable, especially a few data points that deviate very far from the median. I do not want to get rid of most of the outliers because I think they still have value, but the problematic points need to be removed so my analysis is not negatively affected.\n\n\nOutliers\nHere I will make a data frame that detects outliers in each column that have a Z-score greater than 3.5 and outputs the row to find some problematic data points.\n\n\nCode\ndf2=df.drop(['Year','RegionName'],axis=1)\nz=stats.zscore(df2)\noutliers=df[(abs(z)&gt;3.5).any(axis=1)]\n\n\n\n\nCode\noutliers\n\n\n\n\n\n\n\n\n\nYear\nTotal households\nMarried couple households\nMarried couple with children of the householder under 18 years\nMale householder,no spouse/partner present with children of the householder under 18 years\nFemale householder, no spouse/partner present with children of the householder under 18 years\nNumber of women 15 to 50 years old who had a birth in the past 12 months\nLess than 9th grade\n9th to 12th grade,no diploma\nHigh school graduate (includes equivalency)\n...\nTotal population\nSex ratio (males per 100 females)\nMedian Age\nRace-White\nRace-Black\nRace-American Indian and Alaska Native\nRace-Asian\nHispanic or Latino\nRegionName\nTypical Home Value\n\n\n\n\n10\n2021\n0.055153\n0.056519\n0.063510\n-0.009826\n0.015737\n-0.060886\n-0.074551\n-0.124297\n-0.002245\n...\n-0.000330\n0.005051\n0.007833\n-0.256084\n-0.049722\n-0.113205\n0.014744\n0.017542\nArizona\n0.465593\n\n\n18\n2021\n0.020610\n0.004144\n-0.013885\n0.047402\n0.002291\n0.016327\n0.014671\n-0.077256\n0.005821\n...\n-0.006944\n0.012146\n0.016216\n-0.342397\n-0.052871\n0.667543\n0.017400\n0.011539\nCalifornia\n0.263897\n\n\n34\n2021\n0.083419\n0.083260\n0.110537\n0.092350\n0.068272\n0.025547\n-0.030368\n-0.152002\n-0.007428\n...\n0.014126\n0.012552\n0.009434\n-0.237309\n-0.045173\n0.066859\n0.034113\n0.029537\nFlorida\n0.326356\n\n\n42\n2021\n0.053258\n0.069152\n0.116342\n0.055582\n-0.143698\n0.002767\n-0.064805\n-0.035668\n-0.015822\n...\n0.018138\n0.013000\n0.015152\n-0.063111\n0.057980\n-0.210801\n-0.021324\n0.061446\nHawaii\n0.216170\n\n\n43\n2022\n0.009686\n-0.029597\n-0.055693\n0.103088\n0.310981\n-0.138189\n-0.045804\n0.054461\n0.017893\n...\n-0.000941\n0.001974\n0.014925\n-0.001749\n-0.086856\n-0.152367\n-0.054849\n-0.476645\nHawaii\n0.119270\n\n\n45\n2019\n0.024348\n-0.183459\n-0.220651\n-0.411395\n-0.851894\n4.711793\n-0.446483\n-0.795935\n0.045187\n...\n0.018730\n-0.001976\n0.002717\n0.012729\n0.052422\n-0.006905\n-0.039110\n0.031601\nIdaho\n0.108631\n\n\n46\n2021\n0.057974\n0.079034\n0.073181\n0.571831\n-0.045821\n-0.150307\n0.138723\n0.079400\n0.108126\n...\n0.063712\n-0.001980\n0.010840\n-0.030580\n0.120161\n0.049110\n0.001232\n0.097702\nIdaho\n0.576048\n\n\n50\n2021\n0.025819\n0.021762\n0.029648\n0.059900\n-0.051231\n0.006543\n0.027617\n-0.071533\n-0.026874\n...\n-0.000028\n0.015576\n0.010363\n-0.140766\n-0.037765\n1.598558\n0.014678\n0.027537\nIllinois\n0.168975\n\n\n72\n2018\n0.054252\n0.037673\n0.050175\n-0.070806\n0.065664\n-0.024363\n-0.124215\n0.028409\n0.086337\n...\n0.001869\n0.000000\n0.011211\n-0.000125\n0.157708\n-0.082869\n0.034703\n0.023380\nMaine\n0.064244\n\n\n75\n2022\n0.019730\n0.015848\n-0.026318\n0.199123\n-0.149499\n0.034946\n-0.098515\n0.041105\n0.033876\n...\n0.009541\n0.013485\n0.008949\n0.013462\n0.049219\n0.474843\n-0.032864\n0.716742\nMaine\n0.106837\n\n\n101\n2019\n0.014441\n-0.195616\n-0.357689\n-0.411364\n-0.874712\n2.332518\n-0.618755\n-0.851116\n0.170165\n...\n0.006093\n-0.003945\n0.009975\n-0.001078\n0.540150\n-0.006028\n0.048139\n-0.030693\nMontana\n0.056170\n\n\n103\n2022\n0.033685\n0.019739\n0.064245\n0.001460\n0.205620\n0.375366\n0.583672\n-0.088329\n0.058703\n...\n0.016840\n-0.010659\n0.002494\n0.017740\n-0.357706\n-0.056952\n-0.228635\n0.032370\nMontana\n0.098459\n\n\n110\n2021\n0.041820\n0.031044\n0.055005\n0.048530\n0.030733\n-0.005101\n0.174745\n-0.107987\n0.006687\n...\n0.020725\n0.010934\n0.007813\n-0.215727\n-0.027236\n0.052680\n0.033282\n0.044593\nNevada\n0.312454\n\n\n122\n2021\n0.051154\n0.079736\n0.070533\n0.149218\n-0.072579\n-0.124290\n-0.103086\n-0.094506\n-0.010343\n...\n0.009084\n0.011247\n0.012953\n-0.383837\n-0.114053\n-0.011588\n-0.049523\n0.025448\nNew Mexico\n0.279211\n\n\n126\n2021\n0.027643\n0.014309\n-0.005062\n0.004763\n-0.047201\n-0.018414\n0.034458\n-0.053481\n-0.032565\n...\n0.019655\n0.011640\n0.015306\n-0.103374\n-0.088169\n0.627134\n0.028334\n0.030694\nNew York\n0.182680\n\n\n133\n2019\n0.013039\n-0.191167\n-0.231652\n-0.397627\n-0.884407\n3.664530\n-0.382714\n-0.847600\n0.228239\n...\n0.002612\n-0.020735\n0.002825\n0.003411\n-0.140562\n-0.007269\n-0.224016\n0.110844\nNorth Dakota\n0.036121\n\n\n149\n2019\n0.009528\n-0.266410\n-0.349068\n-0.442714\n-0.835774\n1.914363\n-0.486150\n-0.827213\n1.166320\n...\n-0.000396\n0.001042\n0.000000\n-0.006074\n0.018445\n0.264731\n-0.003000\n0.023664\nPennsylvania\n0.054333\n\n\n154\n2021\n0.081037\n0.083137\n0.080883\n-0.363321\n0.022460\n0.458082\n-0.023583\n0.110515\n0.010123\n...\n0.034218\n0.006289\n0.012469\n-0.069564\n-0.303738\n-0.013170\n-0.079701\n0.087661\nRhode Island\n0.261654\n\n\n160\n2018\n0.003454\n0.029545\n0.017244\n-0.000755\n-0.071035\n0.034423\n-0.063419\n0.021516\n-0.081849\n...\n0.014453\n0.001967\n0.008130\n0.005823\n0.121858\n0.026157\n0.394246\n0.086483\nSouth Dakota\n0.042984\n\n\n170\n2021\n0.081233\n0.078094\n0.081198\n0.062968\n0.020952\n0.111101\n0.016162\n-0.066879\n-0.002312\n...\n0.018350\n0.012183\n0.011396\n-0.335800\n-0.008021\n0.571214\n0.072469\n0.028855\nTexas\n0.269736\n\n\n173\n2019\n0.024992\n-0.167414\n-0.191234\n-0.336425\n-0.789190\n4.233353\n-0.506266\n-0.796194\n-0.043176\n...\n0.014189\n-0.004926\n0.006452\n0.033724\n-0.085396\n0.019897\n0.007536\n0.026285\nUtah\n0.069457\n\n\n177\n2019\n0.005333\n-0.223165\n-0.330161\n-0.269993\n-0.876571\n2.478238\n-0.535917\n-0.832822\n0.639239\n...\n-0.003688\n0.014463\n-0.006961\n-0.007161\n0.238027\n0.679862\n-0.274559\n-0.003373\nVermont\n0.031649\n\n\n178\n2021\n0.028147\n0.080267\n0.105794\n-0.243446\n-0.071089\n-0.122692\n-0.188336\n-0.179850\n-0.062629\n...\n0.034586\n0.003055\n0.002336\n0.005170\n-0.245363\n-0.578552\n0.318726\n0.064313\nVermont\n0.263214\n\n\n179\n2022\n0.025640\n0.003709\n-0.015261\n0.395730\n-0.194364\n0.201600\n-0.007340\n-0.097474\n0.005142\n...\n0.002314\n-0.002030\n0.006993\n-0.000163\n-0.112360\n0.050243\n0.032261\n0.631380\nVermont\n0.091870\n\n\n189\n2019\n-0.008885\n-0.236678\n-0.335715\n-0.388981\n-0.871107\n1.649110\n-0.494669\n-0.781250\n1.090989\n...\n-0.007578\n-0.005092\n0.002336\n-0.007006\n-0.040100\n0.369722\n0.063303\n0.049044\nWest Virginia\n0.015365\n\n\n190\n2021\n-0.008204\n-0.019896\n-0.027457\n-0.184190\n0.148089\n-0.145993\n-0.234619\n-0.096763\n-0.039035\n...\n-0.005127\n0.014330\n-0.002331\n-0.030346\n-0.229882\n-0.367676\n-0.014325\n0.131492\nWest Virginia\n0.202372\n\n\n191\n2022\n0.019579\n-0.020436\n0.013439\n0.550396\n-0.059651\n0.075458\n0.055790\n-0.065771\n-0.022501\n...\n-0.004376\n-0.003027\n0.002336\n-0.008671\n0.197499\n0.241107\n-0.047427\n0.474959\nWest Virginia\n0.079937\n\n\n196\n2018\n0.019735\n-0.008194\n-0.073422\n-0.121206\n0.043319\n0.431683\n-0.029365\n-0.012097\n0.057379\n...\n-0.002724\n-0.016299\n0.026667\n0.000074\n-0.364031\n0.160714\n0.201780\n-0.003906\nWyoming\n0.023767\n\n\n197\n2019\n0.012491\n-0.145165\n-0.230388\n-0.224044\n-0.888434\n1.387621\n-0.656774\n-0.870279\n0.131951\n...\n0.001769\n0.020468\n-0.010390\n-0.004907\n0.940551\n-0.148965\n-0.202170\n0.012298\nWyoming\n0.063963\n\n\n201\n2019\n0.014241\n-0.355454\n-0.490505\n-0.365204\n-0.869834\n0.977101\n-0.287304\n-0.701942\n0.298245\n...\n0.004689\n-0.008811\n0.011799\n0.011158\n0.002899\n-0.023810\n0.042995\n0.002877\nDistrict of Columbia\n0.017659\n\n\n202\n2021\n0.096015\n-0.010890\n0.155305\n-0.343210\n0.020759\n-0.059821\n-0.015675\n-0.257760\n-0.112117\n...\n-0.050583\n0.010000\n0.014577\n-0.154024\n-0.097916\n-0.320785\n-0.035861\n-0.032185\nDistrict of Columbia\n0.097177\n\n\n203\n2022\n0.023172\n0.000929\n-0.045818\n-0.211063\n-0.092319\n0.055245\n-0.370224\n0.098800\n-0.089982\n...\n0.002616\n-0.001100\n0.002874\n0.016235\n-0.023242\n1.443404\n0.009714\n-0.084908\nDistrict of Columbia\n-0.009417\n\n\n\n\n32 rows × 34 columns\n\n\n\nConsidering how little data I have, this is a lot of rows to drop. I will keep them for now and possibly for the rest of my analysis. If I repeat this with more data I will be sure to drop the outliers.\n\n\nCorrelation\n\n\nCode\ndf2.corr()\n\n\n\n\n\n\n\n\n\nTotal households\nMarried couple households\nMarried couple with children of the householder under 18 years\nMale householder,no spouse/partner present with children of the householder under 18 years\nFemale householder, no spouse/partner present with children of the householder under 18 years\nNumber of women 15 to 50 years old who had a birth in the past 12 months\nLess than 9th grade\n9th to 12th grade,no diploma\nHigh school graduate (includes equivalency)\nSome college, no degree\n...\nMean Rent Paid\nTotal population\nSex ratio (males per 100 females)\nMedian Age\nRace-White\nRace-Black\nRace-American Indian and Alaska Native\nRace-Asian\nHispanic or Latino\nTypical Home Value\n\n\n\n\nTotal households\n1.000000\n0.438800\n0.465624\n0.248086\n0.336708\n-0.260457\n0.312382\n0.287555\n-0.294137\n-0.318927\n...\n0.421822\n0.390172\n0.331799\n0.277745\n-0.536511\n-0.153728\n0.009718\n0.049971\n0.088902\n0.578869\n\n\nMarried couple households\n0.438800\n1.000000\n0.981878\n0.790430\n0.945954\n-0.831271\n0.757746\n0.931202\n-0.766019\n-0.920144\n...\n0.497780\n0.276947\n0.331144\n0.021856\n-0.274077\n-0.166218\n0.019215\n0.078385\n0.021463\n0.444488\n\n\nMarried couple with children of the householder under 18 years\n0.465624\n0.981878\n1.000000\n0.771685\n0.943687\n-0.815491\n0.781559\n0.917415\n-0.774410\n-0.906359\n...\n0.499087\n0.241894\n0.325107\n0.017160\n-0.278718\n-0.198401\n-0.003718\n0.081323\n0.027813\n0.447811\n\n\nMale householder,no spouse/partner present with children of the householder under 18 years\n0.248086\n0.790430\n0.771685\n1.000000\n0.797750\n-0.742020\n0.682252\n0.800932\n-0.621534\n-0.745090\n...\n0.412152\n0.194073\n0.243893\n0.024614\n-0.212158\n-0.045523\n0.056621\n0.111638\n0.158003\n0.364732\n\n\nFemale householder, no spouse/partner present with children of the householder under 18 years\n0.336708\n0.945954\n0.943687\n0.797750\n1.000000\n-0.894087\n0.803087\n0.972320\n-0.749640\n-0.896076\n...\n0.438733\n0.151822\n0.228283\n0.085614\n-0.219316\n-0.214236\n0.019388\n0.115063\n-0.051127\n0.338099\n\n\nNumber of women 15 to 50 years old who had a birth in the past 12 months\n-0.260457\n-0.831271\n-0.815491\n-0.742020\n-0.894087\n1.000000\n-0.711113\n-0.893892\n0.592598\n0.785771\n...\n-0.411501\n-0.118924\n-0.275845\n-0.072924\n0.213962\n0.149411\n-0.005464\n-0.171662\n0.015633\n-0.327840\n\n\nLess than 9th grade\n0.312382\n0.757746\n0.781559\n0.682252\n0.803087\n-0.711113\n1.000000\n0.782436\n-0.655789\n-0.724942\n...\n0.460252\n0.192067\n0.237910\n0.159691\n-0.212471\n-0.298075\n0.009762\n0.029356\n-0.046078\n0.297504\n\n\n9th to 12th grade,no diploma\n0.287555\n0.931202\n0.917415\n0.800932\n0.972320\n-0.893892\n0.782436\n1.000000\n-0.752483\n-0.875352\n...\n0.424624\n0.179373\n0.191868\n0.090643\n-0.152399\n-0.195105\n0.055358\n0.137906\n-0.017625\n0.290155\n\n\nHigh school graduate (includes equivalency)\n-0.294137\n-0.766019\n-0.774410\n-0.621534\n-0.749640\n0.592598\n-0.655789\n-0.752483\n1.000000\n0.625286\n...\n-0.423344\n-0.185994\n-0.201587\n-0.058643\n0.172882\n0.132303\n0.004590\n-0.111726\n0.009065\n-0.296042\n\n\nSome college, no degree\n-0.318927\n-0.920144\n-0.906359\n-0.745090\n-0.896076\n0.785771\n-0.724942\n-0.875352\n0.625286\n1.000000\n...\n-0.410235\n-0.127849\n-0.290762\n-0.002780\n0.222860\n0.165423\n-0.042741\n-0.026843\n0.002079\n-0.339538\n\n\nAssociates degree\n0.317707\n0.947215\n0.930499\n0.808561\n0.954101\n-0.873807\n0.742061\n0.942365\n-0.734199\n-0.921569\n...\n0.421006\n0.202411\n0.261304\n0.052382\n-0.213265\n-0.163936\n0.034185\n0.103434\n0.015772\n0.368901\n\n\nBachelors degree\n-0.212064\n-0.779303\n-0.778599\n-0.715225\n-0.855243\n0.855915\n-0.739918\n-0.865743\n0.584652\n0.712947\n...\n-0.360112\n-0.060273\n-0.220864\n-0.049123\n0.183743\n0.210422\n-0.048662\n-0.174529\n0.027426\n-0.245080\n\n\nNumber of people employed\n-0.164870\n-0.003424\n-0.035541\n0.058006\n0.031240\n0.019247\n0.050115\n0.095316\n-0.012144\n0.016294\n...\n0.180419\n0.199823\n-0.381214\n-0.100719\n0.501511\n0.082050\n0.009338\n0.120333\n-0.263606\n-0.247353\n\n\nNumber of people unemployed\n0.474298\n0.179621\n0.192406\n0.019238\n0.081436\n-0.064336\n0.111410\n0.031265\n-0.108437\n-0.100239\n...\n0.128883\n0.342345\n0.444477\n0.206341\n-0.617429\n-0.175998\n0.068743\n-0.036782\n0.222112\n0.549155\n\n\nMedian Household Income\n-0.024458\n-0.004581\n-0.021028\n-0.010217\n-0.068167\n0.019591\n-0.012365\n-0.070054\n0.021158\n0.063929\n...\n0.437673\n0.252681\n0.059726\n-0.224007\n0.052497\n-0.076748\n-0.040125\n0.118803\n-0.214884\n0.242465\n\n\nMean Household Income\n0.129516\n0.169651\n0.165921\n0.104215\n0.115443\n-0.128515\n0.078931\n0.114442\n-0.101892\n-0.098668\n...\n0.542114\n0.318451\n0.082443\n-0.122832\n-0.061203\n-0.164692\n-0.135585\n0.213101\n-0.159560\n0.450604\n\n\nMedian earnings for male full-time, year-round workers\n0.521425\n0.358641\n0.380646\n0.239736\n0.267579\n-0.248847\n0.222373\n0.209678\n-0.291935\n-0.254937\n...\n0.402790\n0.242736\n0.490989\n0.080710\n-0.520318\n-0.167742\n-0.061886\n0.058246\n0.141308\n0.613716\n\n\nMedian earnings for female full-time, year-round workers\n0.527228\n0.396203\n0.417353\n0.297018\n0.314391\n-0.319496\n0.320358\n0.255450\n-0.255274\n-0.341490\n...\n0.449579\n0.233298\n0.499738\n0.025937\n-0.598228\n-0.333074\n-0.037168\n-0.015659\n0.071689\n0.681039\n\n\nOccupied Housing Units\n1.000000\n0.438800\n0.465624\n0.248086\n0.336708\n-0.260457\n0.312382\n0.287555\n-0.294137\n-0.318927\n...\n0.421822\n0.390172\n0.331799\n0.277745\n-0.536511\n-0.153728\n0.009718\n0.049971\n0.088902\n0.578869\n\n\nTotal Housing Units\n0.686693\n0.172911\n0.203209\n0.019719\n0.110703\n-0.051470\n0.155896\n0.083528\n-0.148170\n-0.090230\n...\n0.229018\n0.221057\n0.102697\n0.222890\n-0.283962\n0.003257\n0.108058\n0.029278\n-0.099109\n0.208816\n\n\nMedian Rooms\n0.196957\n0.217412\n0.214178\n0.151148\n0.150782\n-0.132356\n0.065169\n0.121083\n-0.129029\n-0.188487\n...\n0.135262\n0.309887\n0.268522\n0.065965\n-0.321603\n-0.235029\n-0.039576\n0.070796\n0.210841\n0.502435\n\n\nRenter Occupied Housing Units\n0.368283\n0.063216\n0.090782\n0.093202\n0.093006\n-0.040640\n0.135533\n0.109633\n-0.004732\n-0.068992\n...\n0.088938\n-0.028178\n-0.058786\n0.149449\n0.031237\n0.027316\n0.028427\n0.026458\n-0.041721\n-0.124036\n\n\nMean Rent Paid\n0.421822\n0.497780\n0.499087\n0.412152\n0.438733\n-0.411501\n0.460252\n0.424624\n-0.423344\n-0.410235\n...\n1.000000\n0.420228\n0.334809\n-0.060136\n-0.268605\n-0.180041\n-0.021600\n0.112019\n-0.076343\n0.570452\n\n\nTotal population\n0.390172\n0.276947\n0.241894\n0.194073\n0.151822\n-0.118924\n0.192067\n0.179373\n-0.185994\n-0.127849\n...\n0.420228\n1.000000\n0.216560\n-0.059961\n-0.053172\n-0.080334\n0.018665\n0.225440\n0.131966\n0.544573\n\n\nSex ratio (males per 100 females)\n0.331799\n0.331144\n0.325107\n0.243893\n0.228283\n-0.275845\n0.237910\n0.191868\n-0.201587\n-0.290762\n...\n0.334809\n0.216560\n1.000000\n-0.016104\n-0.420289\n-0.006381\n-0.009656\n-0.131272\n0.118501\n0.539996\n\n\nMedian Age\n0.277745\n0.021856\n0.017160\n0.024614\n0.085614\n-0.072924\n0.159691\n0.090643\n-0.058643\n-0.002780\n...\n-0.060136\n-0.059961\n-0.016104\n1.000000\n-0.230943\n-0.119643\n0.039093\n0.032161\n-0.041476\n0.028251\n\n\nRace-White\n-0.536511\n-0.274077\n-0.278718\n-0.212158\n-0.219316\n0.213962\n-0.212471\n-0.152399\n0.172882\n0.222860\n...\n-0.268605\n-0.053172\n-0.420289\n-0.230943\n1.000000\n0.168399\n-0.169048\n0.027705\n-0.093049\n-0.589996\n\n\nRace-Black\n-0.153728\n-0.166218\n-0.198401\n-0.045523\n-0.214236\n0.149411\n-0.298075\n-0.195105\n0.132303\n0.165423\n...\n-0.180041\n-0.080334\n-0.006381\n-0.119643\n0.168399\n1.000000\n0.039821\n-0.071010\n-0.043821\n-0.254423\n\n\nRace-American Indian and Alaska Native\n0.009718\n0.019215\n-0.003718\n0.056621\n0.019388\n-0.005464\n0.009762\n0.055358\n0.004590\n-0.042741\n...\n-0.021600\n0.018665\n-0.009656\n0.039093\n-0.169048\n0.039821\n1.000000\n-0.060736\n0.038923\n-0.066153\n\n\nRace-Asian\n0.049971\n0.078385\n0.081323\n0.111638\n0.115063\n-0.171662\n0.029356\n0.137906\n-0.111726\n-0.026843\n...\n0.112019\n0.225440\n-0.131272\n0.032161\n0.027705\n-0.071010\n-0.060736\n1.000000\n0.025958\n0.079361\n\n\nHispanic or Latino\n0.088902\n0.021463\n0.027813\n0.158003\n-0.051127\n0.015633\n-0.046078\n-0.017625\n0.009065\n0.002079\n...\n-0.076343\n0.131966\n0.118501\n-0.041476\n-0.093049\n-0.043821\n0.038923\n0.025958\n1.000000\n0.198465\n\n\nTypical Home Value\n0.578869\n0.444488\n0.447811\n0.364732\n0.338099\n-0.327840\n0.297504\n0.290155\n-0.296042\n-0.339538\n...\n0.570452\n0.544573\n0.539996\n0.028251\n-0.589996\n-0.254423\n-0.066153\n0.079361\n0.198465\n1.000000\n\n\n\n\n32 rows × 32 columns\n\n\n\n\n\nCode\nplt.figure(figsize=(12, 8)) \nsns.heatmap(df2.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\n\nMany Variables are highly correlated. More on this in the dimensionality reduction section."
  },
  {
    "objectID": "ARM.html",
    "href": "ARM.html",
    "title": "ARM",
    "section": "",
    "text": "Methods"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering my data may reveal if certain states or years may be grouped together. This can reveal trends in my data set, like if certain regions are developing in similar ways.\n\nKMEAN\nK-means Clustering separates an unlabeled dataset into distinct clusters. In this method, the parameter K signifies the predetermined number of clusters to be formed during the process, or another way to say it, the number of centroids (means).For instance, if K=2, the data will be grouped into two clusters, and if K=3, there will be three clusters and three centroids, ect. Every data point is then assigned to each of the clusters to reduce the in-cluster sum of squares  The process goes as follows. First you initialize the number of random centroids. You can determine this figure with model selection methods which I will talk about below. Then, you compute the distance from each data point to each centroid and assign the point to the nearest centroid. Then, you compute the centroids as the mean points of the cluster and repeat computing distance, reassigning points, recomputing centroids until convergence.\n\n\nDBSAN\nWhile K-means clustering is better for normal shaped data, density based clustering is more efficient for oddly shaped data. The underlying concept is that a cluster in the data space is characterized by a region with high point density, and it is separated from other clusters by regions with low point density. The two parameters required for DBSCAN are epsilon and minimum points, where epsilon is basically the maximum distance from another point for two data points to be considered “neighbors”, and minimum points is the minimum number of neighbors a data point must have to be considered a core point.  The process goes as follows. Find the neighbor points and identify the core points. For each core point, assign it to a cluster if its not already. Then you find all the connected points and assign them to the same cluster as the core point. Its like a chain, in that if there are a,b, and c data points, if a and b are neighbors, and c and b are neighbors, then a and c are then neighbors. Then you iterate through the unvisited points again.\n\n\nHierarchical Clustering\nHierarchical clustering builds a hierarchy of clusters in the from of a dendrogram, allowing us to visualize the relationships between data points. There are top down and bottom up algorithms, where bottom up treats each data point as a single cluster then merges the clusters together until there is one big cluster. Top down does the opposite.\nThe bottom up process goes as follows. Assign each data point as a single cluster. Then, find the closet pair of clusters and make them one cluster. Then, find the two closest clusters and make them one cluster. Repeat merging the clusters together until one cluster remains.\n\n\nModel Selection Methods\nThese methods aid in determining the optimal number of clusters.\nElbow: Inertia is the sum of squared distance of samples to their closest cluster, or put simply a measure of how “well clustered” a dataset is. You want low inertia and low k. In the elbow method, you plot inertia against the number of clusters and find the k value where the decrease in inertia begins to significantly slow.\nSilhouette Score: The silhouette score measures similarity of an object to its own cluster in comparison to other clusters. A higher silhouette score indicates well-separated clusters and helps you determine the number of clusters.\n\n\nData Selection\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.decomposition import PCA\nrecord=pd.read_csv('data/RecordData.csv')\nrecord=record.drop(['Year','RegionName'],axis=1)\nrecord.head()\n\n\n\n\n\n\n\n\n\nTotal households\nMarried couple households\nMarried couple with children of the householder under 18 years\nMale householder,no spouse/partner present with children of the householder under 18 years\nFemale householder, no spouse/partner present with children of the householder under 18 years\nNumber of women 15 to 50 years old who had a birth in the past 12 months\nLess than 9th grade\n9th to 12th grade,no diploma\nHigh school graduate (includes equivalency)\nSome college, no degree\n...\nMean Rent Paid\nTotal population\nSex ratio (males per 100 females)\nMedian Age\nRace-White\nRace-Black\nRace-American Indian and Alaska Native\nRace-Asian\nHispanic or Latino\nTypical Home Value\n\n\n\n\n0\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n0.033732\n...\n0.050667\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.047111\n0.038688\n\n\n1\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n1.429793\n...\n0.024112\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n0.036934\n0.070360\n\n\n2\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n0.007408\n...\n0.066914\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n0.082710\n0.263465\n\n\n3\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n0.014854\n...\n0.060395\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n-0.125037\n0.076587\n\n\n4\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n-0.065209\n...\n-0.019983\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.026241\n0.005789\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\nHyperparameter Tuning\n\n\nCode\nscaler=StandardScaler()\nscaled=scaler.fit_transform(record)\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(scaled)\nagg_cluster = AgglomerativeClustering(n_clusters=5, linkage='ward')\nagg_labels = agg_cluster.fit_predict(record)\nplt.figure(figsize=(8, 5))\nplt.scatter(components[:, 0], components[:, 1], c=agg_labels, cmap='plasma')\nplt.title('Agglomerative Clustering')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.colorbar(label='Cluster')\nplt.show()\n\n\n\n\n\n\n\nCode\ninertia = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(record)\n    inertia.append(kmeans.inertia_)\n\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, 11), inertia, marker='o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal k')\nplt.show()\n\n\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\n\n\n\n\n\nCode\ndbscan = DBSCAN(eps=0.5, min_samples=5) \ndbscan_labels = dbscan.fit_predict(scaled)\nplt.figure(figsize=(8, 5))\nplt.scatter(components[:, 0], components[:, 1], c=dbscan_labels, cmap='viridis')\nplt.title('DBSCAN Clustering')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.colorbar(label='Cluster')\nplt.show()\n\n\n\n\n\n\n\nResults\nIt seems that hierarchical clustering had the most meaningful results. I think there are a lot of connections to be made in my data because it is based on years and states. Obviously some years there will be higher percent changes overall, and states close to each other most likely move in similar directions.\n\n\nConclusion\nIt is difficult to conclude much from this analysis as clustering does not apply too much to my project."
  },
  {
    "objectID": "DataCleaning.html",
    "href": "DataCleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Record Data Cleaning Methods\nIn this section I will clean and merge all my record data so that I will be left with one table that I will use to build my models. I will also reconfigure my data so it will be suitable for my analysis.\nImportant: It is at this point I discovered my project was based on time series data. For the rest of the project I needed my data to follow the IID assumption (independent and identically distributed). The time series data I have is not independent, so I pivoted how I would use my data. An observational unit in my case is a specific state coupled with a specific year, like Alabama-2019. The target variable, or variable I am trying to predict, is the percent increase in home value from one year to the next. Below I will explain what a single row of my finished table means more thoroughly. Although all my data is not independent, so you will need to be wary of the validity of some of my conclusions, you can still gain useful information about real estate trends from my analysis and models I construct.\nIn the code below, you will see me read in specific columns from each table, change column names to accurately represent what is in the column, merge and concatenate all the tables together as one, drop NA values that result from the merges, convert each cell to percent change from the previous year, and check for potential problems with the data, specifically outliers.\n\n\nRecord Data Cleaning Code\n\n\nCode\nimport requests\nimport json\nimport re\nimport pandas as pd\nDP02cols=['NAME','DP02_0001E','DP02_0002E','DP02_0003E','DP02_0007E','DP02_0011E','DP02_0037E','DP02_0060E','DP02_0061E','DP02_0062E','DP02_0063E','DP02_0064E','DP02_0065E']\nDP03cols=['NAME','DP03_0026PE','DP03_0005E','DP03_0062E','DP03_0063E','DP03_0093E','DP03_0094E']\nDP04cols=['NAME','DP04_0002E','DP04_0001E','DP04_0037E','DP04_0047E','DP04_0134E']\nDP05cols=['NAME','DP05_0001E','DP05_0004E','DP05_0018E','DP05_0037E','DP05_0038E','DP05_0039E','DP05_0044E','DP05_0071E']\n\nDP0217=pd.read_csv('data/2017DP02.csv',skiprows=1)\nDP0218=pd.read_csv('data/2018DP02.csv',skiprows=1)\nDP0219=pd.read_csv('data/2019DP02.csv',skiprows=1)\nDP0221=pd.read_csv('data/2021DP02.csv',skiprows=1)\nDP0222=pd.read_csv('data/2022DP02.csv',skiprows=1)\n\nDP0317 = pd.read_csv('data/2017DP03.csv', skiprows=1)\nDP0318 = pd.read_csv('data/2018DP03.csv', skiprows=1)\nDP0319 = pd.read_csv('data/2019DP03.csv', skiprows=1)\nDP0321 = pd.read_csv('data/2021DP03.csv', skiprows=1)\nDP0322 = pd.read_csv('data/2022DP03.csv', skiprows=1)\n\nDP0417 = pd.read_csv('data/2017DP04.csv', skiprows=1)\nDP0418 = pd.read_csv('data/2018DP04.csv', skiprows=1)\nDP0419 = pd.read_csv('data/2019DP04.csv', skiprows=1)\nDP0421 = pd.read_csv('data/2021DP04.csv', skiprows=1)\nDP0422 = pd.read_csv('data/2022DP04.csv', skiprows=1)\n\nDP0517 = pd.read_csv('data/2017DP05.csv', skiprows=1)\nDP0518 = pd.read_csv('data/2018DP05.csv', skiprows=1)\nDP0519 = pd.read_csv('data/2019DP05.csv', skiprows=1)\nDP0521 = pd.read_csv('data/2021DP05.csv', skiprows=1)\nDP0522 = pd.read_csv('data/2022DP05.csv', skiprows=1)\n\nDP0217=DP0217[DP02cols]\nDP0217['Year']=2017\nDP0218=DP0218[DP02cols]\nDP0218['Year']=2018\nDP0219=DP0219[DP02cols]\nDP0219['Year']=2019\nDP0221=DP0221[DP02cols]\nDP0221['Year']=2021\nDP0222=DP0222[DP02cols]\nDP0222['Year']=2022\n\nDP0317 = DP0317[DP03cols]\nDP0318 = DP0318[DP03cols]\nDP0319 = DP0319[DP03cols]\nDP0321 = DP0321[DP03cols]\nDP0322 = DP0322[DP03cols]\n\nDP0417 = DP0417[DP04cols]\nDP0418 = DP0418[DP04cols]\nDP0419 = DP0419[DP04cols]\nDP0421 = DP0421[DP04cols]\nDP0422 = DP0422[DP04cols]\n\nDP0517 = DP0517[DP05cols]\nDP0518 = DP0518[DP05cols]\nDP0519 = DP0519[DP05cols]\nDP0521 = DP0521[DP05cols]\nDP0522 = DP0522[DP05cols]\n\ndf2017=pd.merge(DP0217,DP0317,on='NAME')\ndf2017=pd.merge(df2017,DP0417,on='NAME')\ndf2017=pd.merge(df2017,DP0517,on='NAME')\n\ndf2018=pd.merge(DP0218,DP0318,on='NAME')\ndf2018=pd.merge(df2018,DP0418,on='NAME')\ndf2018=pd.merge(df2018,DP0518,on='NAME')\n\ndf2019=pd.merge(DP0219,DP0319,on='NAME')\ndf2019=pd.merge(df2019,DP0419,on='NAME')\ndf2019=pd.merge(df2019,DP0519,on='NAME')\n\n\ndf2021=pd.merge(DP0221,DP0321,on='NAME')\ndf2021=pd.merge(df2021,DP0421,on='NAME')\ndf2021=pd.merge(df2021,DP0521,on='NAME')\n\ndf2022=pd.merge(DP0222,DP0322,on='NAME')\ndf2022=pd.merge(df2022,DP0422,on='NAME')\ndf2022=pd.merge(df2022,DP0522,on='NAME')\n\n\n\n\nCode\ndf2017.columns\n\n\nIndex(['NAME', 'DP02_0001E', 'DP02_0002E', 'DP02_0003E', 'DP02_0007E',\n       'DP02_0011E', 'DP02_0037E', 'DP02_0060E', 'DP02_0061E', 'DP02_0062E',\n       'DP02_0063E', 'DP02_0064E', 'DP02_0065E', 'Year', 'DP03_0026PE',\n       'DP03_0005E', 'DP03_0062E', 'DP03_0063E', 'DP03_0093E', 'DP03_0094E',\n       'DP04_0002E', 'DP04_0001E', 'DP04_0037E', 'DP04_0047E', 'DP04_0134E',\n       'DP05_0001E', 'DP05_0004E', 'DP05_0018E', 'DP05_0037E', 'DP05_0038E',\n       'DP05_0039E', 'DP05_0044E', 'DP05_0071E'],\n      dtype='object')\n\n\n\n\nCode\nus_states = [\n    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\",\n    \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\",\n    \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n    \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\",\n    \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\",\n    \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\",\n    \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\",\n    \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\",\n    \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\",\n    \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\",\n    \"District of Columbia\"\n]\nfeature_df=pd.DataFrame()\nfor state in us_states:\n    row1=df2017[df2017['NAME']==state]\n    row2=df2018[df2018['NAME']==state]\n    row3=df2019[df2019['NAME']==state]\n    row4=df2021[df2021['NAME']==state]\n    row5=df2022[df2022['NAME']==state]\n    df=pd.concat([row1,row2,row3,row4,row5])\n    df=df.drop('NAME',axis=1)\n    df=df.set_index('Year').pct_change().reset_index()\n    df['RegionName']=state\n    feature_df=pd.concat([feature_df,df])\nfeature_df=feature_df.dropna()\nfeature_df['Year']=feature_df['Year'].astype(str)\n\n\n\n\nCode\nfeature_df.head()\n\n\n\n\n\n\n\n\n\nYear\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\n...\nDP04_0134E\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0037E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nDP05_0071E\nRegionName\n\n\n\n\n1\n2018\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n...\n0.050667\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.047111\nAlabama\n\n\n2\n2019\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n...\n0.024112\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n0.036934\nAlabama\n\n\n3\n2021\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n...\n0.066914\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n0.082710\nAlabama\n\n\n4\n2022\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n...\n0.060395\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n-0.125037\nAlabama\n\n\n1\n2018\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n...\n-0.019983\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.026241\nAlaska\n\n\n\n\n5 rows × 33 columns\n\n\n\n\n\nCode\nzillow=pd.read_csv('data/Zillow.csv')\nzillowlong=zillow.melt(id_vars=['RegionName'],var_name='Year',value_name='Typical Home Value')\n\n\n\n\nCode\nzillowlong.head()\n\n\n\n\n\n\n\n\n\nRegionName\nYear\nTypical Home Value\n\n\n\n\n0\nCalifornia\n2017\n498316.221451\n\n\n1\nTexas\n2017\n192963.504413\n\n\n2\nFlorida\n2017\n221751.216789\n\n\n3\nNew York\n2017\n316846.572473\n\n\n4\nPennsylvania\n2017\n174153.095534\n\n\n\n\n\n\n\n\n\nCode\ntarget_df=pd.DataFrame()\nfor state in us_states:\n    row1=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2017')]\n    row2=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2018')]\n    row3=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2019')]\n    row4=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2021')]\n    row5=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2022')]\n    df=pd.concat([row1,row2,row3,row4,row5])\n    df=df.drop('RegionName',axis=1)\n    df=df.set_index('Year').pct_change().reset_index()\n    df['RegionName']=state\n    target_df=pd.concat([target_df,df])\n\n\n\n\nCode\ntarget_df=target_df.dropna()\nfinal_data=pd.merge(feature_df,target_df,on=['RegionName','Year'])\nnew_column_names=['Year','Total households','Married couple households','Married couple with children of the householder under 18 years','Male householder,no spouse/partner present with children of the householder under 18 years','Female householder, no spouse/partner present with children of the householder under 18 years','Number of women 15 to 50 years old who had a birth in the past 12 months','Less than 9th grade','9th to 12th grade,no diploma','High school graduate (includes equivalency)','Some college, no degree','Associates degree','Bachelors degree','Number of people employed','Number of people unemployed','Median Household Income','Mean Household Income','Median earnings for male full-time, year-round workers','Median earnings for female full-time, year-round workers','Occupied Housing Units','Total Housing Units','Median Rooms','Renter Occupied Housing Units','Mean Rent Paid','Total population','Sex ratio (males per 100 females)','Median Age','Race-White','Race-Black','Race-American Indian and Alaska Native','Race-Asian','Hispanic or Latino','RegionName','Typical Home Value']\nfinal_data.columns=new_column_names\nfinal_data.to_csv('./data/RecordData.csv', index=False)\n\n\n\n\nCode\nfinal_data.head()\n\n\n\n\n\n\n\n\n\nYear\nTotal households\nMarried couple households\nMarried couple with children of the householder under 18 years\nMale householder,no spouse/partner present with children of the householder under 18 years\nFemale householder, no spouse/partner present with children of the householder under 18 years\nNumber of women 15 to 50 years old who had a birth in the past 12 months\nLess than 9th grade\n9th to 12th grade,no diploma\nHigh school graduate (includes equivalency)\n...\nTotal population\nSex ratio (males per 100 females)\nMedian Age\nRace-White\nRace-Black\nRace-American Indian and Alaska Native\nRace-Asian\nHispanic or Latino\nRegionName\nTypical Home Value\n\n\n\n\n0\n2018\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n...\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.047111\nAlabama\n0.038688\n\n\n1\n2019\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n...\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n0.036934\nAlabama\n0.070360\n\n\n2\n2021\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n...\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n0.082710\nAlabama\n0.263465\n\n\n3\n2022\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n...\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n-0.125037\nAlabama\n0.076587\n\n\n4\n2018\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n...\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.026241\nAlaska\n0.005789\n\n\n\n\n5 rows × 34 columns\n\n\n\n\n\nCode\nfinal_data.shape\n\n\n(204, 34)\n\n\n\n\nRecord Data Overview\nAs you can see with the cleaned record data above, my data set has 204 rows and 34 columns. Two of the columns serve as an index, with the state and year as the unique identifiers. Each cell represents the percent change of the variable from the previous year. For example, the first row is Alabama-2018. This row represents the percent change from 2017 to 2018 for all the census predictor variables and the home value target variable. More data would be ideal for my analysis, but for the purposes of this project I will continue with the data I already wrangled. 31 features are a lot and I should definitely aim to cut this number down. More on this in the dimensionality reduction and data exploration tabs, but I start below.\nNow I will quickly look at the max of each column just to see if there are any obvious problems.\n\n\nCode\nDP02colss=['DP02_0001E','DP02_0002E','DP02_0003E','DP02_0007E','DP02_0011E','DP02_0037E','DP02_0060E','DP02_0061E','DP02_0062E','DP02_0063E','DP02_0064E','DP02_0065E']\nDP03colss=['DP03_0026PE','DP03_0005E','DP03_0062E','DP03_0063E','DP03_0093E','DP03_0094E']\nDP04colss=['DP04_0002E','DP04_0001E','DP04_0037E','DP04_0047E','DP04_0134E']\nDP05colss=['DP05_0001E','DP05_0004E','DP05_0018E','DP05_0037E','DP05_0038E','DP05_0039E','DP05_0044E','DP05_0071E']\n\n\n\n\nCode\nfor name in DP02colss:\n    print(feature_df[name].max())\n\n\n0.0960146791508043\n0.08325965443347161\n0.21027912686129047\n0.5718313168095854\n0.31098100619511193\n4.71179314694135\n0.5836723372781065\n0.2331338411316648\n1.16632039564257\n3.7135790077430455\n0.20096335286522993\n1.8600379486638907\n\n\n\n\nCode\nfor name in DP03colss:\n    print(feature_df[name].max())\n\n\n0.08159905963183367\n0.9855738883335392\n0.1558987776230465\n0.13903133903133913\n0.18793842369265668\n0.19923024054982807\n\n\n\n\nCode\nfor name in DP04colss:\n    print(feature_df[name].max())\n\n\n0.0960146791508043\n0.10739311182290723\n0.03921568627450989\n0.1304236996210817\n0.17613636363636354\n\n\n\n\nCode\nfor name in DP05colss:\n    print(feature_df[name].max())\n\n\n0.06371228802533757\n0.024340770791075217\n0.026666666666666616\n0.04291548345376328\n0.9405508590128171\n1.598558282208589\n0.3942459396751741\n0.7167422884813359\n\n\nLooks like there are some obvious outliers, with one cell having a 470% increase in the metric. With the ACS data, for each measure they release to the public, they also release a margin of error. The margin of error can be high at times, explaining why some percent change values may be out of the ordinary. In the next tab I will dig deeper into the outliers."
  },
  {
    "objectID": "DataGathering.html",
    "href": "DataGathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "Census Data\nI will attempt to gather all the data I need to answer my 10 questions. Most of the data I acquire will come from census.gov. I will use data tables from the U.S. Census Bureau’s American Community Survey (ACS), a nationwide survey that collects and produces information on social, economic, housing, and demographic characteristics about our nation’s population each year.\nI chose census data and specifically the ACS tables because, from experience and knowledge I have accumulated in my life, I inferred that the variables in these tables could be useful in predicting home prices. For example, one variable I will examine is the number of people with bachelor’s degrees. Usually, higher education is linked to higher incomes, and therefore, higher home prices. The variables I choose to include from the tables may prove valuable or they may not; part of my analysis is determining which variables are the most important. Also, a lot of my variables are heavily correlated with each other, like total households and total married households. This can become an issue in the models I create as multicollinearity increases variance and makes it hard to know the impact of an individual variable in the model. I am aware of this and will address it later in my project.\nI will import the DP02-DP05 tables from 2017-2022, excluding 2020 because there was not accurate data that year due to COVID. Ultimately, I will not need all the columns in the tables, but it will be nice to have easy access to them in my future analysis. I only went back to 2017 for now, but if I wanted to improve my models, I could retrieve more data. The same methodology will apply no matter how much data I acquire.\nHere are what the ACS tables contain: DP02: Selected Social Characteristics in the United States  DP03: Selected Economic Characteristics in the United States  DP04: Selected Housing Characteristics  DP05: ACS Demographic and Housing Estimates\nUsing the API, you can query different geography hierarchies, like if you want data on regions in the U.S. or, in my analysis, on U.S. states.\nI will start by retreiving one table to see what we are working with.\n\n\nCode\nimport requests\nimport json\nimport re\nimport pandas as pd\nDP02_URL_2017=\"https://api.census.gov/data/2017/acs/acs1/profile?get=group(DP02)&for=state:*\"\nDP02_2017= requests.get(DP02_URL_2017)\nDP02_2017 = DP02_2017.json()\nDP02_2017=pd.DataFrame(DP02_2017)\nDP02_2017.head()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n\n\n\n\n0\nDP02_0001E\nDP02_0001EA\nDP02_0001M\nDP02_0001MA\nDP02_0001PE\nDP02_0001PEA\nDP02_0001PM\nDP02_0001PMA\nDP02_0002E\nDP02_0002EA\n...\nDP02_0152EA\nDP02_0152M\nDP02_0152MA\nDP02_0152PE\nDP02_0152PEA\nDP02_0152PM\nDP02_0152PMA\nGEO_ID\nNAME\nstate\n\n\n1\n1091980\nNone\n9693\nNone\n1091980\nNone\n-888888888\n(X)\n716451\nNone\n...\nNone\n9854\nNone\n73.4\nNone\n0.6\nNone\n0400000US28\nMississippi\n28\n\n\n2\n2385135\nNone\n13054\nNone\n2385135\nNone\n-888888888\n(X)\n1527260\nNone\n...\nNone\n15052\nNone\n81.3\nNone\n0.4\nNone\n0400000US29\nMissouri\n29\n\n\n3\n423091\nNone\n4068\nNone\n423091\nNone\n-888888888\n(X)\n262726\nNone\n...\nNone\n4698\nNone\n81.3\nNone\n0.9\nNone\n0400000US30\nMontana\n30\n\n\n4\n754490\nNone\n4583\nNone\n754490\nNone\n-888888888\n(X)\n484989\nNone\n...\nNone\n6206\nNone\n84.4\nNone\n0.5\nNone\n0400000US31\nNebraska\n31\n\n\n\n\n5 rows × 1219 columns\n\n\n\nThe column names do not tell us much right now because they are codes that match to different variable labels in the census data. For example, DP02_0001E maps to total household counts. In the data cleaning section, we will be sure to give each column a proper label that tells us what the column represents. Also in the data cleaning section, we will only keep a select few columns out of the over 1000 columns in the tables.\nI will use the following loop to get each table I want and turn them into csv files for easy retrieval for the rest of my analysis. If you would like to see my csv files, please refer to this GitHub Repository. Nolan Penoyer Website/data (Every file that starts with a year)\n\n\nCode\nstring='https://api.census.gov/data/2016/acs/acs1/profile?get=group(DP05)&for=state'\nlist1=['2016','2017','2018','2019','2021']\nlist2=['2017','2018','2019','2021','2022']\nlist3=['DP01','DP02','DP03','DP04']\nlist4=['DP02','DP03','DP04','DP05']\nfor i in range(5):\n    string=string.replace('DP05',list3[0])\n    for w in range(4):\n        file='./data/'\n        csv='.csv'\n        name=file+list2[i]+list4[w]+csv\n        csvname=file+list2[i]+list3[w]\n        string=string.replace(list1[i],list2[i])\n        string=string.replace(list3[w],list4[w])\n        response=requests.get(string)\n        response = response.json()\n        df=pd.DataFrame(response)\n        df.to_csv(name, index=False)\n\n\n\n\nHome Value Data\nMy home value data comes from Zillow. Specifically, I will use Zillow’s Home Value Index (ZHVI), which reflects the typical value of homes in the 35th to 65th percentile, a home that will be in my price range. The ZHVI was created to accurately capture the seasonally adjusted value of a typical property across the nation as opposed to just the homes that sold. If you would like to learn more about how Zillow calculates this value and why it is a more accurate measure of home value as opposed to other alternatives like median home price, you can visit the link I provided.\nZillow has an API, but since it was one table I needed, I downloaded the csv file. Like my census data, I calculated this data on the state level for years 2017-2022, excluding 2020. If you would like to see my csv file, please refer to this GitHub Repository. Nolan Penoyer Website/data (Zillow.csv))\n\n\nNews Data\nAlthough not a focus of my project, news headline text data could potentially provide us with some insight on the sentiment of the real estate market and therefore what direction the general population thinks home prices are heading. The overall sentiment can provide us with signals that people are overvaluing or undervaluing homes, or that a rise or drop in home prices may be pending. Knowing if the general public believes buying a home, especially in a certain state, is a smart decision, can tell us which direction prices are headed, depending if you agree with the general public or not. Another potential data science project I would like to study in the future is if market sentiment matches market performance, and if there is a lag between the two.\nI will use the news API to search and retreive live articles from across the web. I will only search for articles that contain “Real Estate” in the headline. If you would like to see my csv file, please refer to this GitHub Repository. Nolan Penoyer Website/data (news.csv))\n\n\nCode\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\nAPI_KEY='6a2b4aca491e4bedb90a4c7275f2ddf6'\nTOPIC='Real Estate'\nURLpost = {'apiKey': API_KEY,\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\n#print(baseURL)\nresponse = requests.get(baseURL, URLpost)\nresponse = response.json()\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        #print(\"ERROR\")\n        out=''\n    return out\n\narticle_list=response['articles']   #list of dictionaries for each article\narticle_keys=article_list[0].keys()\n#print(\"AVAILABLE KEYS:\")\n#print(article_keys)\nindex=0\ncleaned_data=[];  \nfor article in article_list:\n    tmp=[]\n    for key in article_keys:\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n            #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            if(src in author): \n                tmp.append(author)\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        # if(key=='description'):\n        #     tmp.append(string_cleaner(article[key]))\n\n        # if(key=='content'):\n        #     tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n            #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n                print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data.append(tmp)\n    index+=1\n\ndf = pd.DataFrame(cleaned_data)\ndf.head()\ndf.to_csv('./data/news.csv', index=False) #,index_label=['title','src','author','date','description'])\n\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\nlifehackercom\nelizabeth yuko\nwhat to do if your home inspector missed a maj...\n2023-11-12T18:00:00Z\n\n\n1\nslashdotorg\nmsmash\nalmost no one pays a 6% real-estate commission...\n2023-11-17T18:40:00Z\n\n\n2\nbusiness insider\ngeorge glover\nreal estate made china rich now it's looking m...\n2023-11-16T10:11:14Z\n\n\n3\nbusiness insider\ncork gaines\nbaby boomers got rich off real estate and they...\n2023-11-19T10:32:01Z\n\n\n4\nbusiness insider\ntheron mohamed\nstocks may crash 30%, a recession looks immine...\n2023-11-19T11:30:01Z\n\n\n\n\n\n\n\nHere I will use an API in R to retreive text data that can give me some insight into some real estate trends and what states people are talking about.\n\n\nReddit Data\nI can also retrieve Reddit text data for the same reason. Reddit comments are a more accurate measure of what the public believes than news data as there are no biases and incentives from corporations to report news a certain way. I also can break up the text data by state, which I attempted to do below. I need to find a work around as Reddit limits how many requests you are allowed in a period of time. I will not use Reddit data for the rest of this project.\n\n\nCode\nlibrary(RedditExtractoR)\nlibrary(dplyr)\nstates &lt;- c(\n  \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\",\n  \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\",\n  \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n  \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\",\n  \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\",\n  \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\",\n  \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\",\n  \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\",\n  \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\",\n  \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n)\ntext&lt;-data.frame()\nsubreddit=\"RealEstate\"\nfor (state in states){\n    state_df&lt;- find_thread_urls(keywords = state,subreddit=subreddit, sort_by=\"top\", period = 'year')\n    state_df&lt;-state_df|&gt;\n    mutate(State=state)\n    text&lt;-rbind(text,state_df)\n}"
  },
  {
    "objectID": "DimensionalityReduction.html",
    "href": "DimensionalityReduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "PCA\nPrincipal component analysis is a method where we can transform high dimensional data into a lower-dimensional representation. The first principle component maximizes the variance in the data, and each subsequent principle component is orthogonal to the others. These components very quickly encapsulate all the data’s variability, as you will see that the first few principle components do most of the job in explaining the variance. Therefore, you can use PCA to see what variables are necessary and what variables you do not need as they are the most important patterns.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from sklearn.decomposition import PCA\n#from sklearn import preprocessing\nfrom statsmodels.multivariate.pca import PCA\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nrecord=pd.read_csv('data/RecordData.csv')\nrecord=record.drop(['Year','RegionName'],axis=1)\nrecord.head()\n\n\n\n\n\n\n\n\n\nTotal households\nMarried couple households\nMarried couple with children of the householder under 18 years\nMale householder,no spouse/partner present with children of the householder under 18 years\nFemale householder, no spouse/partner present with children of the householder under 18 years\nNumber of women 15 to 50 years old who had a birth in the past 12 months\nLess than 9th grade\n9th to 12th grade,no diploma\nHigh school graduate (includes equivalency)\nSome college, no degree\n...\nMean Rent Paid\nTotal population\nSex ratio (males per 100 females)\nMedian Age\nRace-White\nRace-Black\nRace-American Indian and Alaska Native\nRace-Asian\nHispanic or Latino\nTypical Home Value\n\n\n\n\n0\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n0.033732\n...\n0.050667\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.047111\n0.038688\n\n\n1\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n1.429793\n...\n0.024112\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n0.036934\n0.070360\n\n\n2\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n0.007408\n...\n0.066914\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n0.082710\n0.263465\n\n\n3\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n0.014854\n...\n0.060395\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n-0.125037\n0.076587\n\n\n4\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n-0.065209\n...\n-0.019983\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.026241\n0.005789\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\nCode\npca=PCA(record,standardize=True,method='eig')\ncomponents=pca.factors\nloadings=pca.loadings\npcFeats=pd.concat([record,components],axis=1)\ncorrelation=pcFeats.corr()\ncorrelation=correlation[:-len(components.columns)].loc[:,'comp_00':]\nfig,ax = plt.subplots(figsize=(20, 7))\nsns.heatmap(correlation,annot=True)\nplt.show()\n\n\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\statsmodels\\multivariate\\pca.py:563: EstimationWarning: Only 31 eigenvalues are positive.  This is the maximum number of components that can be extracted.\n  warnings.warn('Only {num:d} eigenvalues are positive.  '\n\n\n\n\n\nThis shows us what features are correlated with the first principle component, and what features will change together. For example, there is a high negative correlation between the first principle component and percent change of married couple households and married couple households with children under 18 years old. So, you could say that as the percent change of married couple households decrease, so does the percent change of married couple households with children, which makes a lot of sense. There are definitely a lot if redundant variables in my data set.\n\n\nCode\nprint(loadings)\n\n\n                                                     comp_00   comp_01  \\\nTotal households                                   -0.175812  0.256873   \nMarried couple households                          -0.283751 -0.104949   \nMarried couple with children of the householder... -0.284422 -0.094214   \nMale householder,no spouse/partner present with... -0.236433 -0.144831   \nFemale householder, no spouse/partner present w... -0.273693 -0.169895   \nNumber of women 15 to 50 years old who had a bi...  0.250620  0.157151   \nLess than 9th grade                                -0.238491 -0.125836   \n9th to 12th grade,no diploma                       -0.265389 -0.198263   \nHigh school graduate (includes equivalency)         0.223171  0.110411   \nSome college, no degree                             0.257255  0.140807   \nAssociates degree                                  -0.270666 -0.154192   \nBachelors degree                                    0.235542  0.177002   \nNumber of people employed                           0.031062 -0.255672   \nNumber of people unemployed                        -0.099124  0.343979   \nMedian Household Income                            -0.013525  0.031944   \nMean Household Income                              -0.074350  0.059318   \nMedian earnings for male full-time, year-round ... -0.157543  0.269660   \nMedian earnings for female full-time, year-roun... -0.172815  0.260006   \nOccupied Housing Units                             -0.175812  0.256873   \nTotal Housing Units                                -0.082453  0.171418   \nMedian Rooms                                       -0.086558  0.169374   \nRenter Occupied Housing Units                      -0.024645 -0.046997   \nMean Rent Paid                                     -0.183594  0.064758   \nTotal population                                   -0.104039  0.140432   \nSex ratio (males per 100 females)                  -0.133004  0.200413   \nMedian Age                                         -0.033965  0.074912   \nRace-White                                          0.134997 -0.286344   \nRace-Black                                          0.080493 -0.062941   \nRace-American Indian and Alaska Native             -0.006469 -0.002458   \nRace-Asian                                         -0.037633 -0.031664   \nHispanic or Latino                                 -0.017097  0.104454   \nTypical Home Value                                 -0.187627  0.273763   \n\n                                                     comp_02   comp_03  \\\nTotal households                                   -0.041297  0.334708   \nMarried couple households                          -0.021953 -0.018613   \nMarried couple with children of the householder... -0.032164 -0.005769   \nMale householder,no spouse/partner present with... -0.024172 -0.056104   \nFemale householder, no spouse/partner present w... -0.054392 -0.014988   \nNumber of women 15 to 50 years old who had a bi...  0.038264  0.062428   \nLess than 9th grade                                -0.038444  0.036285   \n9th to 12th grade,no diploma                       -0.045592 -0.007149   \nHigh school graduate (includes equivalency)         0.018343  0.008309   \nSome college, no degree                             0.061736  0.050604   \nAssociates degree                                  -0.045661 -0.054160   \nBachelors degree                                    0.037066  0.035572   \nNumber of people employed                           0.351453  0.266233   \nNumber of people unemployed                        -0.180975 -0.133079   \nMedian Household Income                             0.521423 -0.032403   \nMean Household Income                               0.498077 -0.011832   \nMedian earnings for male full-time, year-round ...  0.087065 -0.078914   \nMedian earnings for female full-time, year-roun...  0.052860 -0.080028   \nOccupied Housing Units                             -0.041297  0.334708   \nTotal Housing Units                                -0.045309  0.469018   \nMedian Rooms                                        0.026582 -0.296800   \nRenter Occupied Housing Units                      -0.120724  0.476224   \nMean Rent Paid                                      0.296787  0.087065   \nTotal population                                    0.241117  0.066477   \nSex ratio (males per 100 females)                  -0.023775 -0.152739   \nMedian Age                                         -0.189800  0.190196   \nRace-White                                          0.124471  0.039450   \nRace-Black                                         -0.047717  0.074449   \nRace-American Indian and Alaska Native             -0.096688  0.056859   \nRace-Asian                                          0.146661  0.044865   \nHispanic or Latino                                 -0.138762 -0.168922   \nTypical Home Value                                  0.155783 -0.102195   \n\n                                                     comp_04   comp_05  \\\nTotal households                                   -0.072272  0.021530   \nMarried couple households                          -0.012204  0.076903   \nMarried couple with children of the householder...  0.001663  0.050258   \nMale householder,no spouse/partner present with... -0.084484  0.141246   \nFemale householder, no spouse/partner present w...  0.016201 -0.048344   \nNumber of women 15 to 50 years old who had a bi... -0.031907  0.042023   \nLess than 9th grade                                 0.056733 -0.111619   \n9th to 12th grade,no diploma                       -0.035041 -0.031175   \nHigh school graduate (includes equivalency)         0.017201 -0.010685   \nSome college, no degree                            -0.043660 -0.062417   \nAssociates degree                                  -0.027370  0.029546   \nBachelors degree                                   -0.072152  0.090322   \nNumber of people employed                          -0.118417  0.043255   \nNumber of people unemployed                        -0.068599 -0.018621   \nMedian Household Income                             0.159509 -0.006163   \nMean Household Income                               0.090642 -0.108469   \nMedian earnings for male full-time, year-round ...  0.165256 -0.008443   \nMedian earnings for female full-time, year-roun...  0.234965 -0.063180   \nOccupied Housing Units                             -0.072272  0.021530   \nTotal Housing Units                                 0.091177  0.060000   \nMedian Rooms                                       -0.343691 -0.119331   \nRenter Occupied Housing Units                      -0.086785  0.057854   \nMean Rent Paid                                      0.073437  0.082692   \nTotal population                                   -0.463358  0.200304   \nSex ratio (males per 100 females)                   0.189825  0.298399   \nMedian Age                                         -0.051145 -0.482304   \nRace-White                                         -0.177610  0.087159   \nRace-Black                                          0.048970  0.595597   \nRace-American Indian and Alaska Native              0.020349  0.192401   \nRace-Asian                                         -0.427826 -0.275755   \nHispanic or Latino                                 -0.462499  0.232964   \nTypical Home Value                                 -0.097289  0.050536   \n\n                                                     comp_06   comp_07  \\\nTotal households                                   -0.069601  0.040616   \nMarried couple households                          -0.017941  0.015132   \nMarried couple with children of the householder... -0.054417  0.025334   \nMale householder,no spouse/partner present with...  0.023200 -0.035089   \nFemale householder, no spouse/partner present w... -0.001008 -0.009835   \nNumber of women 15 to 50 years old who had a bi...  0.006934  0.131700   \nLess than 9th grade                                 0.020635  0.161289   \n9th to 12th grade,no diploma                        0.034525 -0.011627   \nHigh school graduate (includes equivalency)         0.020852  0.064396   \nSome college, no degree                             0.008818 -0.075002   \nAssociates degree                                   0.012272 -0.011935   \nBachelors degree                                   -0.031654  0.122012   \nNumber of people employed                           0.108385  0.158484   \nNumber of people unemployed                         0.085554 -0.060479   \nMedian Household Income                             0.134848 -0.017320   \nMean Household Income                              -0.007171 -0.052465   \nMedian earnings for male full-time, year-round ... -0.080137 -0.235689   \nMedian earnings for female full-time, year-roun... -0.057507  0.019407   \nOccupied Housing Units                             -0.069601  0.040616   \nTotal Housing Units                                 0.031256 -0.124282   \nMedian Rooms                                        0.060697  0.196985   \nRenter Occupied Housing Units                      -0.170729  0.195749   \nMean Rent Paid                                      0.013183  0.106861   \nTotal population                                    0.092974  0.098734   \nSex ratio (males per 100 females)                  -0.104101  0.017397   \nMedian Age                                          0.160790 -0.077742   \nRace-White                                         -0.207497  0.100943   \nRace-Black                                         -0.047667 -0.494744   \nRace-American Indian and Alaska Native              0.897478  0.020161   \nRace-Asian                                          0.003018 -0.667031   \nHispanic or Latino                                 -0.074448  0.110356   \nTypical Home Value                                 -0.021517  0.076785   \n\n                                                     comp_08   comp_09  ...  \\\nTotal households                                   -0.024058 -0.007651  ...   \nMarried couple households                          -0.072003  0.067468  ...   \nMarried couple with children of the householder... -0.031529  0.091676  ...   \nMale householder,no spouse/partner present with...  0.091098 -0.240744  ...   \nFemale householder, no spouse/partner present w... -0.036796  0.030232  ...   \nNumber of women 15 to 50 years old who had a bi... -0.038232  0.087709  ...   \nLess than 9th grade                                -0.009238  0.048028  ...   \n9th to 12th grade,no diploma                       -0.020437  0.011635  ...   \nHigh school graduate (includes equivalency)         0.137697 -0.187922  ...   \nSome college, no degree                             0.033581 -0.017658  ...   \nAssociates degree                                  -0.069527  0.021401  ...   \nBachelors degree                                   -0.170101  0.040172  ...   \nNumber of people employed                          -0.090326 -0.104117  ...   \nNumber of people unemployed                        -0.152481  0.260190  ...   \nMedian Household Income                             0.000005 -0.095904  ...   \nMean Household Income                               0.073436 -0.192447  ...   \nMedian earnings for male full-time, year-round ...  0.108608 -0.026196  ...   \nMedian earnings for female full-time, year-roun...  0.229946  0.090084  ...   \nOccupied Housing Units                             -0.024058 -0.007651  ...   \nTotal Housing Units                                 0.017312  0.267119  ...   \nMedian Rooms                                       -0.318885 -0.185227  ...   \nRenter Occupied Housing Units                       0.235107 -0.268981  ...   \nMean Rent Paid                                      0.045632 -0.076585  ...   \nTotal population                                   -0.246867  0.283750  ...   \nSex ratio (males per 100 females)                  -0.069576 -0.205501  ...   \nMedian Age                                         -0.373063 -0.487366  ...   \nRace-White                                         -0.072007  0.199947  ...   \nRace-Black                                         -0.348405 -0.269896  ...   \nRace-American Indian and Alaska Native              0.187707  0.013777  ...   \nRace-Asian                                          0.231839  0.089941  ...   \nHispanic or Latino                                  0.495382 -0.267351  ...   \nTypical Home Value                                 -0.021344 -0.099602  ...   \n\n                                                     comp_21   comp_22  \\\nTotal households                                    0.169498  0.128268   \nMarried couple households                          -0.045654 -0.033762   \nMarried couple with children of the householder...  0.016424  0.041158   \nMale householder,no spouse/partner present with... -0.086888  0.370482   \nFemale householder, no spouse/partner present w...  0.005347 -0.011706   \nNumber of women 15 to 50 years old who had a bi...  0.186794  0.014547   \nLess than 9th grade                                 0.388083  0.064448   \n9th to 12th grade,no diploma                       -0.077566 -0.082440   \nHigh school graduate (includes equivalency)         0.164758 -0.006101   \nSome college, no degree                            -0.123447  0.048764   \nAssociates degree                                  -0.167815 -0.050109   \nBachelors degree                                   -0.020373  0.068385   \nNumber of people employed                           0.025580 -0.368189   \nNumber of people unemployed                        -0.417803  0.163299   \nMedian Household Income                            -0.468547  0.196464   \nMean Household Income                               0.079183  0.148365   \nMedian earnings for male full-time, year-round ...  0.205027 -0.245579   \nMedian earnings for female full-time, year-roun... -0.128739 -0.340876   \nOccupied Housing Units                              0.169498  0.128268   \nTotal Housing Units                                -0.151628  0.020717   \nMedian Rooms                                       -0.034594 -0.149744   \nRenter Occupied Housing Units                      -0.329769 -0.025981   \nMean Rent Paid                                      0.062506 -0.112738   \nTotal population                                   -0.002161 -0.235638   \nSex ratio (males per 100 females)                   0.028859 -0.004810   \nMedian Age                                         -0.058409 -0.017427   \nRace-White                                          0.039972  0.354390   \nRace-Black                                          0.041398 -0.080753   \nRace-American Indian and Alaska Native              0.104833  0.084749   \nRace-Asian                                          0.085402  0.020323   \nHispanic or Latino                                 -0.086480 -0.106973   \nTypical Home Value                                  0.200839  0.404371   \n\n                                                     comp_23   comp_24  \\\nTotal households                                    0.000492 -0.103973   \nMarried couple households                           0.153542 -0.057876   \nMarried couple with children of the householder...  0.163272  0.006682   \nMale householder,no spouse/partner present with...  0.141139 -0.287004   \nFemale householder, no spouse/partner present w... -0.019604  0.035783   \nNumber of women 15 to 50 years old who had a bi...  0.604318  0.151100   \nLess than 9th grade                                -0.223222  0.014378   \n9th to 12th grade,no diploma                        0.050416 -0.013698   \nHigh school graduate (includes equivalency)         0.067479 -0.018181   \nSome college, no degree                            -0.090887 -0.104636   \nAssociates degree                                  -0.226625  0.093421   \nBachelors degree                                   -0.530006 -0.156186   \nNumber of people employed                           0.081571  0.293511   \nNumber of people unemployed                         0.193346  0.118846   \nMedian Household Income                            -0.090852  0.123182   \nMean Household Income                               0.128288 -0.404439   \nMedian earnings for male full-time, year-round ... -0.191821  0.156294   \nMedian earnings for female full-time, year-roun...  0.062441  0.128101   \nOccupied Housing Units                              0.000492 -0.103973   \nTotal Housing Units                                 0.043906 -0.039509   \nMedian Rooms                                        0.053321  0.006471   \nRenter Occupied Housing Units                      -0.083737  0.143552   \nMean Rent Paid                                      0.101653 -0.018896   \nTotal population                                   -0.072202 -0.243183   \nSex ratio (males per 100 females)                   0.065547 -0.132478   \nMedian Age                                          0.062445  0.038031   \nRace-White                                          0.045858  0.191156   \nRace-Black                                          0.023737  0.122264   \nRace-American Indian and Alaska Native             -0.044680  0.034204   \nRace-Asian                                          0.011389  0.062082   \nHispanic or Latino                                  0.004705  0.001797   \nTypical Home Value                                 -0.130502  0.588755   \n\n                                                     comp_25   comp_26  \\\nTotal households                                   -0.040343 -0.085560   \nMarried couple households                           0.160969  0.331180   \nMarried couple with children of the householder...  0.053048  0.514200   \nMale householder,no spouse/partner present with... -0.120517 -0.050679   \nFemale householder, no spouse/partner present w... -0.113155  0.275259   \nNumber of women 15 to 50 years old who had a bi...  0.090827  0.118528   \nLess than 9th grade                                -0.119909  0.028291   \n9th to 12th grade,no diploma                       -0.095398  0.072306   \nHigh school graduate (includes equivalency)        -0.056369  0.276287   \nSome college, no degree                            -0.237598  0.582038   \nAssociates degree                                  -0.099311 -0.058067   \nBachelors degree                                   -0.197119  0.225768   \nNumber of people employed                          -0.493854 -0.053257   \nNumber of people unemployed                        -0.457176 -0.122938   \nMedian Household Income                             0.314416  0.054652   \nMean Household Income                              -0.243343 -0.109597   \nMedian earnings for male full-time, year-round ... -0.010173  0.021468   \nMedian earnings for female full-time, year-roun... -0.044933  0.035273   \nOccupied Housing Units                             -0.040343 -0.085560   \nTotal Housing Units                                -0.029815  0.011554   \nMedian Rooms                                       -0.083961 -0.009879   \nRenter Occupied Housing Units                       0.086408  0.013355   \nMean Rent Paid                                      0.015892  0.024955   \nTotal population                                    0.340632 -0.020419   \nSex ratio (males per 100 females)                  -0.119384  0.011443   \nMedian Age                                          0.145749  0.045935   \nRace-White                                         -0.103261 -0.024549   \nRace-Black                                         -0.011943  0.011015   \nRace-American Indian and Alaska Native             -0.018438  0.025702   \nRace-Asian                                         -0.036537  0.036997   \nHispanic or Latino                                 -0.012287  0.005524   \nTypical Home Value                                  0.087765  0.047863   \n\n                                                     comp_27   comp_28  \\\nTotal households                                    0.066526  0.006929   \nMarried couple households                          -0.254154 -0.175368   \nMarried couple with children of the householder... -0.262388 -0.259918   \nMale householder,no spouse/partner present with... -0.075252 -0.007972   \nFemale householder, no spouse/partner present w...  0.264549  0.364573   \nNumber of women 15 to 50 years old who had a bi...  0.387601  0.120382   \nLess than 9th grade                                 0.055247 -0.002357   \n9th to 12th grade,no diploma                        0.082592  0.691310   \nHigh school graduate (includes equivalency)         0.080792  0.032452   \nSome college, no degree                             0.211011 -0.121656   \nAssociates degree                                   0.694944 -0.428628   \nBachelors degree                                   -0.102778  0.193833   \nNumber of people employed                          -0.198655 -0.091045   \nNumber of people unemployed                        -0.115168 -0.019105   \nMedian Household Income                             0.063394  0.135053   \nMean Household Income                               0.042184 -0.057950   \nMedian earnings for male full-time, year-round ... -0.078793  0.020425   \nMedian earnings for female full-time, year-roun... -0.000315  0.018244   \nOccupied Housing Units                              0.066526  0.006929   \nTotal Housing Units                                -0.003947  0.036732   \nMedian Rooms                                       -0.010314  0.011221   \nRenter Occupied Housing Units                      -0.026691  0.002421   \nMean Rent Paid                                      0.046975 -0.031621   \nTotal population                                    0.045646  0.012980   \nSex ratio (males per 100 females)                   0.039241  0.017358   \nMedian Age                                         -0.026603 -0.037062   \nRace-White                                         -0.012052 -0.005421   \nRace-Black                                          0.036960  0.038965   \nRace-American Indian and Alaska Native             -0.014610 -0.026870   \nRace-Asian                                         -0.003698  0.003502   \nHispanic or Latino                                  0.024882  0.028680   \nTypical Home Value                                 -0.054063  0.025133   \n\n                                                     comp_29   comp_30  \nTotal households                                    0.017677  0.000367  \nMarried couple households                           0.244609 -0.717438  \nMarried couple with children of the householder... -0.058420  0.632089  \nMale householder,no spouse/partner present with... -0.013495  0.017440  \nFemale householder, no spouse/partner present w... -0.737323 -0.182147  \nNumber of women 15 to 50 years old who had a bi...  0.053636 -0.017513  \nLess than 9th grade                                 0.095278 -0.065620  \n9th to 12th grade,no diploma                        0.549732  0.168329  \nHigh school graduate (includes equivalency)         0.077544 -0.003223  \nSome college, no degree                             0.138212 -0.075044  \nAssociates degree                                   0.194413  0.047726  \nBachelors degree                                    0.028283 -0.008947  \nNumber of people employed                          -0.028978 -0.010478  \nNumber of people unemployed                        -0.026546 -0.025869  \nMedian Household Income                            -0.012970  0.020813  \nMean Household Income                              -0.018451 -0.016799  \nMedian earnings for male full-time, year-round ...  0.019183 -0.005316  \nMedian earnings for female full-time, year-roun... -0.012839 -0.010708  \nOccupied Housing Units                              0.017677  0.000367  \nTotal Housing Units                                -0.002885 -0.017756  \nMedian Rooms                                        0.006431 -0.017342  \nRenter Occupied Housing Units                      -0.017480 -0.016466  \nMean Rent Paid                                      0.009435  0.000556  \nTotal population                                   -0.076875  0.069598  \nSex ratio (males per 100 females)                  -0.011386  0.008668  \nMedian Age                                         -0.024783  0.020056  \nRace-White                                          0.002162 -0.048329  \nRace-Black                                         -0.005091  0.002328  \nRace-American Indian and Alaska Native             -0.011340 -0.006273  \nRace-Asian                                          0.016829 -0.017703  \nHispanic or Latino                                 -0.044713 -0.015516  \nTypical Home Value                                  0.048004 -0.023156  \n\n[32 rows x 31 columns]\n\n\nThe loadings tell us what features have the most impact on the principle componets. For example, the median rooms has the most information captured in the second principle component.For my analysis I will now attempt to determine the optimal number of principle components.\n\n\nCode\nplt.figure(figsize=(8,6))\npca.plot_scree()\nplt.show()\n\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\nIt is difficult to see the elbow here, but it seems like there may be an elbow at around 5 principle componenets.\n\n\nCode\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nscaler=StandardScaler()\nscaled=scaler.fit_transform(record)\npca = PCA()\ncomponents = pca.fit_transform(scaled)\n\n\n\n\nCode\nexplainedVarianceRatio=pca.explained_variance_ratio_\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(explainedVarianceRatio) + 1), np.cumsum(explainedVarianceRatio), marker='o')\nplt.title('Scree Plot')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.grid(True)\nplt.show()\n\n\n\n\n\nWe will go with 5 principle components\nHere we will plot the first two principle components to see if it reveals anything.\n\n\nCode\npca = PCA(n_components=2)\nresult = pca.fit_transform(scaled)\nresultdf=pd.DataFrame(data=result,columns=['PC1','PC2'])\n\n\n\n\nCode\nplt.scatter(resultdf['PC1'], resultdf['PC2'])\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA Result')\nplt.show()\n\n\n\n\n\nThis graph does not tell us much, but overall with PCA which features are highly correlated with what principle components and how many principle componenets are optimal, so ultimatley I can reduce the dimensions of my data so I can have more accurate analysis in the future.\nI will continue my analysis as normal, but if I go back to improve my models, this PCA analysis will useful in helping me see what columns are not contributing to an accurate model.\n\n\nt-SNE\nIn t-Distributed Stochastic Neighbor Embedding, the key parameter is perplexity which affects the tradeoff between preserving local vs global structures. It aims to map similar datapoints together while keeping differrent data points apart.\n\n\nCode\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntsne_results = tsne.fit_transform(scaled)\n\n\n[t-SNE] Computing 121 nearest neighbors...\n[t-SNE] Indexed 204 samples in 0.001s...\n[t-SNE] Computed neighbors for 204 samples in 0.411s...\n[t-SNE] Computed conditional probabilities for sample 204 / 204\n[t-SNE] Mean sigma: 2.276934\n[t-SNE] KL divergence after 250 iterations with early exaggeration: 45.974541\n[t-SNE] KL divergence after 300 iterations: 0.267783\n\n\n\n\nCode\nimport plotly.express as px\ntsne = TSNE(n_components=2, random_state=42)\ntsne_result = tsne.fit_transform(scaled)\ntsne_df = pd.DataFrame(data=tsne_result, columns=['TSNE1', 'TSNE2'])\n\n\n\n\nCode\nfig = px.scatter(tsne_df, x='TSNE1', y='TSNE2', title='t-SNE')\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nEvaluation and Comparison\nPCA utilizes a linear approach, preserving the overall structure of data but may encounter challenges with non-linear relationships and sensitivity to outliers. It is computationally efficient and interpretable, making it well-suited for handling substantial datasets. It is better for data sets with a large number of features, over 50 is commonly used benchmark.\nt-SNE is a non-linear technique that excels in capturing local structures and intricate patterns. However, it comes with increased computational demands and sensitivity to hyperparameters. T-SNE is better tailored for smaller datasets.\nI would say for my data, PCA is a better method that more easily allows me to see what features I should drop from my analysis, but overall the visulizations produced from both methods show some clustering that reveals reduncincy in my variables."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Housing Trends\nIn every aspect of life, discovering insights before others can give us a competitive advantage that will allow us to get ahead. In some activities we do, although we may know information others do not, the nature of the framework we are operating in may restrict how much utility we gain. For example, knowing the opponents next move in a game of checkers will only give us the satisfaction of winning the checkers game. However, other information is much more valuable because of the gravity of the affair in the larger scheme of life.\nShelter is a need that every human must have fulfilled. In the United States, 66% of Americans are homeowners. These homeowners on average have 70% of their net worth invested into their home. Real estate can also be highly profitable for those wanting to create cash flow from owning homes and multifamily homes. Therefore, it is easy to see that the housing market is one area where having insights that others do not have, like housing trends in a certain city or state, will give us a multitude of utility. I would like to buy a multifamily home after I graduate; if I knew what areas would provide me with consistent tenants and a property that goes up in value, I would be very much ahead of the game in the real estate realm and in life in general.\n\nLike most problems in the real-world that heavily impact our lives, there are an overwhelming number of variables and factors that affect the real estate market. Furthermore, there are countless companies and people just like me who would like to uncover a method to reveal information about the market. I am not going to pretend that myself as one person will be successful in predicting whether a home’s price will go up or down a significant amount in the years to come, let alone the exact price of the home. However, I hope this project will reveal some insights that others and I can build off to better know what factors impact real estate the most and ultimately what direction these factors are driving the market.\nAs I said before, there are a multitude of underlying variables that I could focus on, but for the purposes of this project, I will use census data to attempt to reveal housing trends, specifically, if a home will go up in value or not and by how much. There are also different geography levels I could try to predict, from home prices on a certain street to home prices in a certain country, but I chose the state level as I do not know what state I would like to live in yet. Hopefully, the methods in this project can be replicated and applied to any geography level.\n\n\n\nWhat Has Already Been Done\nIn The Winners in China’s Urban Housing Reform, the researchers used census data to reveal who benefited from China’s housing reform that privatized public housing. With the census data, they were able to estimate the housing subsidy received by renters in the public sector by examining private sector prices of houses of comparable quality and size. The census data contained information on six modes of housing tenure that could be compared and information on housing characteristics like housing space, number of bathrooms, ect. John R. Logan and Zhang (2010). Although this study has a different end goal than me, I will use census data in a similar way to try to estimate housing prices.\nIn House Price Prediction Using Regression Techniques: A Comparative Study, the researchers attempt to forecast housing prices using different predictive techniques like multiple linear regression, LASSO, and gradient boosting among others Madhuri, Anuradha, and Pujitha (2019). I could potentially use the same techniques to accomplish my goals.\n\n\nQuestions I Want to Answer\n\nWhich variable or variables in census data reveal the most about the direction of housing prices in a state?\nHow do demographic trends play a role?\nDoes age have an impact?\nDoes sex have an impact?\nDoes race have an impact?\nDoes education level have an impact?\nDoes migration data tell most of the story?\nHow much are home prices impacted by trends in population?\nCan we predict which states to invest in in the United States?\nCan we predict which states are the best for landlords to have a steady flow of tenants?\n\n\n\n\n\n\nReferences\n\nJohn R. Logan, Yiping Fang, and Zhanxin Zhang. 2010. “The Winners in China’s Urban Housing Reform.” Housing Studies 25 (1): 101–17. https://doi.org/10.1080/02673030903240660.\n\n\nMadhuri, CH. Raga, G. Anuradha, and M. Vani Pujitha. 2019. “House Price Prediction Using Regression Techniques: A Comparative Study.” In 2019 International Conference on Smart Structures and Systems (ICSSS), 1–5. https://doi.org/10.1109/ICSSS.2019.8882834."
  },
  {
    "objectID": "index.html#academic-interests",
    "href": "index.html#academic-interests",
    "title": "About Me",
    "section": "Academic Interests",
    "text": "Academic Interests\n\nDeep Learning\nNatural Language Processing\nTime Series Analysis"
  },
  {
    "objectID": "index.html#contact-info",
    "href": "index.html#contact-info",
    "title": "About Me",
    "section": "Contact Info",
    "text": "Contact Info\n\nnetID: nsp50\nEmail: nsp50@georgetown.edu\nAlt. Email: npenoyer34@gmail.com"
  },
  {
    "objectID": "Conclusions.html",
    "href": "Conclusions.html",
    "title": "Nolan Penoyer Website",
    "section": "",
    "text": "I discovered that we can accurately predict what states will have home values that increase or decrease significantly from year to year based on census data. These are promising results, as we can see what states to buy property in just looking at census data that comes out every year. With the regression task, we tried to predict an actual number that represented the percent change of home values after plugging in the census data. With a mean squared error of only .0045, this model shows promise of being useful for its purpose.\nMissing the 2020 year in my dataset was detrimental for my analysis. The percent change from 2019-2021 is higher than other years because it is a longer time period. Also, some of the margin of errors for my census data were high, so not all the data may be accurate. A lot of my variables were correlated as seen with the exploratory data analysis and PCA, but I did not want to remove any data that could be valuable. Same goes for the many outliers in my dataset.\nIn the future, I would like to gather as much data as possible so I can freely remove outliers. I also want to take the time to really decide what few variables impact home values the most. This will help me simplify the models, helping with both interpretation and accuracy. Also, I want to focus on smaller geographic regions and explore other geography hierarchies, as a state is a large area.\nI learned a lot from this project. Although I am not ready to buy a home just yet, I am almost there 😊."
  }
]