[
  {
    "objectID": "NaiveBayes.html",
    "href": "NaiveBayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Introduction to Naive Bayes\nNaive Bayes is a supervised machine learning algorithm used for classification tasks. The algorithm is rooted in Bayes Theorem in that it is based on the probability of a hypothesis, given data and prior knowledge. Below is the equation the Naive Bayes Algorithm uses.\n\nhttps://www.saedsayad.com/naive_bayesian.htm\nWhere P(c|x)= The probability of the target variable class given the features  P(x|c)= The probability of the features given the class  and P(c), P(x) are the prior probabilties of the target variable class and features.\nThe algorithm estimates the probability of each class for a given data point and assing the class with the highest probability to the data point.\nIt assumes that all features are independent of each other; this is usually not the case in the real world but the algorithm still provides accurate predictions.\nUltimatley, through Naive Bayes you want to acheive accurate classifications given the algoirthm is given data with a set of variables. In my case, I would like to predict if a certain state’s home value will increase substantially in a certain year based on census data.\nDifferent variants of Naive Bayes are used for different applications. Multinomial Naive Bayes is used for word counts and frequency analysis (discrete data). Gaussian Naive Bayes is used for numeric data that is approximatley normally distributed and independent. Bernoulli Naive Bayes is used for binary data, like if a word appears in a document or not.\n\n\nPrepare Data for Naive Bayes\n\nimport pandas as pd\n\n\nrecord=pd.read_csv('data/RecordData.csv')\nrecord=record.drop('DP05_0073E',axis=1)\n\n\nrecord.head()\n\n\n\n\n\n\n\n\nYear\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\n...\nDP04_0134E\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0037E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nRegionName\nTypical Home Value\n\n\n\n\n0\n2018\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n...\n0.050667\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\nAlabama\n0.038688\n\n\n1\n2019\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n...\n0.024112\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\nAlabama\n0.070360\n\n\n2\n2021\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n...\n0.066914\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\nAlabama\n0.263465\n\n\n3\n2022\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n...\n0.060395\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\nAlabama\n0.076587\n\n\n4\n2018\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n...\n-0.019983\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\nAlaska\n0.005789\n\n\n\n\n5 rows × 33 columns\n\n\n\nCurrently my explanatory variable is continuious. I must change it to a label variable. To do this, I will choose home value percent changes that are above the mean to 1s, home value percent changes that are below the mean below the mean to 0s.\n\nmean=record['Typical Home Value'].mean()\nrecord['Typical Home Value']=record['Typical Home Value'].apply(lambda x:1 if x&gt;mean else 0)\n\n\nrecord=record.drop(['Year','RegionName'],axis=1)\n\n\nrecord.head()\n\n\n\n\n\n\n\n\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\nDP02_0063E\n...\nDP04_0047E\nDP04_0134E\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0037E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nTypical Home Value\n\n\n\n\n0\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n0.033732\n...\n0.007581\n0.050667\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0\n\n\n1\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n1.429793\n...\n-0.000968\n0.024112\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n0\n\n\n2\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n0.007408\n...\n-0.004602\n0.066914\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n1\n\n\n3\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n0.014854\n...\n0.017788\n0.060395\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n0\n\n\n4\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n-0.065209\n...\n-0.043262\n-0.019983\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0\n\n\n\n\n5 rows × 31 columns\n\n\n\nYou must split your fata into training and testing data sets so you can make sure you do not over or under fit.\n\n\nFeature Selection\n\nimport pandas as pd\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport nltk\nnltk.download('vader_lexicon')\nsid = SentimentIntensityAnalyzer()\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\npeno\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\n\ntext=pd.read_csv('./data/news.csv')\n\n\ntext['score'] = text['2'].apply(lambda x: sid.polarity_scores(x))\ntext['score'] = text['score'].apply(lambda x: x['compound']).astype(float)\n\n\ntext['score'].mean()\n\n-0.11629099999999999\n\n\nWill say if title is above mean it is positive/neutral, below mean negative.\n\ntitles=[]\ny=[]\n#ITERATE OVER ROWS\n# for i in range(0,10):  \nfor i in range(0,text.shape[0]):\n    # QUICKLY CLEAN TEXT\n    keep=\"abcdefghijklmnopqrstuvwxyz \"\n    replace=\".,!;\"\n    tmp=\"\"\n    for char in text[\"2\"][i].replace(\"&lt;br /&gt;\",\"\").lower():\n        if char in replace:\n            tmp+=\" \"\n        if char in keep:\n            tmp+=char\n    tmp=\" \".join(tmp.split())\n    titles.append(tmp)\n    if(text[\"score\"][i]&lt;=text['score'].mean()):\n        y.append(0)\n    if(text[\"score\"][i]&gt;=text['score'].mean()):\n        y.append(1)\n\n\ny=np.array(y)\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef vectorize(corpus,MAX_FEATURES):\n    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n    # RUN COUNT VECTORIZER ON OUR COURPUS \n    Xs  =  vectorizer.fit_transform(corpus)   \n    X=np.array(Xs.todense())\n    #CONVERT TO ONE-HOT VECTORS (can also be done with binary=true in CountVectorizer)\n    maxs=np.max(X,axis=0)\n    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n\n(x,vocab0)=vectorize(titles,MAX_FEATURES=10000)\n\n\nprint(x.shape,y.shape)\n\n(100, 666) (100,)\n\n\n\nvocab1 = dict([(value, key) for key, value in vocab0.items()])\n\n\ndf2=pd.DataFrame(x)\ns = df2.sum(axis=0)\ndf2=df2[s.sort_values(ascending=False).index[:]]\nprint(df2.head())\n\n   173  457  394  262  499  645  372  438  30   433  ...  235  238  246  239  \\\n0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n1  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n2  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n4  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n\n   240  241  242  244  245  665  \n0  0.0  0.0  0.0  0.0  0.0  0.0  \n1  0.0  0.0  0.0  0.0  0.0  0.0  \n2  0.0  0.0  0.0  0.0  0.0  0.0  \n3  0.0  1.0  0.0  0.0  0.0  0.0  \n4  0.0  0.0  0.0  0.0  0.0  0.0  \n\n[5 rows x 666 columns]\n\n\n\nprint()\ni1=0\nvocab2={}\nfor i2 in list(df2.columns):\n    # print(i2)\n    vocab2[i1]=vocab1[int(i2)]\n    i1+=1\n\n\n\n\n\ndf2.columns = range(df2.columns.size)\nprint(df2.head())\nprint(df2.sum(axis=0))\nx=df2.to_numpy()\n\n   0    1    2    3    4    5    6    7    8    9    ...  656  657  658  659  \\\n0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n1  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n2  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n4  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n\n   660  661  662  663  664  665  \n0  0.0  0.0  0.0  0.0  0.0  0.0  \n1  0.0  0.0  0.0  0.0  0.0  0.0  \n2  0.0  0.0  0.0  0.0  0.0  0.0  \n3  0.0  1.0  0.0  0.0  0.0  0.0  \n4  0.0  0.0  0.0  0.0  0.0  0.0  \n\n[5 rows x 666 columns]\n0      15.0\n1      14.0\n2      12.0\n3      11.0\n4       7.0\n       ... \n661     1.0\n662     1.0\n663     1.0\n664     1.0\n665     1.0\nLength: 666, dtype: float64\n\n\n\nimport random\nN=x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\nprint(train_index[0:10])\nprint(test_index[0:10])\n\n[22, 84, 76, 50, 95, 46, 54, 44, 3, 48]\n[96, 92, 14, 51, 91, 26, 63, 49, 36, 98]\n\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\nHere I remove features from high to low.\n\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\npartial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n5 50 50 73.75 45.0\n10 100 100 85.0 55.00000000000001\n15 150 150 87.5 60.0\n20 200 200 95.0 55.00000000000001\n25 250 250 95.0 60.0\n30 300 300 96.25 55.00000000000001\n35 350 350 97.5 55.00000000000001\n40 400 400 97.5 60.0\n45 450 450 97.5 60.0\n50 500 500 97.5 60.0\n55 550 550 97.5 65.0\n60 600 600 98.75 70.0\n65 650 650 100.0 65.0\n70 700 666 100.0 70.0\n75 750 666 100.0 70.0\n80 800 666 100.0 70.0\n85 850 666 100.0 70.0\n90 900 666 100.0 70.0\n95 950 666 100.0 70.0\n100 1000 666 100.0 70.0\n5 3250 666 100.0 70.0\n10 5500 666 100.0 70.0\n15 7750 666 100.0 70.0\n20 10000 666 100.0 70.0\n\n\n\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\nsave_results(\"./data\"+\"/partial_grid_search\")\nplot_results(\"./data\"+\"/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes with Labeled Record Data\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\n\nNameError: name 'record' is not defined\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\nNameError: name 'X' is not defined\n\n\n\nX_train.shape, X_test.shape\n\n((142, 30), (62, 30))\n\n\n\nnaive = GaussianNB()\nnaive.fit(X_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\ny_pred = naive.predict(X_test)\n\n\ny_pred \n\narray([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n       1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n       1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1], dtype=int64)\n\n\n\naccuracy_score(y_test, y_pred)\n\n0.7096774193548387\n\n\nFairly accurate for a first model\n\ny_pred_train = naive.predict(X_train)\n\n\naccuracy_score(y_train, y_pred_train)\n\n0.6197183098591549\n\n\nNot very accurate on the training set so there is some undefitting going on.\nMay need to examine the data more to remove outliers\n\ny_test.value_counts()\n\nTypical Home Value\n0    43\n1    19\nName: count, dtype: int64\n\n\n\n43/(43+19)\n\n0.6935483870967742\n\n\nOur model only does slightly better than just predicting the most frequent class, so the model is not very good.\n\n\nNaive Bayes with Labeled Text Data\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n\ntrain_MNB_model(x,y,i_print=True)\n\n(100, 666) (100,)\n100.0 70.0 0.0 0.0\n\n\n(100.0, 70.0, 0.0, 0.0)\n\n\nHave 100 percent accuracy on training set and 70 percent accuracy on the test set, so overall not a bad model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "GU netID: nsp50\nNolan Penoyer is currently a graduate student in the Data Science and Analytics program at Georgetown University. He received his bachelor’s degree in data science from the United States Air Force Academy. Upon graduation, he will work as an operations research analyst in the Air Force. Recently, the Air Force modernized and reclassified its position to focus more on information and big data. The direction the military is headed is clear, and he desires to be a leader in the implementation of the emerging tools in information and advanced analytics in support of the defense of the United States."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n2025: Georgetown University\n2023: United States Air Force Academy\n2019: East Syracuse Minoa Central Highschool"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "About Me",
    "section": "Interests",
    "text": "Interests\n\nTravel\nSports\nMusic\nReading"
  },
  {
    "objectID": "DecisionTrees.html",
    "href": "DecisionTrees.html",
    "title": "DecisionTrees",
    "section": "",
    "text": "Methods\nDecision trees are a supervised machine learning method which can be used for both classification and regression. The overall goal of a decision tree is to predict the value of a target variable by learning rules based on the features. The process begins at the root node, which represents the entire dataset you are working with. The root node has no parent node, in that there are no nodes before it; it is not the child of any other node. Then the root node is split into two or more sub nodes based on a set of conditional rules of a feature. For example, a rule could be, in my case, the percent change in the number of highschool graduates is greater than 5%. If this a true for the given datapoint, then it will head towards a trajectory in the tree, and if it is not true it will go on a different trajectory. Each non-terminal node, or nodes that have children, is a decision node where a split happens. Decision trees use multiple algorithms to strategically split the tree on certain features and conditions, using various attribute selection measures as well to see which features are most important. The leaf node, or terminal node, is the end of the tree where a prediction is made, which could be either a number or label depending on if it is a classification or regression tree. Decision trees are useful for their interprebility and acccuracy, but can be prone to overfitting. Pruning helps remedy this by remvoing decsision nodes in the tree.\nRandom forests are an extension of decision trees that can also aid in avoiding overfitting while still maintaining accuracy and also can provide us insight on feature importance. The process for creating a random forest goes as follows. You create a bootstrapped dataset, where bootstrapping is sampling your existing dataset with replacement.Then, you create a decision tree from the bootstrapped dataset, but with a random subset of variables at each step.Then you repeat this many times constructing a “forest” of decision trees. Then, when you input a datapoint in your model, it is fed into each tree and each tree makes a prediction, then the trees ‘vote’ for classification, or average the predictions for regression, to make a final prediction. You can see the total amount that the RSS decreased due to splits over a given feature and average over all the trees to see which variables contributed most to decreasing error.\nBoosting works similarly to random forests, but the trees are made sequentially, in that each tree is grown based on information from previous trees. Specifically I will discuss the Adaboost model. Instead of constructing whole decision trees, the trees are usually just a node and two leaves, only using one variable to make a decision. We call it a stump. Individual stumps are weak predictors, but errors that the first stump makes influence how the second stump is made. By attaching weights to each sample from the dataset that you are bootstrapping, the next stump that will be made will take the errors of the current stump into account. By increasing sample weights for incorrectly classified samples, and decreasing sample weights for correctly classfied samples, points that did poorily will be heavily sampled for the creation of the next stump. Each stump also has a different “Amount of say” for the final classification, depending on their performance.\n\n\nClass Distribution\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import randint\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\nHere I will recycle code from my Naive Bayes analysis to transform my continuous target variable into a label.\n\nrecord=pd.read_csv('data/RecordData.csv')\nrecord=record.drop('DP05_0037E',axis=1)\nmean=record['Typical Home Value'].mean()\nrecordClass=record.copy()\nrecordClass['Typical Home Value']=recordClass['Typical Home Value'].apply(lambda x:1 if x&gt;mean else 0)\nrecordClass=recordClass.drop(['Year','RegionName'],axis=1)\nrecordClass.head()\n\n\n\n\n\n\n\n\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\nDP02_0063E\n...\nDP04_0047E\nDP04_0134E\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nDP05_0073E\nTypical Home Value\n\n\n\n\n0\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n0.033732\n...\n0.007581\n0.050667\n0.002692\n0.003198\n0.010283\n-0.000327\n-0.123824\n-0.027097\n0.217687\n0\n\n\n1\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n1.429793\n...\n-0.000968\n0.024112\n0.003133\n-0.006376\n0.002545\n0.009572\n0.054480\n0.015884\n-0.178100\n0\n\n\n2\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n0.007408\n...\n-0.004602\n0.066914\n0.027878\n0.008556\n0.010152\n-0.010947\n0.067268\n0.040587\n-0.005625\n1\n\n\n3\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n0.014854\n...\n0.017788\n0.060395\n0.006829\n-0.001060\n-0.005025\n-0.002353\n-0.017318\n0.146484\n10.243876\n0\n\n\n4\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n-0.065209\n...\n-0.043262\n-0.019983\n-0.003186\n-0.009174\n0.011594\n0.139775\n0.012942\n-0.060828\n0.054155\n0\n\n\n\n\n5 rows × 31 columns\n\n\n\nHere will be my data for my regression model.\n\nrecord=record.drop(['Year','RegionName'],axis=1)\nrecord.head()\n\n\n\n\n\n\n\n\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\nDP02_0063E\n...\nDP04_0047E\nDP04_0134E\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nDP05_0073E\nTypical Home Value\n\n\n\n\n0\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n0.033732\n...\n0.007581\n0.050667\n0.002692\n0.003198\n0.010283\n-0.000327\n-0.123824\n-0.027097\n0.217687\n0.038688\n\n\n1\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n1.429793\n...\n-0.000968\n0.024112\n0.003133\n-0.006376\n0.002545\n0.009572\n0.054480\n0.015884\n-0.178100\n0.070360\n\n\n2\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n0.007408\n...\n-0.004602\n0.066914\n0.027878\n0.008556\n0.010152\n-0.010947\n0.067268\n0.040587\n-0.005625\n0.263465\n\n\n3\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n0.014854\n...\n0.017788\n0.060395\n0.006829\n-0.001060\n-0.005025\n-0.002353\n-0.017318\n0.146484\n10.243876\n0.076587\n\n\n4\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n-0.065209\n...\n-0.043262\n-0.019983\n-0.003186\n-0.009174\n0.011594\n0.139775\n0.012942\n-0.060828\n0.054155\n0.005789\n\n\n\n\n5 rows × 31 columns\n\n\n\n\nrecordClass['Typical Home Value'].value_counts()\n\nTypical Home Value\n0    145\n1     59\nName: count, dtype: int64\n\n\nThe classes are fairly uneven and there is overall not a lot of data, so the algorithm might struggle to learn patterns, especially with the minority class. The model might be biased towards the majority class as well. We also must not only rely on accuracy to understand the performance of our model, as just predicting the majority class the entire time yields a high accuracy score.\n\n\nBaseline Model\n\ndef random_classifier(y_data):\n    ypred=[];\n    max_label=np.max(y_data);\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\",Counter(ypred).values()) # counts the elements' frequency\n    print(\"probability of prediction:\",np.fromiter(Counter(ypred).values(), dtype=float)/len(y_data)) # counts the elements' frequency\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data, ypred))\n    \n\n\nprint(\"\\nBINARY CLASS: UNIFORM LOAD\")\ny=recordClass['Typical Home Value']\nrandom_classifier(y)\n\n\nBINARY CLASS: UNIFORM LOAD\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([94, 110])\nprobability of prediction: [0.46078431 0.53921569]\naccuracy 0.4852941176470588\npercision, recall, fscore, (array([0.71276596, 0.29090909]), array([0.46206897, 0.54237288]), array([0.56066946, 0.37869822]), array([145,  59], dtype=int64))\n\n\nThis is just a model that randomly predicts each label with equal probability. We would obviously hope to better than this model, especially with the recall and f score on the minority class label.\nI’ll do a baseline model for regression as well.\n\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\nnaive_model = DummyRegressor(strategy='mean')\nnaive_model.fit(X_train, y_train)\nnaive_predictions = naive_model.predict(X_test)\nmse = mean_squared_error(y_test, naive_predictions)\nprint(mse)\n\n0.009404738957389904\n\n\n\n\nDecision Tree Baseline\nMy PCA and data exploration revealed that a lot of my variables are highly correlated in my analysis and that I need to get rid of some of my features. For now I will just include all my features and see how a baseline decision tree does.\nI will perform both regression and classification using a basic decision tree. I will start with classification.\n\nX=recordClass.drop(['Typical Home Value'],axis=1)\ny=recordClass['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=tree.DecisionTreeClassifier()\nmodel=model.fit(X_train,y_train)\ny_pred_train=model.predict(X_train)\nprint(classification_report(y_train, y_pred_train))\ncm = confusion_matrix(y_train, y_pred_train)\ncm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\nsns.heatmap(cm_df, annot=True,)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix')\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       102\n           1       1.00      1.00      1.00        40\n\n    accuracy                           1.00       142\n   macro avg       1.00      1.00      1.00       142\nweighted avg       1.00      1.00      1.00       142\n\n\n\n\n\n\nObviously this model perfectly predicted the data it was trained on, lets check the test now.\n\ny_pred=model.predict(X_test)\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\nsns.heatmap(cm_df, annot=True,)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix')\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.88      0.88        43\n           1       0.74      0.74      0.74        19\n\n    accuracy                           0.84        62\n   macro avg       0.81      0.81      0.81        62\nweighted avg       0.84      0.84      0.84        62\n\n\n\n\n\n\nThis performed fairly well, most likely overfit though. We will tune the model later.\n\ndef plot_tree(model):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\n\nplot_tree(model)\n\n\n\n\nOnce I rename the variables this visual will easier to interpret.\nNow lets run the regression.\n\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=4)\nregr_3 = DecisionTreeRegressor(max_depth=50)\nregr_1.fit(X_train, y_train)\nregr_2.fit(X_train, y_train)\nregr_3.fit(X_train, y_train)\nprint(\"MODEL-1: Training, test MSE:\",mean_squared_error(y_train, regr_1.predict(X_train)),mean_squared_error(y_test, regr_1.predict(X_test)))\nprint(\"MODEL-2: Training, test MSE:\",mean_squared_error(y_train, regr_2.predict(X_train)),mean_squared_error(y_test, regr_2.predict(X_test)))\nprint(\"MODEL-3: Training, test MSE:\",mean_squared_error(y_train, regr_3.predict(X_train)),mean_squared_error(y_test, regr_3.predict(X_test)))\n\nMODEL-1: Training, test MSE: 0.0031465562469082313 0.008522672475205358\nMODEL-2: Training, test MSE: 0.0010368920203482712 0.005067322999766866\nMODEL-3: Training, test MSE: 0.0 0.007006317726230445\n\n\nThe max depth of four seems to perform the best. Again, we can tune the depth later in our analysis. This performed better than the dummy regressor.\n\nplot_tree(regr_2)\n\n\n\n\n\n\nRandom Forest\nNow I will use a random forest model to see how well it does above the decision tree baseline.\nWe will start with the classification task.\n\nX=recordClass.drop(['Typical Home Value'],axis=1)\ny=recordClass['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=RandomForestClassifier()\nmodel=model.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\nsns.heatmap(cm_df, annot=True,)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix')\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.93      1.00      0.97        43\n           1       1.00      0.84      0.91        19\n\n    accuracy                           0.95        62\n   macro avg       0.97      0.92      0.94        62\nweighted avg       0.95      0.95      0.95        62\n\n\n\n\n\n\nThis is a pretty good model and classifies well.\nNow lets create a regression random forest model.\n\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=RandomForestRegressor()\nmodel=model.fit(X_train,y_train)\nprint(\"MODEL: Training, test MSE:\",mean_squared_error(y_train, regr_1.predict(X_train)),mean_squared_error(y_test, regr_1.predict(X_test)))\n\nMODEL: Training, test MSE: 0.003146556246908231 0.008522672475205358\n\n\nThe normal decision tree actually performed better. We can investigate this more.\n\n\nModel Tuning\nNow I will find the optimal paramaters for both the decision trees and randm forests\n\nX=recordClass.drop(['Typical Home Value'],axis=1)\ny=recordClass['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\ntest_results=[]\ntrain_results=[]\nfor num_layer in range(1,20):\n    model=tree.DecisionTreeClassifier(max_depth=num_layer)\n    model=model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    y_pred_train=model.predict(X_train)\n    test_results.append([num_layer,accuracy_score(y_test,y_pred),recall_score(y_test,y_pred,pos_label=0),recall_score(y_test,y_pred,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(y_train,y_pred_train),recall_score(y_train,y_pred_train,pos_label=0),recall_score(y_train,y_pred_train,pos_label=1)])\n    \n\n\ntest_results=np.array(test_results)\ntrain_results=np.array(train_results)\n\n\ncol=1\nplt.plot(test_results[:, 0], test_results[:, col], '-or')\nplt.plot(train_results[:, 0], train_results[:, col], '-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('ACCURACY (Y=0): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\ncol=2\nplt.plot(test_results[:, 0], test_results[:, col], '-or')\nplt.plot(train_results[:, 0], train_results[:, col], '-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y=0): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\ncol=3\nplt.plot(test_results[:, 0], test_results[:, col], '-or')\nplt.plot(train_results[:, 0], train_results[:, col], '-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y=1): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\nLooks like optimal max_depth is 3.\n\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\nr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=4)\nregr_3 = DecisionTreeRegressor(max_depth=50)\nregr_1.fit(X_train, y_train)\nregr_2.fit(X_train, y_train)\nregr_3.fit(X_train, y_train)\nprint(\"MODEL-1: Training, test MSE:\",mean_squared_error(y_train, regr_1.predict(X_train)),mean_squared_error(y_test, regr_1.predict(X_test)))\nprint(\"MODEL-2: Training, test MSE:\",mean_squared_error(y_train, regr_2.predict(X_train)),mean_squared_error(y_test, regr_2.predict(X_test)))\nprint(\"MODEL-3: Training, test MSE:\",mean_squared_error(y_train, regr_3.predict(X_train)),mean_squared_error(y_test, regr_3.predict(X_test)))\n\n\ntest_results=[]\ntrain_results=[]\nfor num_layer in range(1,20):\n    model=tree.DecisionTreeRegressor(max_depth=num_layer)\n    model=model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    y_pred_train=model.predict(X_train)\n    test_results.append([num_layer,accuracy_score(y_test,y_pred),recall_score(y_test,y_pred,pos_label=0),recall_score(y_test,y_pred,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(y_train,y_pred_train),recall_score(y_train,y_pred_train,pos_label=0),recall_score(y_train,y_pred_train,pos_label=1)])\n    \n\n\ntest_results=np.array(test_results)\ntrain_results=np.array(train_results)\n\n\ncol=1\nplt.plot(test_results[:, 0], test_results[:, col], '-or')\nplt.plot(train_results[:, 0], train_results[:, col], '-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('ACCURACY (Y=0): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\ncol=2\nplt.plot(test_results[:, 0], test_results[:, col], '-or')\nplt.plot(train_results[:, 0], train_results[:, col], '-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y=0): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\ncol=3\nplt.plot(test_results[:, 0], test_results[:, col], '-or')\nplt.plot(train_results[:, 0], train_results[:, col], '-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y=1): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\nLooks like 3 or 4.\nNow on to the random forests.\n\nX=recordClass.drop(['Typical Home Value'],axis=1)\ny=recordClass['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=RandomForestClassifier()\n\n\nparam = {'n_estimators': randint(50,500),'max_depth': randint(1,20)}\nsearch = RandomizedSearchCV(model, param_distributions = param, n_iter=5, cv=5)\n\n\nsearch.fit(X_train, y_train)\n\nRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=5,\n                   param_distributions={'max_depth': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F535775F10&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F532392390&gt;})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=5,\n                   param_distributions={'max_depth': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F535775F10&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F532392390&gt;})estimator: RandomForestClassifierRandomForestClassifier()RandomForestClassifierRandomForestClassifier()\n\n\n\nbestParams = search.best_params_\nprint(f'Best Hyperparameters: {bestParams}')\n\nBest Hyperparameters: {'max_depth': 13, 'n_estimators': 419}\n\n\nFor the classification random forest model has the optimal performance with max_depth=6 and number of estimators=386.\n\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=RandomForestRegressor()\n\n\nparam = {'n_estimators': randint(50,500),'max_depth': randint(1,20)}\nsearch = RandomizedSearchCV(model, param_distributions = param, n_iter=5, cv=5)\n\n\nsearch.fit(X_train, y_train)\n\nRandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=5,\n                   param_distributions={'max_depth': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F532694AD0&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F532695050&gt;})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=5,\n                   param_distributions={'max_depth': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F532694AD0&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F532695050&gt;})estimator: RandomForestRegressorRandomForestRegressor()RandomForestRegressorRandomForestRegressor()\n\n\n\nbestParams = search.best_params_\nprint(f'Best Hyperparameters: {bestParams}')\n\nBest Hyperparameters: {'max_depth': 15, 'n_estimators': 412}\n\n\nFor the classification random forest model has the optimal performance with max_depth=15 and number of estimators=412.\n\n\nFinal Results\nNow lets train the models with our tuned hyper parameters and see their performance.\n\nX=recordClass.drop(['Typical Home Value'],axis=1)\ny=recordClass['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=tree.DecisionTreeClassifier(max_depth=3)\nmodel=model.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\nsns.heatmap(cm_df, annot=True,)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix')\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.84      0.82        43\n           1       0.59      0.53      0.56        19\n\n    accuracy                           0.74        62\n   macro avg       0.69      0.68      0.69        62\nweighted avg       0.74      0.74      0.74        62\n\n\n\n\n\n\nChanging the depth here doesnt change much.\n\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel = DecisionTreeRegressor(max_depth=4)\nmodel.fit(X_train, y_train)\nprint(\"MODEL: Training, test MSE:\",mean_squared_error(y_train, model.predict(X_train)),mean_squared_error(y_test, regr_1.predict(X_test)))\n\nMODEL: Training, test MSE: 0.0010368920203482712 0.008522672475205358\n\n\n\nX=recordClass.drop(['Typical Home Value'],axis=1)\ny=recordClass['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=RandomForestClassifier(max_depth= 13,n_estimators=419)\nmodel=model.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(classification_report(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\ncm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\nsns.heatmap(cm_df, annot=True,)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix')\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.93      1.00      0.97        43\n           1       1.00      0.84      0.91        19\n\n    accuracy                           0.95        62\n   macro avg       0.97      0.92      0.94        62\nweighted avg       0.95      0.95      0.95        62\n\n\n\n\n\n\n\nX=record.drop(['Typical Home Value'],axis=1)\ny=record['Typical Home Value']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nmodel=RandomForestRegressor(max_depth=15,n_estimators=412)\nmodel=model.fit(X_train,y_train)\nprint(\"MODEL: Training, test MSE:\",mean_squared_error(y_train, regr_1.predict(X_train)),mean_squared_error(y_test, regr_1.predict(X_test)))\n\nMODEL: Training, test MSE: 0.0031465562469082313 0.008522672475205358\n\n\nMy final models did a fairly good job with predicting the percent increase in home value in states. With a random forest, the classification model only misclassfied three data points.My regression models still need some more work. The decision tree actually performed better than the random forest with the lowest mean squared error being .0059, this is compared to the baseline of .009. We posssibly could improve the regression decision tree and random forests be reducing the number of features as I suspect that is an issue.\n\n\nConclusion\nI discovered that we can accuratly predict what states will have a signficant percent change from year to year based on census data. This is promising results, as we can see what states to buy property in just looking at census data that comes out every year. With the regression task, we tried to predict an actual number that represented the percent increase of home values after plugging in the census data percent change. With a mean squared error of only .0059, this model shows promise of being useful for its purpose.\nMissing the 2020 year in my data was detrimental for my analysis as the percent change from 2019-2021 is higher than other years because it is a longer time period. Also, some of the margin of errors for my census data was fairly high, so not all the data may be accurate. In the future, I want to focus on smaller geographic regions and explore other geography hierarchies, as a state is a large area."
  },
  {
    "objectID": "DataExploration.html",
    "href": "DataExploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndf = pd.read_csv('./data/RecordData.csv')\ndf.head()\n\n\n\n\n\n\n\n\nYear\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\n...\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0037E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nDP05_0073E\nRegionName\nTypical Home Value\n\n\n\n\n0\n2018\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n...\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.217687\nAlabama\n0.038688\n\n\n1\n2019\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n...\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n-0.178100\nAlabama\n0.070360\n\n\n2\n2021\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n...\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n-0.005625\nAlabama\n0.263465\n\n\n3\n2022\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n...\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n10.243876\nAlabama\n0.076587\n\n\n4\n2018\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n...\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.054155\nAlaska\n0.005789\n\n\n\n\n5 rows × 34 columns\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nYear\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\n...\nDP04_0134E\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0037E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nDP05_0073E\nTypical Home Value\n\n\n\n\ncount\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n...\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n204.000000\n\n\nmean\n2020.000000\n0.020044\n-0.050633\n-0.069625\n-0.089833\n-0.206925\n0.524127\n-0.109497\n-0.220982\n0.093385\n...\n0.056447\n0.006469\n0.002941\n0.005769\n-0.021826\n0.001579\n0.059462\n0.019590\n4.530851\n0.110084\n\n\nstd\n1.585028\n0.017131\n0.120031\n0.157685\n0.232427\n0.365501\n0.945060\n0.198461\n0.329615\n0.215643\n...\n0.033730\n0.010885\n0.007600\n0.005859\n0.059540\n0.112276\n0.268675\n0.078313\n11.884183\n0.099373\n\n\nmin\n2018.000000\n-0.009240\n-0.355454\n-0.490505\n-0.554143\n-0.888434\n-0.346730\n-0.656774\n-0.870279\n-0.112117\n...\n-0.019983\n-0.050583\n-0.020735\n-0.010390\n-0.383837\n-0.387319\n-0.578552\n-0.274559\n-0.451586\n-0.016960\n\n\n25%\n2018.750000\n0.009339\n-0.065194\n-0.128874\n-0.290981\n-0.361259\n-0.017399\n-0.161193\n-0.355393\n-0.013111\n...\n0.031688\n-0.000028\n-0.001061\n0.002532\n-0.017285\n-0.032278\n-0.078743\n-0.013964\n-0.009074\n0.049067\n\n\n50%\n2020.000000\n0.016837\n0.004610\n-0.002334\n-0.026105\n-0.022791\n0.063992\n-0.047421\n-0.056749\n0.005985\n...\n0.052233\n0.004786\n0.002001\n0.005263\n-0.001994\n0.004109\n0.025383\n0.017288\n0.078421\n0.066658\n\n\n75%\n2021.250000\n0.025940\n0.021921\n0.030698\n0.047844\n0.020779\n0.951449\n0.011528\n-0.004048\n0.052998\n...\n0.078260\n0.012023\n0.007316\n0.008433\n0.004153\n0.026074\n0.104096\n0.049673\n1.078422\n0.135000\n\n\nmax\n2022.000000\n0.096015\n0.083260\n0.210279\n0.571831\n0.310981\n4.711793\n0.583672\n0.233134\n1.166320\n...\n0.176136\n0.063712\n0.024341\n0.026667\n0.042915\n0.940551\n1.598558\n0.394246\n85.273260\n0.576048\n\n\n\n\n8 rows × 33 columns\n\n\n\nAs you can see there may be some problematic columns that we may not want to use in our analysis. For now lest drop the DP05_0073E as the standard deviation is very high which may indicate incorrectly measured data.\n\ndf=df.drop('DP05_0073E',axis=1)\n\n\nAlabama_data = df[df['RegionName'] == 'Alabama']\n\nplt.figure(figsize=(9, 6))\nplt.plot(Alabama_data['Year'], Alabama_data['Typical Home Value'], marker='o', linestyle='-')\nplt.title(\"Percent Change of Typical Home Value in Alabama\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Typical Home Value Percent Change\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(9, 6))\nplt.plot(Alabama_data['Year'], Alabama_data['Typical Home Value'], label='Typical Home Value', marker='o', linestyle='-')\nplt.plot(Alabama_data['Year'], Alabama_data['DP03_0026PE'], label='Num People Employed', marker='o', linestyle='-')\nplt.title(\"Percent Change of Typical Home Value and Number of People Employed in Alabama\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Percent Change\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\nNewYork_data = df[df['RegionName'] == 'New York']\n\nplt.figure(figsize=(9, 6))\nplt.plot(NewYork_data['Year'], NewYork_data['Typical Home Value'], marker='o', linestyle='-')\nplt.title(\"Percent Change of Typical Home Value in New York\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Typical Home Value Percent Change\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(9, 6))\nplt.plot(NewYork_data['Year'], NewYork_data['Typical Home Value'], label='Typical Home Value', marker='o', linestyle='-')\nplt.plot(NewYork_data['Year'], NewYork_data['DP03_0026PE'], label='Num People Employed', marker='o', linestyle='-')\nplt.title(\"Percent Change of Typical Home Value and Number of People Employed in NewYork\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Percent Change\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\nMichigan_data = df[df['RegionName'] == 'Michigan']\n\nplt.figure(figsize=(9, 6))\nplt.plot(Michigan_data['Year'], Michigan_data['Typical Home Value'], marker='o', linestyle='-')\nplt.title(\"Percent Change of Typical Home Value in Michigan\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Typical Home Value Percent Change\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(9, 6))\nplt.plot(Michigan_data['Year'], Michigan_data['Typical Home Value'], label='Typical Home Value', marker='o', linestyle='-')\nplt.plot(Michigan_data['Year'], Michigan_data['DP03_0026PE'], label='Num People Employed', marker='o', linestyle='-')\nplt.title(\"Percent Change of Typical Home Value and Number of People Employed in NewYork\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Percent Change\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(9, 6))\nsns.boxplot(data=df[['DP02_0062E']])\nplt.title('Box Plots of Goals For Highschool Graduate Changes')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(9, 6))\nsns.boxplot(data=df[['DP04_0002E']])\nplt.title('Box Plots of Goals For Occupied Housing Units Changes')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(9, 6))\nsns.boxplot(data=df[['DP05_0001E']])\nplt.title('Box Plots of Goals For Total Population Changes')\nplt.show()\n\n\n\n\nA lot of Outliers in alot of my variables. I should try to remove them.\n\ndf2=df.drop(['Year','RegionName'],axis=1)\n\n\ndf2.corr()\n\n\n\n\n\n\n\n\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\nDP02_0063E\n...\nDP04_0047E\nDP04_0134E\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0037E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nTypical Home Value\n\n\n\n\nDP02_0001E\n1.000000\n0.438800\n0.465624\n0.248086\n0.336708\n-0.260457\n0.312382\n0.287555\n-0.294137\n-0.318927\n...\n0.368283\n0.421822\n0.390172\n0.331799\n0.277745\n-0.536511\n-0.153728\n0.009718\n0.049971\n0.578869\n\n\nDP02_0002E\n0.438800\n1.000000\n0.981878\n0.790430\n0.945954\n-0.831271\n0.757746\n0.931202\n-0.766019\n-0.920144\n...\n0.063216\n0.497780\n0.276947\n0.331144\n0.021856\n-0.274077\n-0.166218\n0.019215\n0.078385\n0.444488\n\n\nDP02_0003E\n0.465624\n0.981878\n1.000000\n0.771685\n0.943687\n-0.815491\n0.781559\n0.917415\n-0.774410\n-0.906359\n...\n0.090782\n0.499087\n0.241894\n0.325107\n0.017160\n-0.278718\n-0.198401\n-0.003718\n0.081323\n0.447811\n\n\nDP02_0007E\n0.248086\n0.790430\n0.771685\n1.000000\n0.797750\n-0.742020\n0.682252\n0.800932\n-0.621534\n-0.745090\n...\n0.093202\n0.412152\n0.194073\n0.243893\n0.024614\n-0.212158\n-0.045523\n0.056621\n0.111638\n0.364732\n\n\nDP02_0011E\n0.336708\n0.945954\n0.943687\n0.797750\n1.000000\n-0.894087\n0.803087\n0.972320\n-0.749640\n-0.896076\n...\n0.093006\n0.438733\n0.151822\n0.228283\n0.085614\n-0.219316\n-0.214236\n0.019388\n0.115063\n0.338099\n\n\nDP02_0037E\n-0.260457\n-0.831271\n-0.815491\n-0.742020\n-0.894087\n1.000000\n-0.711113\n-0.893892\n0.592598\n0.785771\n...\n-0.040640\n-0.411501\n-0.118924\n-0.275845\n-0.072924\n0.213962\n0.149411\n-0.005464\n-0.171662\n-0.327840\n\n\nDP02_0060E\n0.312382\n0.757746\n0.781559\n0.682252\n0.803087\n-0.711113\n1.000000\n0.782436\n-0.655789\n-0.724942\n...\n0.135533\n0.460252\n0.192067\n0.237910\n0.159691\n-0.212471\n-0.298075\n0.009762\n0.029356\n0.297504\n\n\nDP02_0061E\n0.287555\n0.931202\n0.917415\n0.800932\n0.972320\n-0.893892\n0.782436\n1.000000\n-0.752483\n-0.875352\n...\n0.109633\n0.424624\n0.179373\n0.191868\n0.090643\n-0.152399\n-0.195105\n0.055358\n0.137906\n0.290155\n\n\nDP02_0062E\n-0.294137\n-0.766019\n-0.774410\n-0.621534\n-0.749640\n0.592598\n-0.655789\n-0.752483\n1.000000\n0.625286\n...\n-0.004732\n-0.423344\n-0.185994\n-0.201587\n-0.058643\n0.172882\n0.132303\n0.004590\n-0.111726\n-0.296042\n\n\nDP02_0063E\n-0.318927\n-0.920144\n-0.906359\n-0.745090\n-0.896076\n0.785771\n-0.724942\n-0.875352\n0.625286\n1.000000\n...\n-0.068992\n-0.410235\n-0.127849\n-0.290762\n-0.002780\n0.222860\n0.165423\n-0.042741\n-0.026843\n-0.339538\n\n\nDP02_0064E\n0.317707\n0.947215\n0.930499\n0.808561\n0.954101\n-0.873807\n0.742061\n0.942365\n-0.734199\n-0.921569\n...\n0.041452\n0.421006\n0.202411\n0.261304\n0.052382\n-0.213265\n-0.163936\n0.034185\n0.103434\n0.368901\n\n\nDP02_0065E\n-0.212064\n-0.779303\n-0.778599\n-0.715225\n-0.855243\n0.855915\n-0.739918\n-0.865743\n0.584652\n0.712947\n...\n-0.078217\n-0.360112\n-0.060273\n-0.220864\n-0.049123\n0.183743\n0.210422\n-0.048662\n-0.174529\n-0.245080\n\n\nDP03_0026PE\n-0.164870\n-0.003424\n-0.035541\n0.058006\n0.031240\n0.019247\n0.050115\n0.095316\n-0.012144\n0.016294\n...\n0.197981\n0.180419\n0.199823\n-0.381214\n-0.100719\n0.501511\n0.082050\n0.009338\n0.120333\n-0.247353\n\n\nDP03_0005E\n0.474298\n0.179621\n0.192406\n0.019238\n0.081436\n-0.064336\n0.111410\n0.031265\n-0.108437\n-0.100239\n...\n-0.192926\n0.128883\n0.342345\n0.444477\n0.206341\n-0.617429\n-0.175998\n0.068743\n-0.036782\n0.549155\n\n\nDP03_0062E\n-0.024458\n-0.004581\n-0.021028\n-0.010217\n-0.068167\n0.019591\n-0.012365\n-0.070054\n0.021158\n0.063929\n...\n-0.274300\n0.437673\n0.252681\n0.059726\n-0.224007\n0.052497\n-0.076748\n-0.040125\n0.118803\n0.242465\n\n\nDP03_0063E\n0.129516\n0.169651\n0.165921\n0.104215\n0.115443\n-0.128515\n0.078931\n0.114442\n-0.101892\n-0.098668\n...\n-0.134462\n0.542114\n0.318451\n0.082443\n-0.122832\n-0.061203\n-0.164692\n-0.135585\n0.213101\n0.450604\n\n\nDP03_0093E\n0.521425\n0.358641\n0.380646\n0.239736\n0.267579\n-0.248847\n0.222373\n0.209678\n-0.291935\n-0.254937\n...\n-0.165930\n0.402790\n0.242736\n0.490989\n0.080710\n-0.520318\n-0.167742\n-0.061886\n0.058246\n0.613716\n\n\nDP03_0094E\n0.527228\n0.396203\n0.417353\n0.297018\n0.314391\n-0.319496\n0.320358\n0.255450\n-0.255274\n-0.341490\n...\n-0.106647\n0.449579\n0.233298\n0.499738\n0.025937\n-0.598228\n-0.333074\n-0.037168\n-0.015659\n0.681039\n\n\nDP04_0002E\n1.000000\n0.438800\n0.465624\n0.248086\n0.336708\n-0.260457\n0.312382\n0.287555\n-0.294137\n-0.318927\n...\n0.368283\n0.421822\n0.390172\n0.331799\n0.277745\n-0.536511\n-0.153728\n0.009718\n0.049971\n0.578869\n\n\nDP04_0001E\n0.686693\n0.172911\n0.203209\n0.019719\n0.110703\n-0.051470\n0.155896\n0.083528\n-0.148170\n-0.090230\n...\n0.372963\n0.229018\n0.221057\n0.102697\n0.222890\n-0.283962\n0.003257\n0.108058\n0.029278\n0.208816\n\n\nDP04_0037E\n0.196957\n0.217412\n0.214178\n0.151148\n0.150782\n-0.132356\n0.065169\n0.121083\n-0.129029\n-0.188487\n...\n-0.247484\n0.135262\n0.309887\n0.268522\n0.065965\n-0.321603\n-0.235029\n-0.039576\n0.070796\n0.502435\n\n\nDP04_0047E\n0.368283\n0.063216\n0.090782\n0.093202\n0.093006\n-0.040640\n0.135533\n0.109633\n-0.004732\n-0.068992\n...\n1.000000\n0.088938\n-0.028178\n-0.058786\n0.149449\n0.031237\n0.027316\n0.028427\n0.026458\n-0.124036\n\n\nDP04_0134E\n0.421822\n0.497780\n0.499087\n0.412152\n0.438733\n-0.411501\n0.460252\n0.424624\n-0.423344\n-0.410235\n...\n0.088938\n1.000000\n0.420228\n0.334809\n-0.060136\n-0.268605\n-0.180041\n-0.021600\n0.112019\n0.570452\n\n\nDP05_0001E\n0.390172\n0.276947\n0.241894\n0.194073\n0.151822\n-0.118924\n0.192067\n0.179373\n-0.185994\n-0.127849\n...\n-0.028178\n0.420228\n1.000000\n0.216560\n-0.059961\n-0.053172\n-0.080334\n0.018665\n0.225440\n0.544573\n\n\nDP05_0004E\n0.331799\n0.331144\n0.325107\n0.243893\n0.228283\n-0.275845\n0.237910\n0.191868\n-0.201587\n-0.290762\n...\n-0.058786\n0.334809\n0.216560\n1.000000\n-0.016104\n-0.420289\n-0.006381\n-0.009656\n-0.131272\n0.539996\n\n\nDP05_0018E\n0.277745\n0.021856\n0.017160\n0.024614\n0.085614\n-0.072924\n0.159691\n0.090643\n-0.058643\n-0.002780\n...\n0.149449\n-0.060136\n-0.059961\n-0.016104\n1.000000\n-0.230943\n-0.119643\n0.039093\n0.032161\n0.028251\n\n\nDP05_0037E\n-0.536511\n-0.274077\n-0.278718\n-0.212158\n-0.219316\n0.213962\n-0.212471\n-0.152399\n0.172882\n0.222860\n...\n0.031237\n-0.268605\n-0.053172\n-0.420289\n-0.230943\n1.000000\n0.168399\n-0.169048\n0.027705\n-0.589996\n\n\nDP05_0038E\n-0.153728\n-0.166218\n-0.198401\n-0.045523\n-0.214236\n0.149411\n-0.298075\n-0.195105\n0.132303\n0.165423\n...\n0.027316\n-0.180041\n-0.080334\n-0.006381\n-0.119643\n0.168399\n1.000000\n0.039821\n-0.071010\n-0.254423\n\n\nDP05_0039E\n0.009718\n0.019215\n-0.003718\n0.056621\n0.019388\n-0.005464\n0.009762\n0.055358\n0.004590\n-0.042741\n...\n0.028427\n-0.021600\n0.018665\n-0.009656\n0.039093\n-0.169048\n0.039821\n1.000000\n-0.060736\n-0.066153\n\n\nDP05_0044E\n0.049971\n0.078385\n0.081323\n0.111638\n0.115063\n-0.171662\n0.029356\n0.137906\n-0.111726\n-0.026843\n...\n0.026458\n0.112019\n0.225440\n-0.131272\n0.032161\n0.027705\n-0.071010\n-0.060736\n1.000000\n0.079361\n\n\nTypical Home Value\n0.578869\n0.444488\n0.447811\n0.364732\n0.338099\n-0.327840\n0.297504\n0.290155\n-0.296042\n-0.339538\n...\n-0.124036\n0.570452\n0.544573\n0.539996\n0.028251\n-0.589996\n-0.254423\n-0.066153\n0.079361\n1.000000\n\n\n\n\n31 rows × 31 columns\n\n\n\n\nplt.figure(figsize=(12, 8)) \nsns.heatmap(df2.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\nIt seems my initial hypotheses were correct in that most of the questions I was asking are relevant to home prices."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering my data may reveal if certain states or years may be grouped together. This can reveal trends in my data set, like if certain regions are developing in similar ways.\n\nKMEAN\nK-means Clustering seperates an unlabeled dataset into distinct clusters. In this method, the parameter K signifies the predetermined number of clusters to be formed during the process, or another way to say it, the number of centroids (means).For instance, if K=2, the data will be grouped into two clusters, and if K=3, there will be three clusters and three centroids, ect. Every data point is then assigned to each of the clusters to reduce the in-cluster sum of squares  The process goes as follows. First you initialize the number of random centroids. You can determine this figure with model selection methods which I will talk about below. Then, you compute the distance from each data point to each centroid and assign the point to the nearest centroid. Then, you compute the centroids as the mean points of the cluster and repeat computing distance, reassigning points, recomputing centroids until convergence.\n\n\nDBSAN\nWhile K-means clustering is better for normal shaped data, density based clustering is more efficient for oddly shaped data. The underlying concept is that a cluster in the data space is characterized by a region with high point density, and it is separated from other clusters by regions with low point density. The two parameters required for DBSCAN are epsilon and mininum points, where epsilon is basicially the maximum distance from another point for two data points to be considered “neighbors”, and mininum points is the mininum number of neighbors a data point must have to be considered a core point.  The process goes as follows. Find the neighbor points and identify the core points. For each core point, assign it to a cluster if its not already. Then you find all the connected points and assign them to the same cluster as the core point. Its like a chain, in that if there are a,b, and c data points, if a and b are neighbors, and c and b are neighbors, then a and c are then neighbors. Then you iterate through the unvisited points again.\n\n\nHierarchial Clustering\nHierarchial clustering builds a hierarchy of clusters in the from of a dendogram, allowing us to visualize the relationships between data points. There are top down and bottom up algorithms, where bottom up treats each data point as a single cluster then merges the clusters together until there is one big cluster. Top down does the opposite.\nThe bottom up process goes as follows. Assign each data point as a single cluster. Then, find the closet pair of clusters and make them one cluster. Then, find the two closest clusters and make them one cluster. Repeat merging the clusters together until one cluster remains\n\n\nModel Selection Methods\nThese methods aid in determining the optimal number of clusters.\nElbow: Inertia is the sum of squared distance of samples to their closest cluster, or put simply a measure of how “well clustered” a datset is. You want low inertia and low k. In the elbow method, you plut inertia against the number of clusters and find the k value where the decrease in inertia begins to signficantly slow.\nSilhouette Score: The sihouette score measures similarity of an object to its own cluster in comparison to other clusters. A higher silhouette score indicates well-separated clusters and helps you determine the number of clusters.\n\n\nData Selection\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.decomposition import PCA\n\n\nrecord=pd.read_csv('data/RecordData.csv')\nrecord=record.drop(['Year','DP05_0073E','RegionName'],axis=1)\nrecord.head()\n\n\n\n\n\n\n\n\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\nDP02_0063E\n...\nDP04_0047E\nDP04_0134E\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0037E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nTypical Home Value\n\n\n\n\n0\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n0.033732\n...\n0.007581\n0.050667\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.038688\n\n\n1\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n1.429793\n...\n-0.000968\n0.024112\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n0.070360\n\n\n2\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n0.007408\n...\n-0.004602\n0.066914\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n0.263465\n\n\n3\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n0.014854\n...\n0.017788\n0.060395\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n0.076587\n\n\n4\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n-0.065209\n...\n-0.043262\n-0.019983\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.005789\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n\nHyperparameter Tuning\n\nscaler=StandardScaler()\nscaled=scaler.fit_transform(record)\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(scaled)\n\n\nagg_cluster = AgglomerativeClustering(n_clusters=5, linkage='ward')\nagg_labels = agg_cluster.fit_predict(record)\nplt.figure(figsize=(8, 5))\nplt.scatter(components[:, 0], components[:, 1], c=agg_labels, cmap='plasma')\nplt.title('Agglomerative Clustering')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.colorbar(label='Cluster')\nplt.show()\n\n\n\n\n\ninertia = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(record)\n    inertia.append(kmeans.inertia_)\n\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, 11), inertia, marker='o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal k')\nplt.show()\n\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\npeno\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\n\n\n\n\ndbscan = DBSCAN(eps=0.5, min_samples=5) \ndbscan_labels = dbscan.fit_predict(scaled)\nplt.figure(figsize=(8, 5))\nplt.scatter(components[:, 0], components[:, 1], c=dbscan_labels, cmap='viridis')\nplt.title('DBSCAN Clustering')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.colorbar(label='Cluster')\nplt.show()\n\n\n\n\n\n\nResults\nIt seems that hierachial clustering had the most meaningful results.I think there are a lot of connections to be made in my data because it is based on years and states. Obviously some years there will be higher percent changes overall, and states close to each other most likely move in similar directions\n\n\nConclusion\nIt is difficult to conclude much from this analysis as clustering does not apply too much to my project."
  },
  {
    "objectID": "DataCleaning.html",
    "href": "DataCleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "import requests\nimport json\nimport re\nimport pandas as pd\n\nI clean all the data here so I can begin to do analysis and try to build some useful models. I’ll start by reading in a csv, selecting some columns, and changing the column names.\n\nDP02cols=['NAME','DP02_0001E','DP02_0002E','DP02_0003E','DP02_0007E','DP02_0011E','DP02_0037E','DP02_0060E','DP02_0061E','DP02_0062E','DP02_0063E','DP02_0064E','DP02_0065E']\nDP03cols=['NAME','DP03_0026PE','DP03_0005E','DP03_0062E','DP03_0063E','DP03_0093E','DP03_0094E']\nDP04cols=['NAME','DP04_0002E','DP04_0001E','DP04_0037E','DP04_0047E','DP04_0134E']\nDP05cols=['NAME','DP05_0001E','DP05_0004E','DP05_0018E','DP05_0037E','DP05_0038E','DP05_0039E','DP05_0044E','DP05_0073E']\n\n\nDP0217=pd.read_csv('data/2017DP02.csv',skiprows=1)\nDP0218=pd.read_csv('data/2018DP02.csv',skiprows=1)\nDP0219=pd.read_csv('data/2019DP02.csv',skiprows=1)\nDP0221=pd.read_csv('data/2021DP02.csv',skiprows=1)\nDP0222=pd.read_csv('data/2022DP02.csv',skiprows=1)\nDP0317 = pd.read_csv('data/2017DP03.csv', skiprows=1)\nDP0318 = pd.read_csv('data/2018DP03.csv', skiprows=1)\nDP0319 = pd.read_csv('data/2019DP03.csv', skiprows=1)\nDP0321 = pd.read_csv('data/2021DP03.csv', skiprows=1)\nDP0322 = pd.read_csv('data/2022DP03.csv', skiprows=1)\nDP0417 = pd.read_csv('data/2017DP04.csv', skiprows=1)\nDP0418 = pd.read_csv('data/2018DP04.csv', skiprows=1)\nDP0419 = pd.read_csv('data/2019DP04.csv', skiprows=1)\nDP0421 = pd.read_csv('data/2021DP04.csv', skiprows=1)\nDP0422 = pd.read_csv('data/2022DP04.csv', skiprows=1)\nDP0517 = pd.read_csv('data/2017DP05.csv', skiprows=1)\nDP0518 = pd.read_csv('data/2018DP05.csv', skiprows=1)\nDP0519 = pd.read_csv('data/2019DP05.csv', skiprows=1)\nDP0521 = pd.read_csv('data/2021DP05.csv', skiprows=1)\nDP0522 = pd.read_csv('data/2022DP05.csv', skiprows=1)\n\n\nDP0217=DP0217[DP02cols]\nDP0217['Year']=2017\nDP0218=DP0218[DP02cols]\nDP0218['Year']=2018\nDP0219=DP0219[DP02cols]\nDP0219['Year']=2019\nDP0221=DP0221[DP02cols]\nDP0221['Year']=2021\nDP0222=DP0222[DP02cols]\nDP0222['Year']=2022\n\n\nDP0317 = DP0317[DP03cols]\nDP0318 = DP0318[DP03cols]\nDP0319 = DP0319[DP03cols]\nDP0321 = DP0321[DP03cols]\nDP0322 = DP0322[DP03cols]\n\n\nDP0417 = DP0417[DP04cols]\nDP0418 = DP0418[DP04cols]\nDP0419 = DP0419[DP04cols]\nDP0421 = DP0421[DP04cols]\nDP0422 = DP0422[DP04cols]\n\n\nDP0517 = DP0517[DP05cols]\nDP0518 = DP0518[DP05cols]\nDP0519 = DP0519[DP05cols]\nDP0521 = DP0521[DP05cols]\nDP0522 = DP0522[DP05cols]\n\n\ndf2017=pd.merge(DP0217,DP0317,on='NAME')\ndf2017=pd.merge(df2017,DP0417,on='NAME')\ndf2017=pd.merge(df2017,DP0517,on='NAME')\n\ndf2018=pd.merge(DP0218,DP0318,on='NAME')\ndf2018=pd.merge(df2018,DP0418,on='NAME')\ndf2018=pd.merge(df2018,DP0518,on='NAME')\n\ndf2019=pd.merge(DP0219,DP0319,on='NAME')\ndf2019=pd.merge(df2019,DP0419,on='NAME')\ndf2019=pd.merge(df2019,DP0519,on='NAME')\n\n\ndf2021=pd.merge(DP0221,DP0321,on='NAME')\ndf2021=pd.merge(df2021,DP0421,on='NAME')\ndf2021=pd.merge(df2021,DP0521,on='NAME')\n\ndf2022=pd.merge(DP0222,DP0322,on='NAME')\ndf2022=pd.merge(df2022,DP0422,on='NAME')\ndf2022=pd.merge(df2022,DP0522,on='NAME')\n\n\nus_states = [\n    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\",\n    \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\",\n    \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n    \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\",\n    \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\",\n    \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\",\n    \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\",\n    \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\",\n    \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\",\n    \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\",\n    \"District of Columbia\"\n]\n\n\nfeature_df=pd.DataFrame()\n\n\nfor state in us_states:\n    row1=df2017[df2017['NAME']==state]\n    row2=df2018[df2018['NAME']==state]\n    row3=df2019[df2019['NAME']==state]\n    row4=df2021[df2021['NAME']==state]\n    row5=df2022[df2022['NAME']==state]\n    df=pd.concat([row1,row2,row3,row4,row5])\n    df=df.drop('NAME',axis=1)\n    df=df.set_index('Year').pct_change().reset_index()\n    df['RegionName']=state\n    feature_df=pd.concat([feature_df,df])\n\n\nfeature_df\n\n\n\n\n\n\n\n\nYear\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\n...\nDP04_0134E\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0037E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nDP05_0073E\nRegionName\n\n\n\n\n0\n2017\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAlabama\n\n\n1\n2018\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n...\n0.050667\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.217687\nAlabama\n\n\n2\n2019\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n...\n0.024112\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n-0.178100\nAlabama\n\n\n3\n2021\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n...\n0.066914\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n-0.005625\nAlabama\n\n\n4\n2022\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n...\n0.060395\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n10.243876\nAlabama\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n0\n2017\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nDistrict of Columbia\n\n\n1\n2018\n0.021320\n0.018198\n0.049960\n0.151328\n0.030406\n0.112103\n-0.151330\n-0.025084\n-0.007725\n...\n0.011341\n0.012224\n0.006652\n-0.002941\n0.042915\n0.004893\n0.143195\n-0.033720\n0.598440\nDistrict of Columbia\n\n\n2\n2019\n0.014241\n-0.355454\n-0.490505\n-0.365204\n-0.869834\n0.977101\n-0.287304\n-0.701942\n0.298245\n...\n0.057388\n0.004689\n-0.008811\n0.011799\n0.011158\n0.002899\n-0.023810\n0.042995\n0.392450\nDistrict of Columbia\n\n\n3\n2021\n0.096015\n-0.010890\n0.155305\n-0.343210\n0.020759\n-0.059821\n-0.015675\n-0.257760\n-0.112117\n...\n0.040549\n-0.050583\n0.010000\n0.014577\n-0.154024\n-0.097916\n-0.320785\n-0.035861\n-0.005959\nDistrict of Columbia\n\n\n4\n2022\n0.023172\n0.000929\n-0.045818\n-0.211063\n-0.092319\n0.055245\n-0.370224\n0.098800\n-0.089982\n...\n0.104916\n0.002616\n-0.001100\n0.002874\n0.016235\n-0.023242\n1.443404\n0.009714\n8.276008\nDistrict of Columbia\n\n\n\n\n255 rows × 33 columns\n\n\n\n\nfeature_df=feature_df.dropna()\n\n\nfeature_df['Year']=feature_df['Year'].astype(str)\n\nC:\\Users\\npeno\\AppData\\Local\\Temp\\ipykernel_648\\58169971.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  feature_df['Year']=feature_df['Year'].astype(str)\n\n\n\nfeature_df\n\n\n\n\n\n\n\n\nYear\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\n...\nDP04_0134E\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0037E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nDP05_0073E\nRegionName\n\n\n\n\n1\n2018\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n...\n0.050667\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.217687\nAlabama\n\n\n2\n2019\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n...\n0.024112\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n-0.178100\nAlabama\n\n\n3\n2021\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n...\n0.066914\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n-0.005625\nAlabama\n\n\n4\n2022\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n...\n0.060395\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n10.243876\nAlabama\n\n\n1\n2018\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n...\n-0.019983\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.054155\nAlaska\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4\n2022\n0.002299\n-0.012945\n-0.040358\n-0.228287\n-0.235682\n0.039527\n0.185354\n-0.089400\n0.011854\n...\n0.006749\n0.004454\n0.003820\n0.002564\n0.004535\n-0.387319\n0.073581\n-0.233665\n42.074760\nWyoming\n\n\n1\n2018\n0.021320\n0.018198\n0.049960\n0.151328\n0.030406\n0.112103\n-0.151330\n-0.025084\n-0.007725\n...\n0.011341\n0.012224\n0.006652\n-0.002941\n0.042915\n0.004893\n0.143195\n-0.033720\n0.598440\nDistrict of Columbia\n\n\n2\n2019\n0.014241\n-0.355454\n-0.490505\n-0.365204\n-0.869834\n0.977101\n-0.287304\n-0.701942\n0.298245\n...\n0.057388\n0.004689\n-0.008811\n0.011799\n0.011158\n0.002899\n-0.023810\n0.042995\n0.392450\nDistrict of Columbia\n\n\n3\n2021\n0.096015\n-0.010890\n0.155305\n-0.343210\n0.020759\n-0.059821\n-0.015675\n-0.257760\n-0.112117\n...\n0.040549\n-0.050583\n0.010000\n0.014577\n-0.154024\n-0.097916\n-0.320785\n-0.035861\n-0.005959\nDistrict of Columbia\n\n\n4\n2022\n0.023172\n0.000929\n-0.045818\n-0.211063\n-0.092319\n0.055245\n-0.370224\n0.098800\n-0.089982\n...\n0.104916\n0.002616\n-0.001100\n0.002874\n0.016235\n-0.023242\n1.443404\n0.009714\n8.276008\nDistrict of Columbia\n\n\n\n\n204 rows × 33 columns\n\n\n\n\nzillow=pd.read_csv('data/Zillow.csv')\n\n\nzillowlong=zillow.melt(id_vars=['RegionName'],var_name='Year',value_name='Typical Home Value')\n\n\nzillowlong\n\n\n\n\n\n\n\n\nRegionName\nYear\nTypical Home Value\n\n\n\n\n0\nCalifornia\n2017\n498316.221451\n\n\n1\nTexas\n2017\n192963.504413\n\n\n2\nFlorida\n2017\n221751.216789\n\n\n3\nNew York\n2017\n316846.572473\n\n\n4\nPennsylvania\n2017\n174153.095534\n\n\n...\n...\n...\n...\n\n\n250\nNorth Dakota\n2022\n254075.792578\n\n\n251\nAlaska\n2022\n350130.429150\n\n\n252\nDistrict of Columbia\n2022\n618945.155475\n\n\n253\nVermont\n2022\n368730.928380\n\n\n254\nWyoming\n2022\n331644.235988\n\n\n\n\n255 rows × 3 columns\n\n\n\n\ntarget_df=pd.DataFrame()\n\n\nfor state in us_states:\n    row1=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2017')]\n    row2=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2018')]\n    row3=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2019')]\n    row4=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2021')]\n    row5=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2022')]\n    df=pd.concat([row1,row2,row3,row4,row5])\n    df=df.drop('RegionName',axis=1)\n    df=df.set_index('Year').pct_change().reset_index()\n    df['RegionName']=state\n    target_df=pd.concat([target_df,df])\n\n\ntarget_df\n\n\n\n\n\n\n\n\nYear\nTypical Home Value\nRegionName\n\n\n\n\n0\n2017\nNaN\nAlabama\n\n\n1\n2018\n0.038688\nAlabama\n\n\n2\n2019\n0.070360\nAlabama\n\n\n3\n2021\n0.263465\nAlabama\n\n\n4\n2022\n0.076587\nAlabama\n\n\n...\n...\n...\n...\n\n\n0\n2017\nNaN\nDistrict of Columbia\n\n\n1\n2018\n0.055570\nDistrict of Columbia\n\n\n2\n2019\n0.017659\nDistrict of Columbia\n\n\n3\n2021\n0.097177\nDistrict of Columbia\n\n\n4\n2022\n-0.009417\nDistrict of Columbia\n\n\n\n\n255 rows × 3 columns\n\n\n\n\ntarget_df=target_df.dropna()\n\n\nfinal_data=pd.merge(feature_df,target_df,on=['RegionName','Year'])\n\n\nfinal_data\n\n\n\n\n\n\n\n\nYear\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\n...\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0037E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nDP05_0073E\nRegionName\nTypical Home Value\n\n\n\n\n0\n2018\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n...\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.217687\nAlabama\n0.038688\n\n\n1\n2019\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n...\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n-0.178100\nAlabama\n0.070360\n\n\n2\n2021\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n...\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n-0.005625\nAlabama\n0.263465\n\n\n3\n2022\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n...\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n10.243876\nAlabama\n0.076587\n\n\n4\n2018\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n...\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.054155\nAlaska\n0.005789\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n199\n2022\n0.002299\n-0.012945\n-0.040358\n-0.228287\n-0.235682\n0.039527\n0.185354\n-0.089400\n0.011854\n...\n0.004454\n0.003820\n0.002564\n0.004535\n-0.387319\n0.073581\n-0.233665\n42.074760\nWyoming\n0.083624\n\n\n200\n2018\n0.021320\n0.018198\n0.049960\n0.151328\n0.030406\n0.112103\n-0.151330\n-0.025084\n-0.007725\n...\n0.012224\n0.006652\n-0.002941\n0.042915\n0.004893\n0.143195\n-0.033720\n0.598440\nDistrict of Columbia\n0.055570\n\n\n201\n2019\n0.014241\n-0.355454\n-0.490505\n-0.365204\n-0.869834\n0.977101\n-0.287304\n-0.701942\n0.298245\n...\n0.004689\n-0.008811\n0.011799\n0.011158\n0.002899\n-0.023810\n0.042995\n0.392450\nDistrict of Columbia\n0.017659\n\n\n202\n2021\n0.096015\n-0.010890\n0.155305\n-0.343210\n0.020759\n-0.059821\n-0.015675\n-0.257760\n-0.112117\n...\n-0.050583\n0.010000\n0.014577\n-0.154024\n-0.097916\n-0.320785\n-0.035861\n-0.005959\nDistrict of Columbia\n0.097177\n\n\n203\n2022\n0.023172\n0.000929\n-0.045818\n-0.211063\n-0.092319\n0.055245\n-0.370224\n0.098800\n-0.089982\n...\n0.002616\n-0.001100\n0.002874\n0.016235\n-0.023242\n1.443404\n0.009714\n8.276008\nDistrict of Columbia\n-0.009417\n\n\n\n\n204 rows × 34 columns\n\n\n\n\nfinal_data.to_csv('./data/RecordData.csv', index=False)\n\n\nDP02colss=['DP02_0001E','DP02_0002E','DP02_0003E','DP02_0007E','DP02_0011E','DP02_0037E','DP02_0060E','DP02_0061E','DP02_0062E','DP02_0063E','DP02_0064E','DP02_0065E']\nDP03colss=['DP03_0026PE','DP03_0005E','DP03_0062E','DP03_0063E','DP03_0093E','DP03_0094E']\nDP04colss=['DP04_0002E','DP04_0001E','DP04_0037E','DP04_0047E','DP04_0134E']\nDP05colss=['DP05_0001E','DP05_0004E','DP05_0018E','DP05_0037E','DP05_0038E','DP05_0039E','DP05_0044E','DP05_0073E']\n\n\nfor name in DP02colss:\n    print(feature_df[name].max())\n\n0.0960146791508043\n0.08325965443347161\n0.21027912686129047\n0.5718313168095854\n0.31098100619511193\n4.71179314694135\n0.5836723372781065\n0.2331338411316648\n1.16632039564257\n3.7135790077430455\n0.20096335286522993\n1.8600379486638907\n\n\n\nfor name in DP03colss:\n    print(feature_df[name].max())\n\n0.08159905963183367\n0.9855738883335392\n0.1558987776230465\n0.13903133903133913\n0.18793842369265668\n0.19923024054982807\n\n\n\nfor name in DP04colss:\n    print(feature_df[name].max())\n\n0.0960146791508043\n0.10739311182290723\n0.03921568627450989\n0.1304236996210817\n0.17613636363636354\n\n\n\nfor name in DP05colss:\n    print(feature_df[name].max())\n\n0.06371228802533757\n0.024340770791075217\n0.026666666666666616\n0.04291548345376328\n0.9405508590128171\n1.598558282208589\n0.3942459396751741\n85.27326007326008\n\n\n4.71179314694135 85.27326007326008\nWe may need to drop some columns that are producing outliers."
  },
  {
    "objectID": "DataGathering.html",
    "href": "DataGathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "import requests\nimport json\nimport re\nimport pandas as pd\n\nI will attempt to gather all the data I need to answer my 10 questions. Most of the data I acquire will come from census.gov. I will use data tables from the U.S. Census Bureau’s American Community Survey (ACS), a nationwide survey that collects and produces information on social, economic, housing, and demographic characteristics about our nation’s population each year.\nI do not know how exactly I will be doing my analysis yet or what variables I think will be most useful, but to cover all my bases I will import the DP02-DP05 tables from 2017-2022, excluding 2020 because there was not accurate data that year due to COVID. I may not need all these tables or columns in the tables but it will be nice to have easy access to them in my future analysis.\nHere are what the ACS tables contain: DP02: Selected Social Characteristics in the United States  DP03: Selected Economic Characteristics in the United States  DP04: Selected Housing Characteristics  DP05: ACS Demographic and Housing Estimates\nHere is a link to the webpage: https://www.census.gov/data/developers/data-sets/ACS-supplemental-data.html\nI decided that I just want to focus on the real estate growth potential in every state. I can start broad by going by state, then in the future I can apply this study to different geographical levels, like if I wanted to focus on one state and compare different counties or cities. The methodology will be the same. So, a state in the United States will be my observational unit.\n\nPython API\nI will start by using an API in python to get one table to see what we are working with.\n\nDP02_URL_2017=\"https://api.census.gov/data/2017/acs/acs1/profile?get=group(DP02)&for=state:*\"\nDP02_2017= requests.get(DP02_URL_2017)\nDP02_2017 = DP02_2017.json()\nDP02_2017=pd.DataFrame(DP02_2017)\nDP02_2017.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n\n\n\n\n0\nDP02_0001E\nDP02_0001EA\nDP02_0001M\nDP02_0001MA\nDP02_0001PE\nDP02_0001PEA\nDP02_0001PM\nDP02_0001PMA\nDP02_0002E\nDP02_0002EA\n...\nDP02_0152EA\nDP02_0152M\nDP02_0152MA\nDP02_0152PE\nDP02_0152PEA\nDP02_0152PM\nDP02_0152PMA\nGEO_ID\nNAME\nstate\n\n\n1\n1091980\nNone\n9693\nNone\n1091980\nNone\n-888888888\n(X)\n716451\nNone\n...\nNone\n9854\nNone\n73.4\nNone\n0.6\nNone\n0400000US28\nMississippi\n28\n\n\n2\n2385135\nNone\n13054\nNone\n2385135\nNone\n-888888888\n(X)\n1527260\nNone\n...\nNone\n15052\nNone\n81.3\nNone\n0.4\nNone\n0400000US29\nMissouri\n29\n\n\n3\n423091\nNone\n4068\nNone\n423091\nNone\n-888888888\n(X)\n262726\nNone\n...\nNone\n4698\nNone\n81.3\nNone\n0.9\nNone\n0400000US30\nMontana\n30\n\n\n4\n754490\nNone\n4583\nNone\n754490\nNone\n-888888888\n(X)\n484989\nNone\n...\nNone\n6206\nNone\n84.4\nNone\n0.5\nNone\n0400000US31\nNebraska\n31\n\n\n\n\n5 rows × 1219 columns\n\n\n\nAs you can see the column names do not tell us much right now because they are codes that match to different variable labels in the census data. For example, DP02_0001E maps to total household counts. In the data cleaning section, we will be sure to give each column a proper label that tells us what the column represents. Furthermore, we will only keep a select few columns out of the large number of variables in the data cleaning section.\nI will use the following code to get each table I want and turn them into csvs for easy retrieval for the rest of my analysis.\n\nstring='https://api.census.gov/data/2016/acs/acs1/profile?get=group(DP05)&for=state'\nlist1=['2016','2017','2018','2019','2021']\nlist2=['2017','2018','2019','2021','2022']\nlist3=['DP01','DP02','DP03','DP04']\nlist4=['DP02','DP03','DP04','DP05']\nfor i in range(5):\n    string=string.replace('DP05',list3[0])\n    for w in range(4):\n        file='./data/'\n        csv='.csv'\n        name=file+list2[i]+list4[w]+csv\n        csvname=file+list2[i]+list3[w]\n        string=string.replace(list1[i],list2[i])\n        string=string.replace(list3[w],list4[w])\n        response=requests.get(string)\n        response = response.json()\n        df=pd.DataFrame(response)\n        df.to_csv(name, index=False)\n\n\n\nR API\nHere I will use an API in R to retreive text data that can give me some insight into some real estate trends and what states people are talking about.\n\nlibrary(RedditExtractoR)\nlibrary(dplyr)\n\n\nstates &lt;- c(\n  \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\",\n  \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\",\n  \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n  \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\",\n  \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\",\n  \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\",\n  \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\",\n  \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\",\n  \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\",\n  \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n)\n\n\ntext&lt;-data.frame()\nsubreddit=\"RealEstate\"\n\nAttemped to run this loop but it seems Reddit would not let me execute this.\n\nfor (state in states){\n    state_df&lt;- find_thread_urls(keywords = state,subreddit=subreddit, sort_by=\"top\", period = 'year')\n    state_df&lt;-state_df|&gt;\n    mutate(State=state)\n    text&lt;-rbind(text,state_df)\n}\n\nStart with just general real estate and attempt to run states another time.\n\nRealEstate &lt;- find_thread_urls(keywords = \"Buy\",subreddit=subreddit, sort_by=\"top\", period = 'year')\n\nparsing URLs on page 1...\n\n\nWarning message in file(con, \"r\"):\n\"cannot open URL 'https://www.reddit.com/r/RealEstate/search.json?restrict_sr=on&q=Buy&sort=top&t=year&limit=100': HTTP status was '429 Unknown Error'\"\n\n\nERROR: Error in h(simpleError(msg, call)): error in evaluating the argument 'content' in selecting a method for function 'fromJSON': cannot open the connection to 'https://www.reddit.com/r/RealEstate/search.json?restrict_sr=on&q=Buy&sort=top&t=year&limit=100'\n\n\n\nfind_thread_urls(keywords = \"Buy\" ,subreddit=subreddit, sort_by=\"top\", period = 'year')\n\n\n\nNews API\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\n# THIS CODE WILL NOT WORK UNLESS YOU INSERT YOUR API KEY IN THE NEXT LINE\nAPI_KEY='6a2b4aca491e4bedb90a4c7275f2ddf6'\nTOPIC='Real Estate'\n\n\nURLpost = {'apiKey': API_KEY,\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\nprint(baseURL)\n# print(URLpost)\n\n#GET DATA FROM API\nresponse = requests.get(baseURL, URLpost) #request data from the server\n# print(response.url);  \nresponse = response.json() #extract txt data from request into json\n\n# PRETTY PRINT\n# https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\n#print(json.dumps(response, indent=2))\n\n# #GET TIMESTAMP FOR PULL REQUEST\n#from datetime import datetime\n#timestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n# SAVE TO FILE \n#with open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n    #json.dump(response, outfile, indent=4)\n\nhttps://newsapi.org/v2/everything?\n\n\n\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\narticle_list=response['articles']   #list of dictionaries for each article\narticle_keys=article_list[0].keys()\nprint(\"AVAILABLE KEYS:\")\nprint(article_keys)\nindex=0\ncleaned_data=[];  \nfor article in article_list:\n    tmp=[]\n    for key in article_keys:\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n            #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            if(src in author): \n                print(\" AUTHOR ERROR:\",author);author='NA'\n            tmp.append(author)\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        # if(key=='description'):\n        #     tmp.append(string_cleaner(article[key]))\n\n        # if(key=='content'):\n        #     tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n            #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n                print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data.append(tmp)\n    index+=1\n\nAVAILABLE KEYS:\ndict_keys(['source', 'author', 'title', 'description', 'url', 'urlToImage', 'publishedAt', 'content'])\n AUTHOR ERROR: grit daily\n AUTHOR ERROR: grit daily\n AUTHOR ERROR: abdo riani, senior contributor, abdo riani, senior contributor https://wwwforbescom/sites/abdoriani/\n AUTHOR ERROR: christopher marquis, contributor, christopher marquis, contributor https://wwwforbescom/sites/christophermarquis/\n AUTHOR ERROR: tmz staff\n\n\n\ncleaned_data\n\n[['business insider',\n  'jennifer sor',\n  'distressed commercial real estate debt hit $80 billion last quarter, the highest amount in a decade',\n  '2023-10-19T15:09:14Z'],\n ['grit daily',\n  'NA',\n  'ranka vucetic on 3 common real estate problems solved by bespoke service',\n  '2023-10-31T11:00:08Z'],\n ['business insider',\n  'phil rosen',\n  \"china's country garden reportedly misses final deadline for a $154 million bond payment\",\n  '2023-10-18T16:25:20Z'],\n ['business insider',\n  'theron mohamed',\n  'wall street warning: ray dalio, jamie dimon, larry fink issue grim outlooks for the global economy',\n  '2023-10-25T12:55:06Z'],\n ['business insider',\n  'jennifer ortakales dawkins',\n  'tjmaxx is quietly closing stores in new york and chicago here are the confirmed locations',\n  '2023-11-02T18:54:22Z'],\n ['business insider',\n  'katherine tangalakis-lippert',\n  \"who's funding hamas?\",\n  '2023-10-22T00:32:03Z'],\n ['npr',\n  'jennifer ludden',\n  'to tackle homelessness faster, la has a kind of real estate agency for the unhoused',\n  '2023-10-24T13:45:55Z'],\n ['business insider',\n  'haley tenore',\n  'home prices are dropping fast in phoenix here are 6 of the lowest-priced real estate listings there right now — including one at $215,000',\n  '2023-10-15T08:52:01Z'],\n ['business insider',\n  'kelsey neubauer',\n  'the allure of tennessee: why floridians are trading their sandy beaches for a quieter life',\n  '2023-10-31T21:52:25Z'],\n ['lifehackercom',\n  'daniel oropeza',\n  'evil week: everything you can find out about someone from public information',\n  '2023-11-01T17:54:00Z'],\n ['npr',\n  'alana wise',\n  \"'breakfast club' host dj envy is being sued for alleged investment fraud\",\n  '2023-10-28T12:01:08Z'],\n ['business insider',\n  'hana r alberts',\n  \"a new 'wake-up call for real-estate agents' could reshape how we buy and sell homes\",\n  '2023-11-01T19:25:37Z'],\n ['business insider',\n  'james rodriguez',\n  'real-estate agents may be forced to charge home buyers less, thanks to a new $18 billion verdict',\n  '2023-11-05T11:22:01Z'],\n ['grit daily',\n  'NA',\n  'how techvestor uses data to excel in the short-term rental market',\n  '2023-10-24T11:00:31Z'],\n ['time',\n  'oliver staley',\n  'the housing market is so bad that zillow is offering buyers free money',\n  '2023-10-18T15:47:00Z'],\n ['business insider',\n  'huileng tan',\n  \"jeff bezos just added yet another property to his 'billionaire bunker' collection after snapping up a 7-bedroom mansion for $79 million: report\",\n  '2023-10-13T04:36:07Z'],\n ['business insider',\n  'huileng tan',\n  'bad news, homeowners: 1 of these 3 things needs to happen for house prices to be considered affordable, says a real estate expert',\n  '2023-10-10T04:06:04Z'],\n ['business insider',\n  'tanza loudenback,lisa niser, ea',\n  'inheritance tax: what it is, who has to pay, and how to avoid it',\n  '2023-10-23T19:08:48Z'],\n ['business insider',\n  'sawdah bhaimiya',\n  'google and property developer lendlease ended a deal on $15 billion of bay area real estate projects, as big tech continues to review the role of offices',\n  '2023-11-03T11:58:26Z'],\n ['forbes',\n  'NA',\n  '5 startup opportunities in real estate',\n  '2023-10-27T13:17:21Z'],\n ['business insider',\n  'tom carter',\n  \"wework's inevitable retreat is here\",\n  '2023-11-02T12:51:18Z'],\n ['wired',\n  'christopher null',\n  'hp spectre foldable review: a $5,000 laptop',\n  '2023-10-29T11:00:00Z'],\n ['boing boing',\n  'rob beschizza',\n  'ftx executives blew $8bn on real estate deals, donations to politicians, random \"endorsements\" and various other stupid things',\n  '2023-10-19T23:41:44Z'],\n ['slashdotorg',\n  'editordavid',\n  'dropbox returns over 25% of its san francisco hq to its landlord',\n  '2023-10-22T11:34:00Z'],\n ['business insider',\n  'kneubauer@insidercom (kelsey neubauer)',\n  'floridians are looking to move to these 7 states',\n  '2023-10-17T09:30:01Z'],\n ['business insider',\n  'kylie kirschner',\n  'home prices are dropping in las vegas here are 5 of the cheapest homes for sale right now — all under $300k',\n  '2023-10-26T15:17:37Z'],\n ['business insider',\n  'pete syme',\n  \"a condo on new york's billionaires' row sells for less than half of its original $135 million asking price\",\n  '2023-10-25T14:31:51Z'],\n ['business insider',\n  'lloyd lee',\n  'the average monthly mortgage on a new home is 52% higher than the average rent on an apartment: report',\n  '2023-10-23T05:37:28Z'],\n ['business insider',\n  'phil rosen',\n  \"china's property sector is facing massive risks and authorities must meet it with 'forceful action', imf says\",\n  '2023-10-10T12:59:54Z'],\n ['readwrite',\n  'radek zielinski',\n  'google backs out of $15b bay area campus deals',\n  '2023-11-04T13:00:31Z'],\n ['cool hunting',\n  'david graver',\n  'interview: master distiller viridiana tinoco on clase azuls latest flavorful expression',\n  '2023-11-01T17:24:05Z'],\n ['bloggercom',\n  'calculated risk',\n  'real estate newsletter articles this week: new home sales increase to 759,000 annual rate in september',\n  '2023-10-28T18:11:00Z'],\n ['boing boing',\n  'jennifer sandlin',\n  'sisters win workplace halloween costume award for this amazing roller coaster costume',\n  '2023-11-02T13:12:05Z'],\n ['boing boing',\n  'jason weisberger',\n  'trump idolizes al capone in lament over multiple indictments',\n  '2023-10-10T15:42:48Z'],\n ['business insider',\n  'laura italiano',\n  \"allen weisselberg, trump's loyal ex-cfo, calls tripling the size of trump's penthouse in financial records a 'minor' mistake\",\n  '2023-10-10T16:44:46Z'],\n ['bloggercom',\n  'calculated risk',\n  'real estate newsletter articles this week: current state of the housing market',\n  '2023-10-14T18:11:00Z'],\n ['small business trends',\n  'michael guta',\n  '5 real estate email templates',\n  '2023-10-10T08:21:20Z'],\n ['bloggercom',\n  'calculated risk',\n  'real estate newsletter articles this week: outstanding mortgage rates, ltv and credit scores',\n  '2023-10-07T18:11:00Z'],\n ['bloggercom',\n  'calculated risk',\n  'real estate newsletter articles this week: existing-home sales decreased to new cycle low',\n  '2023-10-21T18:11:00Z'],\n ['boing boing',\n  'david pescovitz',\n  'for $15 million, you could own this large san francisco bay area home featuring six bedroom, four baths, and a meth lab!',\n  '2023-10-30T21:27:49Z'],\n ['business insider',\n  'phil rosen',\n  \"investors warn evergrande could face an 'uncontrolled collapse' that would be a disaster for china's property sector\",\n  '2023-10-09T16:32:15Z'],\n ['business insider',\n  'claire turrell',\n  \"i converted a sheep barn into an airbnb with $2,000 now, i manage over 100 listings here's how i grew my business and weathered the airbnb apocalypse\",\n  '2023-10-19T09:13:27Z'],\n ['business insider',\n  'aria yang',\n  \"i quit my $370k job at meta after having panic attacks and hitting the lowest point of my life i just knew the job wasn't right for me\",\n  '2023-10-09T09:07:01Z'],\n ['business insider',\n  'cork gaines',\n  \"uk household wealth has tanked and it's a sign of what could happen in the us\",\n  '2023-11-01T10:05:02Z'],\n ['business insider',\n  'insider@insidercom (eliza relman)',\n  \"home sweet times square: how new york city can save midtown manhattan from the 'urban doom loop'\",\n  '2023-10-08T09:57:01Z'],\n ['business insider',\n  'daniel geiger,ellen thomas,alistair barr',\n  'data centers are sprouting up as a result of the ai boom, minting fortunes, sucking up energy, and changing rural america',\n  '2023-10-13T16:15:37Z'],\n ['business insider',\n  'filip de mott',\n  'ether could jump 400% in a few years before staging a longer-term rally to $35,000, standard chartered says',\n  '2023-10-11T15:45:12Z'],\n ['business insider',\n  'theron mohamed',\n  \"elon musk warned of cybertruck headaches, consumers' debt woes, and a 'stormy' economic backdrop on tesla's earnings call here are his 14 best quotes\",\n  '2023-10-19T11:36:55Z'],\n ['business insider',\n  'huileng tan',\n  'jeff bezos is moving to miami after 29 years in seattle',\n  '2023-11-03T01:34:19Z'],\n ['bloggercom',\n  'calculated risk',\n  'real estate newsletter articles this week: national house price index up 26% year-over-year in august',\n  '2023-11-04T18:11:00Z'],\n ['bbc news',\n  'https://wwwfacebookcom/bbcnews',\n  \"judge threatens to jail trump for 'blatant' gag order violation\",\n  '2023-10-20T16:53:13Z'],\n ['business insider',\n  'jacob zinkula',\n  '3 reasons it could be the right time to buy a home, despite high interest rates',\n  '2023-10-22T10:18:01Z'],\n ['business insider',\n  'tom carter',\n  'dropbox plans to pay $79 million to shrink its huge sf office, after ceo says the rto push will fail',\n  '2023-10-23T11:39:17Z'],\n ['business insider',\n  'theron mohamed',\n  \"'rich dad poor dad' author echoes warren buffett's signature warning against trying to time the market\",\n  '2023-10-23T11:08:59Z'],\n ['business insider',\n  'john l dorman',\n  'trump mixed up square miles and acres when he bragged to romney about returning land to utah, book says',\n  '2023-10-25T16:11:25Z'],\n ['business insider',\n  'kelsey neubauer',\n  \"locals are being priced out of texas and florida here's where they're looking for affordable homes instead\",\n  '2023-10-15T09:07:01Z'],\n ['business insider',\n  'llopez@insidercom (linette lopez)',\n  'the great china boom is going bust',\n  '2023-10-15T10:22:02Z'],\n ['ritholtzcom',\n  'barry ritholtz',\n  'bubble risk in city property markets?',\n  '2023-10-30T13:00:35Z'],\n ['business insider',\n  'theron mohamed',\n  \"jpmorgan's dimon warns the fed's inflation war will spook markets - and flags a raft of threats to the us economy\",\n  '2023-11-02T11:08:06Z'],\n ['business insider',\n  'jzitser@businessinsidercom (joshua zitser)',\n  \"i advise lottery winners it's easy to make bad decisions – here's what i tell people so they don't blow all their money\",\n  '2023-10-07T08:30:46Z'],\n ['business insider',\n  'theron mohamed',\n  \"be ready for the s&p 500 to crash by 50%, house prices to slide, and a recession to strike, jeremy grantham says here are the elite investor's 16 best quotes from a new interview\",\n  '2023-10-07T10:35:01Z'],\n ['business insider',\n  'theron mohamed',\n  \"the fed's own economists are predicting a goldilocks scenario: no recession, falling inflation, and flat unemployment\",\n  '2023-10-12T14:01:38Z'],\n ['business insider',\n  'filip de mott',\n  'capital is leaving china at the fastest pace in more than 7 years, sending the yuan lower',\n  '2023-10-23T14:24:41Z'],\n ['npr',\n  'greg allen',\n  \"feeling the pinch of high home insurance rates? it's not getting better anytime soon\",\n  '2023-10-26T09:01:17Z'],\n ['uncratecom', 'uncrate', 'sony ps5 slim console', '2023-10-10T19:00:00Z'],\n ['business insider',\n  'adam rogers',\n  'dont blame the pandemic for all the empty storefronts on your street blame big banks',\n  '2023-10-31T09:55:01Z'],\n ['business insider',\n  'jacob shamsian',\n  'trump waited until after he left office to disclose trademarks he owns in china and russia',\n  '2023-11-03T16:38:50Z'],\n ['pitchfork',\n  'evan minsker, matthew strauss',\n  'kevin abstract announces first solo album since brockhamptons indefinite hiatus',\n  '2023-10-11T16:07:44Z'],\n ['forbes',\n  'NA',\n  'modern estate planning that prioritizes people, communities, and the planet',\n  '2023-10-10T13:15:00Z'],\n ['bloggercom',\n  'calculated risk',\n  'fed sloos survey: banks reported tighter standards, weaker demand for all loan types',\n  '2023-11-06T19:00:00Z'],\n ['macrumors',\n  'juli clover',\n  \"have a 13-inch macbook pro? here's why you should upgrade to the new m3 14-inch model\",\n  '2023-11-01T19:06:51Z'],\n ['business insider',\n  'mberg@insidercom (madeline berg)',\n  \"there's one thing taylor swift isn't talking about\",\n  '2023-11-01T09:24:01Z'],\n ['thekitchncom',\n  'ian burke',\n  'this ingenious countertop gadget creates storage out of thin air and matches any kitchen aesthetic',\n  '2023-10-16T20:40:00Z'],\n ['business insider',\n  'jen glantz',\n  'i want to retire before 50, so i asked a financial planner for 3 ways to earn more on my savings',\n  '2023-10-28T11:29:01Z'],\n ['business insider',\n  'jennifer sor',\n  'why now is actually a good time to buy a house',\n  '2023-10-21T12:15:01Z'],\n ['the verge',\n  'elizabeth lopatto',\n  'in the end, the ftx trial was about the friends screwed along the way',\n  '2023-10-26T12:00:00Z'],\n ['business insider',\n  'polly thompson',\n  'joe biden wants more people to start living in empty offices',\n  '2023-10-28T11:33:18Z'],\n ['business insider',\n  'george glover',\n  'china suffers its first foreign investment deficit as us tensions and anti-spying laws spark a western exodus',\n  '2023-11-06T10:07:34Z'],\n ['ritholtzcom',\n  'barry ritholtz',\n  '10 tuesday am reads',\n  '2023-10-31T10:30:05Z'],\n ['business insider',\n  'filip de mott',\n  'beijing will offer fresh stimulus through a rare budget adjustment and increased sovereign debt',\n  '2023-10-24T15:54:56Z'],\n ['business insider',\n  'madeline berg',\n  \"jeff bezos' net worth: how the ex-amazon ceo went from startup founder to one of the richest people in the world\",\n  '2023-11-01T09:16:02Z'],\n ['business insider',\n  'katie balevic,jacob shamsian,lloyd lee',\n  'a jury found sam bankman-fried guilty in his manhattan criminal trial',\n  '2023-11-03T00:02:12Z'],\n ['business insider',\n  'aaron mok,katie balevic,jacob shamsian',\n  \"sbf's mom fought back tears and his dad slumped down when the jury said their son was guilty on all 7 counts\",\n  '2023-11-03T17:56:18Z'],\n ['theregistercom',\n  'laura dobberstein',\n  'google shelves plan to build four san francisco googleburbs',\n  '2023-11-03T08:29:09Z'],\n ['business insider',\n  'cork gaines',\n  'home buyers are finally seeing a light at the end of the tunnel, but it is going to be painful for homeowners and sellers',\n  '2023-10-10T15:19:30Z'],\n ['business insider',\n  'mia jankowicz',\n  \"gigi hadid condemns 'terrorizing of innocent people' in instagram post following hamas attacks on israel\",\n  '2023-10-11T11:56:16Z'],\n ['business insider',\n  'phil rosen',\n  'china is boosting efforts spread the yuan as it sells a record $75 billion in offshore bonds this year',\n  '2023-10-11T17:36:44Z'],\n ['business insider',\n  'phil rosen',\n  \"china's economy is on shaky ground - but its movie industry is hitting historic box office records\",\n  '2023-10-16T13:20:48Z'],\n ['business insider',\n  'george glover',\n  'elite investor john paulson claims in a new lawsuit that his ex-business partner duped him out of millions to fund louis vuitton shopping sprees and las vegas parties',\n  '2023-10-17T11:15:36Z'],\n ['business insider',\n  'hasan chowdhury',\n  'office vacancies have soared to a 20-year high, showing just how tough it is to get workers to return',\n  '2023-10-10T11:56:13Z'],\n ['business insider',\n  'jennifer sor',\n  \"home foreclosures are at their highest level since the start of the pandemic as the 'financial aftermath' take a toll on buyers\",\n  '2023-10-13T13:48:49Z'],\n ['business insider',\n  'filip de mott',\n  \"'bond king' jeffrey gundlach says buy long-dated treasurys, with prices set to rebound going into a recession\",\n  '2023-10-18T18:14:51Z'],\n ['vice news',\n  'roshan abraham, jordan pearson',\n  'realtor influencers are freaking out after $18 billion conspiracy lawsuit',\n  '2023-11-06T18:37:39Z'],\n ['small business trends',\n  'samantha lile',\n  'how to hire a property manager',\n  '2023-10-18T13:00:57Z'],\n ['the washington post',\n  'rachel kurzius',\n  'how to sell a haunted house',\n  '2023-10-23T12:42:01Z'],\n ['theonioncom',\n  'the onion',\n  'donald trump jr takes witness stand in new york fraud trial against father',\n  '2023-11-02T19:09:00Z'],\n ['tmz',\n  'NA',\n  \"jeff bezos buys florida neighbor's mansion for $79 million\",\n  '2023-10-13T16:07:48Z'],\n ['business insider',\n  'kelsey neubauer',\n  'stay or go: why americans are drawn to — or sick of — living in florida',\n  '2023-11-03T16:19:17Z'],\n ['gizmodocom',\n  'dua rashid',\n  'roccat vulcan ii review: a flashy upgrade',\n  '2023-11-02T21:00:00Z'],\n ['business insider',\n  'lakshmi varanasi',\n  'jeff bezos is moving to miami meet some of his neighbors in the exclusive ‘billionaire bunker neighborhood',\n  '2023-11-03T21:37:24Z']]\n\n\n\ndf = pd.DataFrame(cleaned_data)\nprint(df)\ndf.to_csv('./data/news.csv', index=False) #,index_label=['title','src','author','date','description'])\n\n                   0                           1  \\\n0   business insider                jennifer sor   \n1         grit daily                          NA   \n2   business insider                  phil rosen   \n3   business insider              theron mohamed   \n4   business insider  jennifer ortakales dawkins   \n..               ...                         ...   \n95       theonioncom                   the onion   \n96               tmz                          NA   \n97  business insider             kelsey neubauer   \n98        gizmodocom                  dua rashid   \n99  business insider            lakshmi varanasi   \n\n                                                    2                     3  \n0   distressed commercial real estate debt hit $80...  2023-10-19T15:09:14Z  \n1   ranka vucetic on 3 common real estate problems...  2023-10-31T11:00:08Z  \n2   china's country garden reportedly misses final...  2023-10-18T16:25:20Z  \n3   wall street warning: ray dalio, jamie dimon, l...  2023-10-25T12:55:06Z  \n4   tjmaxx is quietly closing stores in new york a...  2023-11-02T18:54:22Z  \n..                                                ...                   ...  \n95  donald trump jr takes witness stand in new yor...  2023-11-02T19:09:00Z  \n96  jeff bezos buys florida neighbor's mansion for...  2023-10-13T16:07:48Z  \n97  stay or go: why americans are drawn to — or si...  2023-11-03T16:19:17Z  \n98          roccat vulcan ii review: a flashy upgrade  2023-11-02T21:00:00Z  \n99  jeff bezos is moving to miami meet some of his...  2023-11-03T21:37:24Z  \n\n[100 rows x 4 columns]\n\n\n\n\nDownload\nhttps://www.zillow.com/research/data/"
  },
  {
    "objectID": "DimensionalityReduction.html",
    "href": "DimensionalityReduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from sklearn.decomposition import PCA\n#from sklearn import preprocessing\nfrom statsmodels.multivariate.pca import PCA\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n\nI start with dropping the year and state columns. Although my data already has standardized units being that it is the percent increase for a given year in a given state, I will still standardize it with mean zero and variance 1.\n\nPCA\n\nrecord=pd.read_csv('data/RecordData.csv')\nrecord=record.drop(['Year','DP05_0073E','RegionName'],axis=1)\nrecord.head()\n\n\n\n\n\n\n\n\nDP02_0001E\nDP02_0002E\nDP02_0003E\nDP02_0007E\nDP02_0011E\nDP02_0037E\nDP02_0060E\nDP02_0061E\nDP02_0062E\nDP02_0063E\n...\nDP04_0047E\nDP04_0134E\nDP05_0001E\nDP05_0004E\nDP05_0018E\nDP05_0037E\nDP05_0038E\nDP05_0039E\nDP05_0044E\nTypical Home Value\n\n\n\n\n0\n0.007341\n0.009226\n-0.008186\n0.090413\n0.007228\n0.004783\n0.022710\n-0.002918\n0.018174\n0.033732\n...\n0.007581\n0.050667\n0.002692\n0.003198\n0.010283\n-0.001775\n-0.000327\n-0.123824\n-0.027097\n0.038688\n\n\n1\n0.022851\n-0.265678\n-0.360479\n-0.317095\n-0.802204\n1.302990\n-0.588360\n-0.705200\n0.439418\n1.429793\n...\n-0.000968\n0.024112\n0.003133\n-0.006376\n0.002545\n0.005908\n0.009572\n0.054480\n0.015884\n0.070360\n\n\n2\n0.036880\n0.013836\n0.056032\n-0.010988\n0.081890\n-0.050374\n-0.025077\n-0.044056\n0.038532\n0.007408\n...\n-0.004602\n0.066914\n0.027878\n0.008556\n0.010152\n-0.013376\n-0.010947\n0.067268\n0.040587\n0.263465\n\n\n3\n0.024848\n0.041137\n0.053954\n-0.058821\n0.009307\n0.106362\n-0.070979\n-0.062720\n-0.020504\n0.014854\n...\n0.017788\n0.060395\n0.006829\n-0.001060\n-0.005025\n0.006291\n-0.002353\n-0.017318\n0.146484\n0.076587\n\n\n4\n0.015195\n0.000753\n-0.043082\n-0.078405\n0.043289\n0.942898\n-0.208181\n0.031255\n0.017837\n-0.065209\n...\n-0.043262\n-0.019983\n-0.003186\n-0.009174\n0.011594\n-0.000634\n0.139775\n0.012942\n-0.060828\n0.005789\n\n\n\n\n5 rows × 31 columns\n\n\n\n\npca=PCA(record,standardize=True,method='eig')\ncomponents=pca.factors\nloadings=pca.loadings\n\n\npcFeats=pd.concat([record,components],axis=1)\ncorrelation=pcFeats.corr()\ncorrelation=correlation[:-len(components.columns)].loc[:,'comp_00':]\n\n\nfig,ax = plt.subplots(figsize=(20, 7))\nsns.heatmap(correlation,annot=True)\nplt.show()\n\n\n\n\nThis shows us what features are correlated with the first principle component, and what features will change together. For example, there is a high negative correlation between the first principle component and percent change of married couple households and married couple households with children under 18 years old. So, you could say that as the percent change of married couple households decrease, so does the percent change of married couple households with children, which makes a lot of sense. There are definitley a lot if redundant variables in my data set.\n\nprint(loadings)\n\n                     comp_00   comp_01   comp_02   comp_03   comp_04  \\\nDP02_0001E         -0.175673 -0.260934  0.070020 -0.330321  0.027259   \nDP02_0002E         -0.283868  0.105401  0.018154  0.017716 -0.040512   \nDP02_0003E         -0.284526  0.094653  0.029675  0.006253 -0.035268   \nDP02_0007E         -0.236327  0.149764  0.008690  0.040273 -0.055856   \nDP02_0011E         -0.273940  0.169863  0.052025  0.020394  0.028655   \nDP02_0037E          0.250797 -0.157965 -0.031902 -0.065628 -0.009657   \nDP02_0060E         -0.238710  0.124871  0.040811 -0.030827  0.034435   \nDP02_0061E         -0.265593  0.199353  0.040195  0.005897  0.048563   \nDP02_0062E          0.223315 -0.110455 -0.015835 -0.008396 -0.011966   \nDP02_0063E          0.257404 -0.141931 -0.056427 -0.055480  0.069882   \nDP02_0064E         -0.270798  0.155756  0.038109  0.054203  0.005297   \nDP02_0065E          0.235744 -0.177624 -0.032053 -0.038397 -0.004967   \nDP03_0026PE         0.030497  0.247655 -0.343971 -0.310877  0.032045   \nDP03_0005E         -0.098648 -0.341029  0.183233  0.160816  0.097013   \nDP03_0062E         -0.013931 -0.043643 -0.519426 -0.001028 -0.120330   \nDP03_0063E         -0.074681 -0.070250 -0.495611 -0.022511 -0.006788   \nDP03_0093E         -0.157299 -0.271912 -0.083721  0.083472 -0.120817   \nDP03_0094E         -0.172684 -0.263518 -0.045605  0.094767 -0.129472   \nDP04_0002E         -0.175673 -0.260934  0.070020 -0.330321  0.027259   \nDP04_0001E         -0.082619 -0.179226  0.086781 -0.456429 -0.114964   \nDP04_0037E         -0.086163 -0.165096 -0.047260  0.290934  0.364261   \nDP04_0047E         -0.024761  0.045191  0.148211 -0.479756 -0.013818   \nDP04_0134E         -0.183816 -0.072695 -0.288294 -0.108330 -0.123876   \nDP05_0001E         -0.103842 -0.142063 -0.244697 -0.107060  0.202590   \nDP05_0004E         -0.132783 -0.200126  0.021660  0.166849 -0.332730   \nDP05_0018E         -0.034015 -0.076520  0.211249 -0.162799  0.390053   \nDP05_0037E          0.134786  0.287644 -0.136889 -0.071346  0.050285   \nDP05_0038E          0.080439  0.063633  0.049926 -0.075772 -0.433031   \nDP05_0039E         -0.006403  0.004149  0.099576 -0.051469 -0.139063   \nDP05_0044E         -0.037631  0.031163 -0.152046 -0.073497  0.500718   \nTypical Home Value -0.187299 -0.274977 -0.159673  0.089512  0.034835   \n\n                     comp_05   comp_06   comp_07   comp_08   comp_09  ...  \\\nDP02_0001E         -0.040148  0.075145 -0.046833  0.011248  0.001809  ...   \nDP02_0002E         -0.080712  0.033228 -0.031057  0.005781 -0.076406  ...   \nDP02_0003E         -0.036268  0.061253 -0.034064 -0.034185 -0.085316  ...   \nDP02_0007E         -0.105859 -0.021156  0.051553  0.081713  0.173931  ...   \nDP02_0011E          0.025420  0.002279  0.003622  0.007610 -0.025463  ...   \nDP02_0037E         -0.044956 -0.002948 -0.137766 -0.016426 -0.124180  ...   \nDP02_0060E          0.132372 -0.043001 -0.153155  0.009641 -0.098655  ...   \nDP02_0061E         -0.012107 -0.030504  0.008204  0.012002 -0.022529  ...   \nDP02_0062E          0.048586 -0.037557 -0.040757 -0.060953  0.336051  ...   \nDP02_0063E          0.027604 -0.013072  0.082291  0.016647 -0.027430  ...   \nDP02_0064E         -0.064334  0.001851 -0.002160  0.034315 -0.041135  ...   \nDP02_0065E         -0.141765  0.058828 -0.154616  0.078077 -0.088818  ...   \nDP03_0026PE        -0.070020 -0.104987 -0.164477  0.149459 -0.012756  ...   \nDP03_0005E         -0.150539 -0.044295  0.022226 -0.065319 -0.223913  ...   \nDP03_0062E          0.137611 -0.162470  0.035140  0.142221 -0.045548  ...   \nDP03_0063E          0.197289 -0.028890  0.079475  0.119573  0.098823  ...   \nDP03_0093E          0.162843  0.050635  0.263325  0.063106 -0.150719  ...   \nDP03_0094E          0.251867  0.008135  0.025185 -0.197968  0.002177  ...   \nDP04_0002E         -0.040148  0.075145 -0.046833  0.011248  0.001809  ...   \nDP04_0001E          0.032252 -0.036723  0.128623 -0.088945 -0.317764  ...   \nDP04_0037E         -0.277007  0.005177 -0.264222  0.181549  0.262218  ...   \nDP04_0047E          0.020016  0.147603 -0.161734 -0.153842  0.535441  ...   \nDP04_0134E          0.032201 -0.028157 -0.094107 -0.004918  0.114581  ...   \nDP05_0001E         -0.539182  0.001229 -0.167979 -0.085317 -0.217447  ...   \nDP05_0004E         -0.104475  0.118259 -0.033703  0.090717  0.284869  ...   \nDP05_0018E          0.238673 -0.171392  0.048718  0.713402 -0.017492  ...   \nDP05_0037E         -0.137006  0.228518 -0.122763 -0.060742 -0.242698  ...   \nDP05_0038E         -0.476806  0.135446  0.422496  0.425822  0.063154  ...   \nDP05_0039E         -0.209812 -0.890122  0.020722 -0.144965  0.083713  ...   \nDP05_0044E         -0.125729  0.034839  0.675657 -0.283303  0.188327  ...   \nTypical Home Value -0.104476  0.036133 -0.085356  0.020754  0.136116  ...   \n\n                     comp_21   comp_22   comp_23   comp_24   comp_25  \\\nDP02_0001E          0.086021  0.001974  0.103495 -0.042613 -0.084438   \nDP02_0002E         -0.013421  0.152611  0.058280  0.163945  0.328531   \nDP02_0003E          0.060703  0.162566 -0.006306  0.056707  0.510411   \nDP02_0007E          0.290511  0.145839  0.285446 -0.128483 -0.047777   \nDP02_0011E         -0.032103 -0.018306 -0.036433 -0.123250  0.282639   \nDP02_0037E         -0.036491  0.605567 -0.151392  0.086464  0.122004   \nDP02_0060E          0.049087 -0.223328 -0.014349 -0.118797  0.027416   \nDP02_0061E         -0.065314  0.049644  0.013941 -0.094492  0.072203   \nDP02_0062E         -0.047241  0.068195  0.017943 -0.058200  0.276777   \nDP02_0063E          0.052915 -0.090217  0.104327 -0.239978  0.581926   \nDP02_0064E         -0.023656 -0.227225 -0.093293 -0.098457 -0.057858   \nDP02_0065E          0.066923 -0.529395  0.155805 -0.198965  0.225851   \nDP03_0026PE        -0.379945  0.079839 -0.292998 -0.489637 -0.057052   \nDP03_0005E          0.243180  0.193134 -0.118721 -0.455295 -0.125096   \nDP03_0062E          0.300134 -0.091765 -0.122793  0.315851  0.055279   \nDP03_0063E          0.184137  0.127770  0.404671 -0.241168 -0.111049   \nDP03_0093E         -0.337883 -0.190830 -0.156774 -0.012981  0.022424   \nDP03_0094E         -0.298945  0.059973 -0.127368 -0.042096  0.034093   \nDP04_0002E          0.086021  0.001974  0.103495 -0.042613 -0.084438   \nDP04_0001E          0.073409  0.042672  0.039970 -0.027118  0.010358   \nDP04_0037E         -0.136699  0.052430 -0.006233 -0.083239 -0.010312   \nDP04_0047E          0.039974 -0.084737 -0.143255  0.087494  0.013129   \nDP04_0134E         -0.135625  0.101271  0.019046  0.016656  0.024662   \nDP05_0001E         -0.248974 -0.072935  0.243300  0.339282 -0.018517   \nDP05_0004E         -0.008677  0.065542  0.132470 -0.119601  0.011411   \nDP05_0018E          0.002218  0.061749 -0.037741  0.147313  0.045425   \nDP05_0037E          0.332069  0.048459 -0.191925 -0.106545 -0.023462   \nDP05_0038E         -0.054520  0.022556 -0.121868 -0.010245  0.010500   \nDP05_0039E          0.063392 -0.043779 -0.034500 -0.019807  0.026121   \nDP05_0044E          0.005480  0.011765 -0.062200 -0.037147  0.037101   \nTypical Home Value  0.351269 -0.127117 -0.589759  0.083395  0.049646   \n\n                     comp_26   comp_27   comp_28   comp_29       comp_30  \nDP02_0001E          0.070044  0.003157  0.016282  0.001361  7.071068e-01  \nDP02_0002E         -0.275362 -0.162082  0.178972  0.733650  3.539391e-14  \nDP02_0003E         -0.293868 -0.243766 -0.058779 -0.629149 -1.885467e-14  \nDP02_0007E         -0.058731  0.016213 -0.038071 -0.011142  2.788586e-16  \nDP02_0011E          0.336281  0.406032 -0.690091  0.133185  7.769155e-15  \nDP02_0037E          0.396062  0.081642  0.078956  0.016217 -1.885364e-15  \nDP02_0060E          0.046992 -0.018188  0.097591  0.069065  1.363749e-15  \nDP02_0061E          0.106112  0.620464  0.630055 -0.150121 -1.403494e-14  \nDP02_0062E          0.078619  0.017798  0.083083  0.006530  1.312863e-15  \nDP02_0063E          0.189481 -0.153294  0.132561  0.082196  5.393421e-15  \nDP02_0064E          0.640132 -0.510027  0.189253 -0.037810 -5.736818e-15  \nDP02_0065E         -0.087846  0.198581  0.041950  0.008574  7.446230e-16  \nDP03_0026PE        -0.211636 -0.076403 -0.038598  0.009450  2.132095e-15  \nDP03_0005E         -0.118311 -0.008990 -0.030766  0.024546  9.500179e-16  \nDP03_0062E          0.070988  0.124192  0.010966 -0.026038 -1.273745e-15  \nDP03_0063E          0.032791 -0.064701 -0.015812  0.014171 -6.817600e-16  \nDP03_0093E         -0.070983  0.032543  0.008149  0.009379  9.601526e-16  \nDP03_0094E         -0.003720  0.013867 -0.004496  0.007744  4.844410e-16  \nDP04_0002E          0.070044  0.003157  0.016282  0.001361 -7.071068e-01  \nDP04_0001E         -0.006838  0.030858  0.008175  0.014548  1.173076e-15  \nDP04_0037E         -0.010496  0.010893  0.006981  0.017404  3.718116e-16  \nDP04_0047E         -0.026359  0.005599 -0.017752  0.015277  1.210923e-15  \nDP04_0134E          0.041376 -0.038627  0.011291 -0.000829 -1.291657e-16  \nDP05_0001E          0.054454  0.018734 -0.074604 -0.072721 -2.829022e-15  \nDP05_0004E          0.039787  0.013295 -0.005899 -0.010252 -5.918665e-16  \nDP05_0018E         -0.030998 -0.034845 -0.025063 -0.021620  4.702222e-16  \nDP05_0037E         -0.006271  0.002065 -0.008771  0.050673  1.209045e-15  \nDP05_0038E          0.036131  0.031398  0.006097 -0.005079 -9.377536e-16  \nDP05_0039E         -0.013638 -0.021549 -0.018325  0.007178  9.296459e-16  \nDP05_0044E         -0.003283  0.003493  0.014631  0.018984  9.614065e-16  \nTypical Home Value -0.044707  0.035093  0.034431  0.029302  1.073314e-15  \n\n[31 rows x 31 columns]\n\n\nThe loadings tell us what features have the most impact on the principle componets. For example, the median rooms has the most information captured in the second principle component.\nFor my analysis I will now attempt to determine the optimal number of principle components.\n\nplt.figure(figsize=(8,6))\npca.plot_scree()\nplt.show()\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n\n\nIt is difficult to see the elbow here, but it seems like there may be an elbow at around 5 principle componenets.\n\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\n\n\nscaler=StandardScaler()\nscaled=scaler.fit_transform(record)\npca = PCA()\ncomponents = pca.fit_transform(scaled)\n\n\nexplainedVarianceRatio=pca.explained_variance_ratio_\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(explainedVarianceRatio) + 1), np.cumsum(explainedVarianceRatio), marker='o')\nplt.title('Scree Plot')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.grid(True)\nplt.show()\n\n\n\n\nWe will go with 5 principle components\nHere we will plot the first two principle components to see if it reveals anything.\n\npca = PCA(n_components=2)\nresult = pca.fit_transform(scaled)\nresultdf=pd.DataFrame(data=result,columns=['PC1','PC2'])\n\n\nplt.scatter(resultdf['PC1'], resultdf['PC2'])\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA Result')\nplt.show()\n\n\n\n\nThis graph does not tell us much, but overall with PCA which features are highly correlated with what principle components and how many principle componenets are optimal, so ultimatley I can reduce the dimensions of my data so I can have more accurate analysis in the future.\n\n\nt-SNE\nIn t-Distributed Stochastic Neighbor Embedding, the key parameter is perplexity which affects the tradeoff between preserving local vs global structures. It aims to map similar datapoints together while keeping differrent data points apart.\n\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntsne_results = tsne.fit_transform(scaled)\n\n[t-SNE] Computing 121 nearest neighbors...\n[t-SNE] Indexed 204 samples in 0.001s...\n[t-SNE] Computed neighbors for 204 samples in 0.411s...\n[t-SNE] Computed conditional probabilities for sample 204 / 204\n[t-SNE] Mean sigma: 2.276934\n[t-SNE] KL divergence after 250 iterations with early exaggeration: 45.974541\n[t-SNE] KL divergence after 300 iterations: 0.267783\n\n\n\nimport plotly.express as px\ntsne = TSNE(n_components=2, random_state=42)\ntsne_result = tsne.fit_transform(scaled)\ntsne_df = pd.DataFrame(data=tsne_result, columns=['TSNE1', 'TSNE2'])\n\n\nfig = px.scatter(tsne_df, x='TSNE1', y='TSNE2', title='t-SNE')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nEvaluation and Comparison\nPCA utilizes a linear approach, preserving the overall structure of data but may encounter challenges with non-linear relationships and sensitivity to outliers. It is computationally efficient and interpretable, making it well-suited for handling substantial datasets. It is better for data sets with a large number of features, over 50 is commonly used benchmark.\nt-SNE is a non-linear technique that excels in capturing local structures and intricate patterns. However, it comes with increased computational demands and sensitivity to hyperparameters. T-SNE is better tailored for smaller datasets.\nI would say for my data, PCA is a better method that more easily allows me to see what features I should drop from my analysis, but overall the visulizations produced from both methods show some clustering that reveals reduncincy in my variables."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Housing Trends\nIn every aspect of life, discovering insights before others can give us a competitive advantage that will allow us to get ahead. In some activities we do, although we may know information others do not, the nature of the framework we are operating in may restrict how much utility we gain. For example, knowing the opponents next move in a game of checkers will only give us the satisfaction of winning the checkers game. However, other information is much more valuable because of the gravity of the affair in the larger scheme of life.\nShelter is a need that every human must have fulfilled. In the United States, 66% of Americans are home owners. These homeowners on average have 70% of their net worth invested into their home. Real estate can also be highly profitable for those wanting to create cash flow from owning homes or mulitifamily homes. Therefore, it is easy to see that the housing market is one area where having insights that others do not have, like housing trends in a certain city, will give us a multitude of utility. I would like to buy a multifamily home after I graduate; if I knew what areas would provide me with consistent tenants and a property that goes up in value, I would be very much ahead of the game in the real estate realm and in life in general.\nIn this project, I will use census data to attempt to reveal housing trends in a certain city.\n\n\nWhat Has Already Been Done\nIn The Winners in China’s Urban Housing Reform, the researchers used census data to reveal who benefited from China’s housing reform that privatized public housing. With the census data, they were able to estimate the housing subsidy received by renters in the public sector by examining private sector prices of houses of comparable quality and size. The census data contained information on six modes of housing tenure that could be compared and contained information on housing characteristics like housing space, number of bathrooms, ect. John R. Logan and Zhang (2010). Although this study has a different end goal than me, I will use census data in a similar way to try to estimate housing prices.\nIn House Price Prediction Using Regression Techniques: A Comparative Study, the researchers attempt to forecast housing prices using different predictive techniques like multiple linear regression, LASSO, and gradient boosting among others Madhuri, Anuradha, and Pujitha (2019). I could potentially use the same techniques to accomplish my goals.\n\n\nQuestions I want to Answer\n\nWhich variable in census data reveals the most about the direction of housing prices in a certain area?\nHow do demographic trends play a role?\nDoes age have an impact?\nDoes sex have an impact?\nDoes race have an impact?\nDoes school enrollment have an impact?\nDoes migration data tell most of the story?\nHow much are home prices impacted by trends in population?\nCan we predict which areas to invest in in a certain city?\nCan we predict which areas are the best for landlords to have a steady flow of tenants?\n\n\n\n\n\n\nReferences\n\nJohn R. Logan, Yiping Fang, and Zhanxin Zhang. 2010. “The Winners in China’s Urban Housing Reform.” Housing Studies 25 (1): 101–17. https://doi.org/10.1080/02673030903240660.\n\n\nMadhuri, CH. Raga, G. Anuradha, and M. Vani Pujitha. 2019. “House Price Prediction Using Regression Techniques: A Comparative Study.” In 2019 International Conference on Smart Structures and Systems (ICSSS), 1–5. https://doi.org/10.1109/ICSSS.2019.8882834."
  }
]