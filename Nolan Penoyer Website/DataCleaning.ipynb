{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Data Cleaning\"\n",
        "format: html\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Record Data Cleaning Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section I will clean and merge all my record data so that I will be left with one table that I will use to build my models. I will also reconfigure my data so it will be suitable for my analysis. \n",
        "\n",
        "**Important: It is at this point I discovered my project was based on time series data. For the rest of the project I needed my data to follow the IID assumption (independent and identically distributed). The time series data I have is not independent, so I pivoted how I would use my data. An observational unit in my case is a specific state coupled with a specific year, like Alabama-2019. The target variable, or variable I am trying to predict, is the percent increase in home value from one year to the next. Below I will explain what a single row of my finished table means more thoroughly. Although all my data is not independent, so you will need to be wary of the validity of some of my conclusions, you can still gain useful information about real estate trends from my analysis and models I construct.**\n",
        "\n",
        "In the code below, you will see me read in specific columns from each table, change column names to accurately represent what is in the column, merge and concatenate all the tables together as one, drop NA values that result from the merges, convert each cell to percent change from the previous year, and check for potential problems with the data, specifically outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Record Data Cleaning Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "DP02cols=['NAME','DP02_0001E','DP02_0002E','DP02_0003E','DP02_0007E','DP02_0011E','DP02_0037E','DP02_0060E','DP02_0061E','DP02_0062E','DP02_0063E','DP02_0064E','DP02_0065E']\n",
        "DP03cols=['NAME','DP03_0026PE','DP03_0005E','DP03_0062E','DP03_0063E','DP03_0093E','DP03_0094E']\n",
        "DP04cols=['NAME','DP04_0002E','DP04_0001E','DP04_0037E','DP04_0047E','DP04_0134E']\n",
        "DP05cols=['NAME','DP05_0001E','DP05_0004E','DP05_0018E','DP05_0037E','DP05_0038E','DP05_0039E','DP05_0044E','DP05_0071E']\n",
        "\n",
        "DP0217=pd.read_csv('data/2017DP02.csv',skiprows=1)\n",
        "DP0218=pd.read_csv('data/2018DP02.csv',skiprows=1)\n",
        "DP0219=pd.read_csv('data/2019DP02.csv',skiprows=1)\n",
        "DP0221=pd.read_csv('data/2021DP02.csv',skiprows=1)\n",
        "DP0222=pd.read_csv('data/2022DP02.csv',skiprows=1)\n",
        "\n",
        "DP0317 = pd.read_csv('data/2017DP03.csv', skiprows=1)\n",
        "DP0318 = pd.read_csv('data/2018DP03.csv', skiprows=1)\n",
        "DP0319 = pd.read_csv('data/2019DP03.csv', skiprows=1)\n",
        "DP0321 = pd.read_csv('data/2021DP03.csv', skiprows=1)\n",
        "DP0322 = pd.read_csv('data/2022DP03.csv', skiprows=1)\n",
        "\n",
        "DP0417 = pd.read_csv('data/2017DP04.csv', skiprows=1)\n",
        "DP0418 = pd.read_csv('data/2018DP04.csv', skiprows=1)\n",
        "DP0419 = pd.read_csv('data/2019DP04.csv', skiprows=1)\n",
        "DP0421 = pd.read_csv('data/2021DP04.csv', skiprows=1)\n",
        "DP0422 = pd.read_csv('data/2022DP04.csv', skiprows=1)\n",
        "\n",
        "DP0517 = pd.read_csv('data/2017DP05.csv', skiprows=1)\n",
        "DP0518 = pd.read_csv('data/2018DP05.csv', skiprows=1)\n",
        "DP0519 = pd.read_csv('data/2019DP05.csv', skiprows=1)\n",
        "DP0521 = pd.read_csv('data/2021DP05.csv', skiprows=1)\n",
        "DP0522 = pd.read_csv('data/2022DP05.csv', skiprows=1)\n",
        "\n",
        "DP0217=DP0217[DP02cols]\n",
        "DP0217['Year']=2017\n",
        "DP0218=DP0218[DP02cols]\n",
        "DP0218['Year']=2018\n",
        "DP0219=DP0219[DP02cols]\n",
        "DP0219['Year']=2019\n",
        "DP0221=DP0221[DP02cols]\n",
        "DP0221['Year']=2021\n",
        "DP0222=DP0222[DP02cols]\n",
        "DP0222['Year']=2022\n",
        "\n",
        "DP0317 = DP0317[DP03cols]\n",
        "DP0318 = DP0318[DP03cols]\n",
        "DP0319 = DP0319[DP03cols]\n",
        "DP0321 = DP0321[DP03cols]\n",
        "DP0322 = DP0322[DP03cols]\n",
        "\n",
        "DP0417 = DP0417[DP04cols]\n",
        "DP0418 = DP0418[DP04cols]\n",
        "DP0419 = DP0419[DP04cols]\n",
        "DP0421 = DP0421[DP04cols]\n",
        "DP0422 = DP0422[DP04cols]\n",
        "\n",
        "DP0517 = DP0517[DP05cols]\n",
        "DP0518 = DP0518[DP05cols]\n",
        "DP0519 = DP0519[DP05cols]\n",
        "DP0521 = DP0521[DP05cols]\n",
        "DP0522 = DP0522[DP05cols]\n",
        "\n",
        "df2017=pd.merge(DP0217,DP0317,on='NAME')\n",
        "df2017=pd.merge(df2017,DP0417,on='NAME')\n",
        "df2017=pd.merge(df2017,DP0517,on='NAME')\n",
        "\n",
        "df2018=pd.merge(DP0218,DP0318,on='NAME')\n",
        "df2018=pd.merge(df2018,DP0418,on='NAME')\n",
        "df2018=pd.merge(df2018,DP0518,on='NAME')\n",
        "\n",
        "df2019=pd.merge(DP0219,DP0319,on='NAME')\n",
        "df2019=pd.merge(df2019,DP0419,on='NAME')\n",
        "df2019=pd.merge(df2019,DP0519,on='NAME')\n",
        "\n",
        "\n",
        "df2021=pd.merge(DP0221,DP0321,on='NAME')\n",
        "df2021=pd.merge(df2021,DP0421,on='NAME')\n",
        "df2021=pd.merge(df2021,DP0521,on='NAME')\n",
        "\n",
        "df2022=pd.merge(DP0222,DP0322,on='NAME')\n",
        "df2022=pd.merge(df2022,DP0422,on='NAME')\n",
        "df2022=pd.merge(df2022,DP0522,on='NAME')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['NAME', 'DP02_0001E', 'DP02_0002E', 'DP02_0003E', 'DP02_0007E',\n",
              "       'DP02_0011E', 'DP02_0037E', 'DP02_0060E', 'DP02_0061E', 'DP02_0062E',\n",
              "       'DP02_0063E', 'DP02_0064E', 'DP02_0065E', 'Year', 'DP03_0026PE',\n",
              "       'DP03_0005E', 'DP03_0062E', 'DP03_0063E', 'DP03_0093E', 'DP03_0094E',\n",
              "       'DP04_0002E', 'DP04_0001E', 'DP04_0037E', 'DP04_0047E', 'DP04_0134E',\n",
              "       'DP05_0001E', 'DP05_0004E', 'DP05_0018E', 'DP05_0037E', 'DP05_0038E',\n",
              "       'DP05_0039E', 'DP05_0044E', 'DP05_0071E'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2017.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "us_states = [\n",
        "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\",\n",
        "    \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\",\n",
        "    \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n",
        "    \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\",\n",
        "    \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\",\n",
        "    \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\",\n",
        "    \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\",\n",
        "    \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\",\n",
        "    \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\",\n",
        "    \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\",\n",
        "    \"District of Columbia\"\n",
        "]\n",
        "feature_df=pd.DataFrame()\n",
        "for state in us_states:\n",
        "    row1=df2017[df2017['NAME']==state]\n",
        "    row2=df2018[df2018['NAME']==state]\n",
        "    row3=df2019[df2019['NAME']==state]\n",
        "    row4=df2021[df2021['NAME']==state]\n",
        "    row5=df2022[df2022['NAME']==state]\n",
        "    df=pd.concat([row1,row2,row3,row4,row5])\n",
        "    df=df.drop('NAME',axis=1)\n",
        "    df=df.set_index('Year').pct_change().reset_index()\n",
        "    df['RegionName']=state\n",
        "    feature_df=pd.concat([feature_df,df])\n",
        "feature_df=feature_df.dropna()\n",
        "feature_df['Year']=feature_df['Year'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>DP02_0001E</th>\n",
              "      <th>DP02_0002E</th>\n",
              "      <th>DP02_0003E</th>\n",
              "      <th>DP02_0007E</th>\n",
              "      <th>DP02_0011E</th>\n",
              "      <th>DP02_0037E</th>\n",
              "      <th>DP02_0060E</th>\n",
              "      <th>DP02_0061E</th>\n",
              "      <th>DP02_0062E</th>\n",
              "      <th>...</th>\n",
              "      <th>DP04_0134E</th>\n",
              "      <th>DP05_0001E</th>\n",
              "      <th>DP05_0004E</th>\n",
              "      <th>DP05_0018E</th>\n",
              "      <th>DP05_0037E</th>\n",
              "      <th>DP05_0038E</th>\n",
              "      <th>DP05_0039E</th>\n",
              "      <th>DP05_0044E</th>\n",
              "      <th>DP05_0071E</th>\n",
              "      <th>RegionName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018</td>\n",
              "      <td>0.007341</td>\n",
              "      <td>0.009226</td>\n",
              "      <td>-0.008186</td>\n",
              "      <td>0.090413</td>\n",
              "      <td>0.007228</td>\n",
              "      <td>0.004783</td>\n",
              "      <td>0.022710</td>\n",
              "      <td>-0.002918</td>\n",
              "      <td>0.018174</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050667</td>\n",
              "      <td>0.002692</td>\n",
              "      <td>0.003198</td>\n",
              "      <td>0.010283</td>\n",
              "      <td>-0.001775</td>\n",
              "      <td>-0.000327</td>\n",
              "      <td>-0.123824</td>\n",
              "      <td>-0.027097</td>\n",
              "      <td>0.047111</td>\n",
              "      <td>Alabama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.022851</td>\n",
              "      <td>-0.265678</td>\n",
              "      <td>-0.360479</td>\n",
              "      <td>-0.317095</td>\n",
              "      <td>-0.802204</td>\n",
              "      <td>1.302990</td>\n",
              "      <td>-0.588360</td>\n",
              "      <td>-0.705200</td>\n",
              "      <td>0.439418</td>\n",
              "      <td>...</td>\n",
              "      <td>0.024112</td>\n",
              "      <td>0.003133</td>\n",
              "      <td>-0.006376</td>\n",
              "      <td>0.002545</td>\n",
              "      <td>0.005908</td>\n",
              "      <td>0.009572</td>\n",
              "      <td>0.054480</td>\n",
              "      <td>0.015884</td>\n",
              "      <td>0.036934</td>\n",
              "      <td>Alabama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021</td>\n",
              "      <td>0.036880</td>\n",
              "      <td>0.013836</td>\n",
              "      <td>0.056032</td>\n",
              "      <td>-0.010988</td>\n",
              "      <td>0.081890</td>\n",
              "      <td>-0.050374</td>\n",
              "      <td>-0.025077</td>\n",
              "      <td>-0.044056</td>\n",
              "      <td>0.038532</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066914</td>\n",
              "      <td>0.027878</td>\n",
              "      <td>0.008556</td>\n",
              "      <td>0.010152</td>\n",
              "      <td>-0.013376</td>\n",
              "      <td>-0.010947</td>\n",
              "      <td>0.067268</td>\n",
              "      <td>0.040587</td>\n",
              "      <td>0.082710</td>\n",
              "      <td>Alabama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022</td>\n",
              "      <td>0.024848</td>\n",
              "      <td>0.041137</td>\n",
              "      <td>0.053954</td>\n",
              "      <td>-0.058821</td>\n",
              "      <td>0.009307</td>\n",
              "      <td>0.106362</td>\n",
              "      <td>-0.070979</td>\n",
              "      <td>-0.062720</td>\n",
              "      <td>-0.020504</td>\n",
              "      <td>...</td>\n",
              "      <td>0.060395</td>\n",
              "      <td>0.006829</td>\n",
              "      <td>-0.001060</td>\n",
              "      <td>-0.005025</td>\n",
              "      <td>0.006291</td>\n",
              "      <td>-0.002353</td>\n",
              "      <td>-0.017318</td>\n",
              "      <td>0.146484</td>\n",
              "      <td>-0.125037</td>\n",
              "      <td>Alabama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018</td>\n",
              "      <td>0.015195</td>\n",
              "      <td>0.000753</td>\n",
              "      <td>-0.043082</td>\n",
              "      <td>-0.078405</td>\n",
              "      <td>0.043289</td>\n",
              "      <td>0.942898</td>\n",
              "      <td>-0.208181</td>\n",
              "      <td>0.031255</td>\n",
              "      <td>0.017837</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.019983</td>\n",
              "      <td>-0.003186</td>\n",
              "      <td>-0.009174</td>\n",
              "      <td>0.011594</td>\n",
              "      <td>-0.000634</td>\n",
              "      <td>0.139775</td>\n",
              "      <td>0.012942</td>\n",
              "      <td>-0.060828</td>\n",
              "      <td>0.026241</td>\n",
              "      <td>Alaska</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Year  DP02_0001E  DP02_0002E  DP02_0003E  DP02_0007E  DP02_0011E  \\\n",
              "1  2018    0.007341    0.009226   -0.008186    0.090413    0.007228   \n",
              "2  2019    0.022851   -0.265678   -0.360479   -0.317095   -0.802204   \n",
              "3  2021    0.036880    0.013836    0.056032   -0.010988    0.081890   \n",
              "4  2022    0.024848    0.041137    0.053954   -0.058821    0.009307   \n",
              "1  2018    0.015195    0.000753   -0.043082   -0.078405    0.043289   \n",
              "\n",
              "   DP02_0037E  DP02_0060E  DP02_0061E  DP02_0062E  ...  DP04_0134E  \\\n",
              "1    0.004783    0.022710   -0.002918    0.018174  ...    0.050667   \n",
              "2    1.302990   -0.588360   -0.705200    0.439418  ...    0.024112   \n",
              "3   -0.050374   -0.025077   -0.044056    0.038532  ...    0.066914   \n",
              "4    0.106362   -0.070979   -0.062720   -0.020504  ...    0.060395   \n",
              "1    0.942898   -0.208181    0.031255    0.017837  ...   -0.019983   \n",
              "\n",
              "   DP05_0001E  DP05_0004E  DP05_0018E  DP05_0037E  DP05_0038E  DP05_0039E  \\\n",
              "1    0.002692    0.003198    0.010283   -0.001775   -0.000327   -0.123824   \n",
              "2    0.003133   -0.006376    0.002545    0.005908    0.009572    0.054480   \n",
              "3    0.027878    0.008556    0.010152   -0.013376   -0.010947    0.067268   \n",
              "4    0.006829   -0.001060   -0.005025    0.006291   -0.002353   -0.017318   \n",
              "1   -0.003186   -0.009174    0.011594   -0.000634    0.139775    0.012942   \n",
              "\n",
              "   DP05_0044E  DP05_0071E  RegionName  \n",
              "1   -0.027097    0.047111     Alabama  \n",
              "2    0.015884    0.036934     Alabama  \n",
              "3    0.040587    0.082710     Alabama  \n",
              "4    0.146484   -0.125037     Alabama  \n",
              "1   -0.060828    0.026241      Alaska  \n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "zillow=pd.read_csv('data/Zillow.csv')\n",
        "zillowlong=zillow.melt(id_vars=['RegionName'],var_name='Year',value_name='Typical Home Value')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RegionName</th>\n",
              "      <th>Year</th>\n",
              "      <th>Typical Home Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>California</td>\n",
              "      <td>2017</td>\n",
              "      <td>498316.221451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Texas</td>\n",
              "      <td>2017</td>\n",
              "      <td>192963.504413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Florida</td>\n",
              "      <td>2017</td>\n",
              "      <td>221751.216789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>New York</td>\n",
              "      <td>2017</td>\n",
              "      <td>316846.572473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pennsylvania</td>\n",
              "      <td>2017</td>\n",
              "      <td>174153.095534</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     RegionName  Year  Typical Home Value\n",
              "0    California  2017       498316.221451\n",
              "1         Texas  2017       192963.504413\n",
              "2       Florida  2017       221751.216789\n",
              "3      New York  2017       316846.572473\n",
              "4  Pennsylvania  2017       174153.095534"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "zillowlong.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_df=pd.DataFrame()\n",
        "for state in us_states:\n",
        "    row1=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2017')]\n",
        "    row2=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2018')]\n",
        "    row3=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2019')]\n",
        "    row4=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2021')]\n",
        "    row5=zillowlong[(zillowlong['RegionName']==state)&(zillowlong['Year']=='2022')]\n",
        "    df=pd.concat([row1,row2,row3,row4,row5])\n",
        "    df=df.drop('RegionName',axis=1)\n",
        "    df=df.set_index('Year').pct_change().reset_index()\n",
        "    df['RegionName']=state\n",
        "    target_df=pd.concat([target_df,df])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_df=target_df.dropna()\n",
        "final_data=pd.merge(feature_df,target_df,on=['RegionName','Year'])\n",
        "new_column_names=['Year','Total households','Married couple households','Married couple with children of the householder under 18 years','Male householder,no spouse/partner present with children of the householder under 18 years','Female householder, no spouse/partner present with children of the householder under 18 years','Number of women 15 to 50 years old who had a birth in the past 12 months','Less than 9th grade','9th to 12th grade,no diploma','High school graduate (includes equivalency)','Some college, no degree','Associates degree','Bachelors degree','Number of people employed','Number of people unemployed','Median Household Income','Mean Household Income','Median earnings for male full-time, year-round workers','Median earnings for female full-time, year-round workers','Occupied Housing Units','Total Housing Units','Median Rooms','Renter Occupied Housing Units','Mean Rent Paid','Total population','Sex ratio (males per 100 females)','Median Age','Race-White','Race-Black','Race-American Indian and Alaska Native','Race-Asian','Hispanic or Latino','RegionName','Typical Home Value']\n",
        "final_data.columns=new_column_names\n",
        "final_data.to_csv('./data/RecordData.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Total households</th>\n",
              "      <th>Married couple households</th>\n",
              "      <th>Married couple with children of the householder under 18 years</th>\n",
              "      <th>Male householder,no spouse/partner present with children of the householder under 18 years</th>\n",
              "      <th>Female householder, no spouse/partner present with children of the householder under 18 years</th>\n",
              "      <th>Number of women 15 to 50 years old who had a birth in the past 12 months</th>\n",
              "      <th>Less than 9th grade</th>\n",
              "      <th>9th to 12th grade,no diploma</th>\n",
              "      <th>High school graduate (includes equivalency)</th>\n",
              "      <th>...</th>\n",
              "      <th>Total population</th>\n",
              "      <th>Sex ratio (males per 100 females)</th>\n",
              "      <th>Median Age</th>\n",
              "      <th>Race-White</th>\n",
              "      <th>Race-Black</th>\n",
              "      <th>Race-American Indian and Alaska Native</th>\n",
              "      <th>Race-Asian</th>\n",
              "      <th>Hispanic or Latino</th>\n",
              "      <th>RegionName</th>\n",
              "      <th>Typical Home Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018</td>\n",
              "      <td>0.007341</td>\n",
              "      <td>0.009226</td>\n",
              "      <td>-0.008186</td>\n",
              "      <td>0.090413</td>\n",
              "      <td>0.007228</td>\n",
              "      <td>0.004783</td>\n",
              "      <td>0.022710</td>\n",
              "      <td>-0.002918</td>\n",
              "      <td>0.018174</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002692</td>\n",
              "      <td>0.003198</td>\n",
              "      <td>0.010283</td>\n",
              "      <td>-0.001775</td>\n",
              "      <td>-0.000327</td>\n",
              "      <td>-0.123824</td>\n",
              "      <td>-0.027097</td>\n",
              "      <td>0.047111</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>0.038688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.022851</td>\n",
              "      <td>-0.265678</td>\n",
              "      <td>-0.360479</td>\n",
              "      <td>-0.317095</td>\n",
              "      <td>-0.802204</td>\n",
              "      <td>1.302990</td>\n",
              "      <td>-0.588360</td>\n",
              "      <td>-0.705200</td>\n",
              "      <td>0.439418</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003133</td>\n",
              "      <td>-0.006376</td>\n",
              "      <td>0.002545</td>\n",
              "      <td>0.005908</td>\n",
              "      <td>0.009572</td>\n",
              "      <td>0.054480</td>\n",
              "      <td>0.015884</td>\n",
              "      <td>0.036934</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>0.070360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021</td>\n",
              "      <td>0.036880</td>\n",
              "      <td>0.013836</td>\n",
              "      <td>0.056032</td>\n",
              "      <td>-0.010988</td>\n",
              "      <td>0.081890</td>\n",
              "      <td>-0.050374</td>\n",
              "      <td>-0.025077</td>\n",
              "      <td>-0.044056</td>\n",
              "      <td>0.038532</td>\n",
              "      <td>...</td>\n",
              "      <td>0.027878</td>\n",
              "      <td>0.008556</td>\n",
              "      <td>0.010152</td>\n",
              "      <td>-0.013376</td>\n",
              "      <td>-0.010947</td>\n",
              "      <td>0.067268</td>\n",
              "      <td>0.040587</td>\n",
              "      <td>0.082710</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>0.263465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022</td>\n",
              "      <td>0.024848</td>\n",
              "      <td>0.041137</td>\n",
              "      <td>0.053954</td>\n",
              "      <td>-0.058821</td>\n",
              "      <td>0.009307</td>\n",
              "      <td>0.106362</td>\n",
              "      <td>-0.070979</td>\n",
              "      <td>-0.062720</td>\n",
              "      <td>-0.020504</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006829</td>\n",
              "      <td>-0.001060</td>\n",
              "      <td>-0.005025</td>\n",
              "      <td>0.006291</td>\n",
              "      <td>-0.002353</td>\n",
              "      <td>-0.017318</td>\n",
              "      <td>0.146484</td>\n",
              "      <td>-0.125037</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>0.076587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018</td>\n",
              "      <td>0.015195</td>\n",
              "      <td>0.000753</td>\n",
              "      <td>-0.043082</td>\n",
              "      <td>-0.078405</td>\n",
              "      <td>0.043289</td>\n",
              "      <td>0.942898</td>\n",
              "      <td>-0.208181</td>\n",
              "      <td>0.031255</td>\n",
              "      <td>0.017837</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003186</td>\n",
              "      <td>-0.009174</td>\n",
              "      <td>0.011594</td>\n",
              "      <td>-0.000634</td>\n",
              "      <td>0.139775</td>\n",
              "      <td>0.012942</td>\n",
              "      <td>-0.060828</td>\n",
              "      <td>0.026241</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>0.005789</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 34 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Year  Total households  Married couple households  \\\n",
              "0  2018          0.007341                   0.009226   \n",
              "1  2019          0.022851                  -0.265678   \n",
              "2  2021          0.036880                   0.013836   \n",
              "3  2022          0.024848                   0.041137   \n",
              "4  2018          0.015195                   0.000753   \n",
              "\n",
              "   Married couple with children of the householder under 18 years  \\\n",
              "0                                          -0.008186                \n",
              "1                                          -0.360479                \n",
              "2                                           0.056032                \n",
              "3                                           0.053954                \n",
              "4                                          -0.043082                \n",
              "\n",
              "   Male householder,no spouse/partner present with children of the householder under 18 years  \\\n",
              "0                                           0.090413                                            \n",
              "1                                          -0.317095                                            \n",
              "2                                          -0.010988                                            \n",
              "3                                          -0.058821                                            \n",
              "4                                          -0.078405                                            \n",
              "\n",
              "   Female householder, no spouse/partner present with children of the householder under 18 years  \\\n",
              "0                                           0.007228                                               \n",
              "1                                          -0.802204                                               \n",
              "2                                           0.081890                                               \n",
              "3                                           0.009307                                               \n",
              "4                                           0.043289                                               \n",
              "\n",
              "   Number of women 15 to 50 years old who had a birth in the past 12 months  \\\n",
              "0                                           0.004783                          \n",
              "1                                           1.302990                          \n",
              "2                                          -0.050374                          \n",
              "3                                           0.106362                          \n",
              "4                                           0.942898                          \n",
              "\n",
              "   Less than 9th grade  9th to 12th grade,no diploma  \\\n",
              "0             0.022710                     -0.002918   \n",
              "1            -0.588360                     -0.705200   \n",
              "2            -0.025077                     -0.044056   \n",
              "3            -0.070979                     -0.062720   \n",
              "4            -0.208181                      0.031255   \n",
              "\n",
              "   High school graduate (includes equivalency)  ...  Total population  \\\n",
              "0                                     0.018174  ...          0.002692   \n",
              "1                                     0.439418  ...          0.003133   \n",
              "2                                     0.038532  ...          0.027878   \n",
              "3                                    -0.020504  ...          0.006829   \n",
              "4                                     0.017837  ...         -0.003186   \n",
              "\n",
              "   Sex ratio (males per 100 females)  Median Age  Race-White  Race-Black  \\\n",
              "0                           0.003198    0.010283   -0.001775   -0.000327   \n",
              "1                          -0.006376    0.002545    0.005908    0.009572   \n",
              "2                           0.008556    0.010152   -0.013376   -0.010947   \n",
              "3                          -0.001060   -0.005025    0.006291   -0.002353   \n",
              "4                          -0.009174    0.011594   -0.000634    0.139775   \n",
              "\n",
              "   Race-American Indian and Alaska Native  Race-Asian  Hispanic or Latino  \\\n",
              "0                               -0.123824   -0.027097            0.047111   \n",
              "1                                0.054480    0.015884            0.036934   \n",
              "2                                0.067268    0.040587            0.082710   \n",
              "3                               -0.017318    0.146484           -0.125037   \n",
              "4                                0.012942   -0.060828            0.026241   \n",
              "\n",
              "   RegionName  Typical Home Value  \n",
              "0     Alabama            0.038688  \n",
              "1     Alabama            0.070360  \n",
              "2     Alabama            0.263465  \n",
              "3     Alabama            0.076587  \n",
              "4      Alaska            0.005789  \n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(204, 34)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Record Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see with the cleaned record data above, my data set has 204 rows and 34 columns. Two of the columns serve as an index, with the state and year as the unique identifiers. Each cell represents the percent change of the variable from the previous year. For example, the first row is Alabama-2018. This row represents the percent change from 2017 to 2018 for all the census predictor variables and the home value target variable. More data would be ideal for my analysis, but for the purposes of this project I will continue with the data I already wrangled. 31 features are a lot and I should definitely aim to cut this number down. More on this in the dimensionality reduction and data exploration tabs, but I start below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now I will quickly look at the max of each column just to see if there are any obvious problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "DP02colss=['DP02_0001E','DP02_0002E','DP02_0003E','DP02_0007E','DP02_0011E','DP02_0037E','DP02_0060E','DP02_0061E','DP02_0062E','DP02_0063E','DP02_0064E','DP02_0065E']\n",
        "DP03colss=['DP03_0026PE','DP03_0005E','DP03_0062E','DP03_0063E','DP03_0093E','DP03_0094E']\n",
        "DP04colss=['DP04_0002E','DP04_0001E','DP04_0037E','DP04_0047E','DP04_0134E']\n",
        "DP05colss=['DP05_0001E','DP05_0004E','DP05_0018E','DP05_0037E','DP05_0038E','DP05_0039E','DP05_0044E','DP05_0071E']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0960146791508043\n",
            "0.08325965443347161\n",
            "0.21027912686129047\n",
            "0.5718313168095854\n",
            "0.31098100619511193\n",
            "4.71179314694135\n",
            "0.5836723372781065\n",
            "0.2331338411316648\n",
            "1.16632039564257\n",
            "3.7135790077430455\n",
            "0.20096335286522993\n",
            "1.8600379486638907\n"
          ]
        }
      ],
      "source": [
        "for name in DP02colss:\n",
        "    print(feature_df[name].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.08159905963183367\n",
            "0.9855738883335392\n",
            "0.1558987776230465\n",
            "0.13903133903133913\n",
            "0.18793842369265668\n",
            "0.19923024054982807\n"
          ]
        }
      ],
      "source": [
        "for name in DP03colss:\n",
        "    print(feature_df[name].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0960146791508043\n",
            "0.10739311182290723\n",
            "0.03921568627450989\n",
            "0.1304236996210817\n",
            "0.17613636363636354\n"
          ]
        }
      ],
      "source": [
        "for name in DP04colss:\n",
        "    print(feature_df[name].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.06371228802533757\n",
            "0.024340770791075217\n",
            "0.026666666666666616\n",
            "0.04291548345376328\n",
            "0.9405508590128171\n",
            "1.598558282208589\n",
            "0.3942459396751741\n",
            "0.7167422884813359\n"
          ]
        }
      ],
      "source": [
        "for name in DP05colss:\n",
        "    print(feature_df[name].max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks like there are some obvious outliers, with one cell having a 470% increase in the metric. With the ACS data, for each measure they release to the public, they also release a margin of error. The margin of error can be high at times, explaining why some percent change values may be out of the ordinary. In the next tab I will dig deeper into the outliers."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
